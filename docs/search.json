[
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "This page lists all bibliographic entries used across the website.\n\n\n1 References",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Linear Algebra and Data Science",
    "section": "",
    "text": "This website is a conceptual and applied guide to Linear Algebra and Spectral Methods,\nwith a clear focus on Data Science applications.\nThe material bridges the gap between: - mathematical theory (linear algebra, spectral theory), - and practical tools used in modern data analysis.\nRather than treating algorithms as black boxes, the emphasis is on: geometry, structure, and interpretation.",
    "crumbs": [
      "Linear Algebra and Data Science"
    ]
  },
  {
    "objectID": "index.html#what-is-this-project",
    "href": "index.html#what-is-this-project",
    "title": "Linear Algebra and Data Science",
    "section": "",
    "text": "This website is a conceptual and applied guide to Linear Algebra and Spectral Methods,\nwith a clear focus on Data Science applications.\nThe material bridges the gap between: - mathematical theory (linear algebra, spectral theory), - and practical tools used in modern data analysis.\nRather than treating algorithms as black boxes, the emphasis is on: geometry, structure, and interpretation.",
    "crumbs": [
      "Linear Algebra and Data Science"
    ]
  },
  {
    "objectID": "index.html#why-linear-algebra-matters-in-data-science",
    "href": "index.html#why-linear-algebra-matters-in-data-science",
    "title": "Linear Algebra and Data Science",
    "section": "2 Why Linear Algebra Matters in Data Science",
    "text": "2 Why Linear Algebra Matters in Data Science\nMany core methods in Data Science are fundamentally linear and spectral:\n\ndimensionality reduction,\nregularization and stability,\nclustering and embedding,\nmatrix factorization and compression.\n\nUnderstanding these methods deeply requires more than knowing how to call an API. It requires understanding: - vector spaces, - projections, - eigenvalues and singular values, - low-rank structure.\nThis project shows how these ideas form a coherent mathematical framework.",
    "crumbs": [
      "Linear Algebra and Data Science"
    ]
  },
  {
    "objectID": "index.html#what-you-will-find-here",
    "href": "index.html#what-you-will-find-here",
    "title": "Linear Algebra and Data Science",
    "section": "3 What You Will Find Here",
    "text": "3 What You Will Find Here\nThe site is organized as a progressive narrative.\n\n3.1 Foundations\n\nVector spaces and subspaces\n\nLinear maps and matrices\n\nInner products, orthogonality, projections\n\nComputational cost and complexity\n\n\n\n3.2 Spectral Theory\n\nEigenvalues and eigenvectors\n\nSpectral theorem for symmetric operators\n\nRayleigh quotients and quadratic forms\n\n\n\n3.3 Singular Value Decomposition\n\nSVD as a generalization of spectral theory\n\nGeometry of singular vectors\n\nOptimal low-rank approximation\n\n\n\n3.4 Applications\n\nPrincipal Component Analysis (PCA)\n\nTruncated SVD and ridge regularization\n\nBias–variance trade-offs\n\nSpectral clustering and graph Laplacians\n\n\n\n3.5 Synthesis\n\nA unified view of spectral methods in Data Science\n\nEach topic is developed with: - precise definitions, - key theorems (with proofs when relevant), - geometric intuition, - Python examples.",
    "crumbs": [
      "Linear Algebra and Data Science"
    ]
  },
  {
    "objectID": "index.html#who-is-this-for",
    "href": "index.html#who-is-this-for",
    "title": "Linear Algebra and Data Science",
    "section": "4 Who Is This For?",
    "text": "4 Who Is This For?\nThis project is intended for:\n\nData Scientists who want a deeper understanding of the tools they use,\nMachine Learning practitioners interested in model stability and structure,\nMathematicians exploring applied spectral methods,\nGraduate students in Data Science, Statistics, or Applied Mathematics.\n\nA basic familiarity with linear algebra is assumed, but advanced topics are developed from first principles.",
    "crumbs": [
      "Linear Algebra and Data Science"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-site",
    "href": "index.html#how-to-use-this-site",
    "title": "Linear Algebra and Data Science",
    "section": "5 How to Use This Site",
    "text": "5 How to Use This Site\nYou can approach the material in different ways:\n\nTheory-first\nStart from the foundations and progress toward applications.\nApplication-first\nJump directly to PCA, regularization, or spectral clustering, then trace the theory back.\nReference mode\nUse the chapters as a conceptual reference for spectral methods.\n\nThe chapters are designed to be read independently, while still forming a coherent whole.",
    "crumbs": [
      "Linear Algebra and Data Science"
    ]
  },
  {
    "objectID": "index.html#philosophy",
    "href": "index.html#philosophy",
    "title": "Linear Algebra and Data Science",
    "section": "6 Philosophy",
    "text": "6 Philosophy\nThe guiding idea of this project is simple:\n\nMany data problems are not about finding complex models,\nbut about identifying the right directions.\n\nSpectral methods provide a principled way to do exactly that.",
    "crumbs": [
      "Linear Algebra and Data Science"
    ]
  },
  {
    "objectID": "index.html#about-the-author",
    "href": "index.html#about-the-author",
    "title": "Linear Algebra and Data Science",
    "section": "7 About the Author",
    "text": "7 About the Author\nThis project is developed by Erik Contreras López,\nMathematician (PhD) and Data Scientist.\nIt reflects both: - academic training in mathematics, - practical experience in data-driven modeling.",
    "crumbs": [
      "Linear Algebra and Data Science"
    ]
  },
  {
    "objectID": "index.html#start-exploring",
    "href": "index.html#start-exploring",
    "title": "Linear Algebra and Data Science",
    "section": "8 Start Exploring",
    "text": "8 Start Exploring\nUse the navigation bar above to begin with the foundations,\nor jump directly to the applications that interest you most.",
    "crumbs": [
      "Linear Algebra and Data Science"
    ]
  },
  {
    "objectID": "chapters/06-pca-application.html",
    "href": "chapters/06-pca-application.html",
    "title": "Principal Component Analysis (PCA): A Spectral and SVD-Based Application",
    "section": "",
    "text": "Principal Component Analysis (PCA) is one of the most widely used techniques in Data Science for:\n\ndimensionality reduction,\ndata visualization,\nnoise filtering,\nfeature extraction.\n\nDespite its popularity, PCA is often presented as a purely algorithmic method. In reality, PCA is a direct application of spectral theory and the Singular Value Decomposition.\nThis chapter presents PCA from a linear algebra perspective, connecting geometry, optimization, and computation.",
    "crumbs": [
      "Data Science Applications",
      "Principal Component Analysis (PCA): A Spectral and SVD-Based Application"
    ]
  },
  {
    "objectID": "chapters/06-pca-application.html#motivation",
    "href": "chapters/06-pca-application.html#motivation",
    "title": "Principal Component Analysis (PCA): A Spectral and SVD-Based Application",
    "section": "",
    "text": "Principal Component Analysis (PCA) is one of the most widely used techniques in Data Science for:\n\ndimensionality reduction,\ndata visualization,\nnoise filtering,\nfeature extraction.\n\nDespite its popularity, PCA is often presented as a purely algorithmic method. In reality, PCA is a direct application of spectral theory and the Singular Value Decomposition.\nThis chapter presents PCA from a linear algebra perspective, connecting geometry, optimization, and computation.",
    "crumbs": [
      "Data Science Applications",
      "Principal Component Analysis (PCA): A Spectral and SVD-Based Application"
    ]
  },
  {
    "objectID": "chapters/06-pca-application.html#setting-and-notation",
    "href": "chapters/06-pca-application.html#setting-and-notation",
    "title": "Principal Component Analysis (PCA): A Spectral and SVD-Based Application",
    "section": "2 Setting and Notation",
    "text": "2 Setting and Notation\nLet\n\\[\nX \\in \\mathbb{R}^{m \\times n}\n\\]\nbe a data matrix, where: - rows correspond to observations, - columns correspond to features.\nWe assume that \\(X\\) is centered, meaning that each column has zero mean.",
    "crumbs": [
      "Data Science Applications",
      "Principal Component Analysis (PCA): A Spectral and SVD-Based Application"
    ]
  },
  {
    "objectID": "chapters/06-pca-application.html#pca-as-a-variance-maximization-problem",
    "href": "chapters/06-pca-application.html#pca-as-a-variance-maximization-problem",
    "title": "Principal Component Analysis (PCA): A Spectral and SVD-Based Application",
    "section": "3 PCA as a Variance Maximization Problem",
    "text": "3 PCA as a Variance Maximization Problem\n\n3.1 One-dimensional case\nWe seek a unit vector \\(v \\in \\mathbb{R}^n\\) that maximizes the variance of the projected data:\n\\[\n\\max_{\\|v\\|=1} \\; \\mathrm{Var}(Xv).\n\\]\nSince \\(X\\) is centered,\n\\[\n\\mathrm{Var}(Xv) = \\frac{1}{m} \\|Xv\\|_2^2\n= \\frac{1}{m} v^\\top X^\\top X v.\n\\]\nThus, PCA solves the optimization problem\n\\[\n\\max_{\\|v\\|=1} v^\\top \\left(\\frac{1}{m} X^\\top X\\right) v.\n\\]",
    "crumbs": [
      "Data Science Applications",
      "Principal Component Analysis (PCA): A Spectral and SVD-Based Application"
    ]
  },
  {
    "objectID": "chapters/06-pca-application.html#spectral-interpretation",
    "href": "chapters/06-pca-application.html#spectral-interpretation",
    "title": "Principal Component Analysis (PCA): A Spectral and SVD-Based Application",
    "section": "4 Spectral Interpretation",
    "text": "4 Spectral Interpretation\nThe matrix\n\\[\n\\Sigma := \\frac{1}{m} X^\\top X\n\\]\nis the empirical covariance matrix. It is symmetric and positive semidefinite.\nBy the Spectral Theorem:\n\neigenvectors of \\(\\Sigma\\) form an orthonormal basis,\neigenvalues are real and nonnegative.\n\n\n\nTheorem (PCA via Spectral Theory)\n\nThe solution of the PCA maximization problem is given by the eigenvector associated with the largest eigenvalue of \\(\\Sigma\\).\n\nSubsequent principal components are obtained by imposing orthogonality constraints.",
    "crumbs": [
      "Data Science Applications",
      "Principal Component Analysis (PCA): A Spectral and SVD-Based Application"
    ]
  },
  {
    "objectID": "chapters/06-pca-application.html#pca-via-singular-value-decomposition",
    "href": "chapters/06-pca-application.html#pca-via-singular-value-decomposition",
    "title": "Principal Component Analysis (PCA): A Spectral and SVD-Based Application",
    "section": "5 PCA via Singular Value Decomposition",
    "text": "5 PCA via Singular Value Decomposition\nInstead of forming \\(\\Sigma\\), we can compute the SVD of \\(X\\):\n\\[\nX = U \\Sigma_X V^\\top.\n\\]\nThen:\n\ncolumns of \\(V\\) are the principal directions,\nsingular values \\(\\sigma_i\\) satisfy\n\n\\[\n\\lambda_i = \\frac{\\sigma_i^2}{m},\n\\]\nwhere \\(\\lambda_i\\) are the eigenvalues of the covariance matrix.\n\nRemark. \n\nRemark\n\nComputing PCA via SVD is numerically more stable than forming \\(X^\\top X\\), especially when the number of features is large.",
    "crumbs": [
      "Data Science Applications",
      "Principal Component Analysis (PCA): A Spectral and SVD-Based Application"
    ]
  },
  {
    "objectID": "chapters/06-pca-application.html#low-dimensional-embedding",
    "href": "chapters/06-pca-application.html#low-dimensional-embedding",
    "title": "Principal Component Analysis (PCA): A Spectral and SVD-Based Application",
    "section": "6 Low-Dimensional Embedding",
    "text": "6 Low-Dimensional Embedding\nLet \\(V_k \\in \\mathbb{R}^{n \\times k}\\) be the matrix of the first \\(k\\) right singular vectors.\nThe PCA embedding of the data is:\n\\[\nZ = X V_k \\in \\mathbb{R}^{m \\times k}.\n\\]\nEach row of \\(Z\\) is the representation of a data point in the \\(k\\)-dimensional principal subspace.",
    "crumbs": [
      "Data Science Applications",
      "Principal Component Analysis (PCA): A Spectral and SVD-Based Application"
    ]
  },
  {
    "objectID": "chapters/06-pca-application.html#optimal-reconstruction-property",
    "href": "chapters/06-pca-application.html#optimal-reconstruction-property",
    "title": "Principal Component Analysis (PCA): A Spectral and SVD-Based Application",
    "section": "7 Optimal Reconstruction Property",
    "text": "7 Optimal Reconstruction Property\nDefine the rank-\\(k\\) approximation\n\\[\nX_k := U_k \\Sigma_k V_k^\\top.\n\\]\n\n\nTheorem (Optimal reconstruction)\n\n\\(X_k\\) minimizes the reconstruction error\n\\[\n\\|X - Y\\|_F\n\\]\namong all matrices \\(Y\\) of rank at most \\(k\\).\n\nThus, PCA provides the best linear compression of the data.\n\n\nProof of the Optimal Reconstruction Theorem (click to expand)\n\n\nProof. \n\nProof\n\nLet \\(X \\in \\mathbb{R}^{m \\times n}\\) have Singular Value Decomposition\n\\[\nX = \\sum_{i=1}^r \\sigma_i \\, u_i v_i^\\top,\n\\]\nwith singular values \\(\\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge \\sigma_r &gt; 0\\).\nFor a fixed integer \\(k &lt; r\\), define the truncated SVD\n\\[\nX_k := \\sum_{i=1}^k \\sigma_i \\, u_i v_i^\\top.\n\\]\nWe claim that \\(X_k\\) minimizes the reconstruction error\n\\[\n\\|X - Y\\|_F\n\\]\namong all matrices \\(Y \\in \\mathbb{R}^{m \\times n}\\) of rank at most \\(k\\).\n\n\n7.1 Step 1: Orthogonal invariance of the Frobenius norm\nThe Frobenius norm is invariant under orthogonal transformations.\nThat is, for any orthogonal matrices \\(U\\) and \\(V\\),\n\\[\n\\|X - Y\\|_F = \\|U^\\top X V - U^\\top Y V\\|_F.\n\\]\nApplying this with the singular vector bases of \\(X\\), we obtain\n\\[\n\\|X - Y\\|_F\n=\n\\|\\Sigma - \\widetilde{Y}\\|_F,\n\\]\nwhere \\(\\widetilde{Y} = U^\\top Y V\\) and \\(\\mathrm{rank}(\\widetilde{Y}) \\le k\\).\n\n\n\n7.2 Step 2: Reduction to a diagonal problem\nThe matrix \\(\\Sigma\\) is diagonal with entries \\(\\sigma_1,\\dots,\\sigma_r\\). Any matrix \\(\\widetilde{Y}\\) of rank at most \\(k\\) can have at most \\(k\\) nonzero singular values.\nThus, minimizing \\(\\|\\Sigma - \\widetilde{Y}\\|_F\\) reduces to choosing at most \\(k\\) diagonal entries of \\(\\Sigma\\) to keep, and setting the rest to zero.\n\n\n\n7.3 Step 3: Optimal choice\nSince\n\\[\n\\|\\Sigma - \\widetilde{Y}\\|_F^2\n=\n\\sum_{i=1}^r (\\sigma_i - \\widetilde{\\sigma}_i)^2,\n\\]\nthe minimum is achieved by choosing\n\\[\n\\widetilde{\\sigma}_i =\n\\begin{cases}\n\\sigma_i, & i = 1,\\dots,k, \\\\\n0, & i &gt; k.\n\\end{cases}\n\\]\nThis corresponds exactly to \\(\\widetilde{Y} = \\Sigma_k\\) and hence \\(Y = X_k\\).\n\n\n\n7.4 Step 4: Conclusion\nTherefore,\n\\[\n\\|X - X_k\\|_F\n=\n\\min_{\\mathrm{rank}(Y)\\le k} \\|X - Y\\|_F,\n\\]\nwhich proves that the truncated SVD \\(X_k\\) is the best rank-\\(k\\) approximation of \\(X\\) in Frobenius norm.\n□",
    "crumbs": [
      "Data Science Applications",
      "Principal Component Analysis (PCA): A Spectral and SVD-Based Application"
    ]
  },
  {
    "objectID": "chapters/06-pca-application.html#computational-example-python",
    "href": "chapters/06-pca-application.html#computational-example-python",
    "title": "Principal Component Analysis (PCA): A Spectral and SVD-Based Application",
    "section": "8 Computational Example (Python)",
    "text": "8 Computational Example (Python)\nimport numpy as np\n\nrng = np.random.default_rng(0)\nX = rng.normal(size=(300, 3))\nX[:, 2] = X[:, 0] + 0.1 * rng.normal(size=300)\n\n# center data\nX = X - X.mean(axis=0)\n\nU, s, Vt = np.linalg.svd(X, full_matrices=False)\n\nexplained_variance = s**2 / np.sum(s**2)\nexplained_variance",
    "crumbs": [
      "Data Science Applications",
      "Principal Component Analysis (PCA): A Spectral and SVD-Based Application"
    ]
  },
  {
    "objectID": "chapters/06-pca-application.html#explained-variance-ratio",
    "href": "chapters/06-pca-application.html#explained-variance-ratio",
    "title": "Principal Component Analysis (PCA): A Spectral and SVD-Based Application",
    "section": "9 Explained Variance Ratio",
    "text": "9 Explained Variance Ratio\nThe proportion of variance explained by the \\(k\\)-th principal component is\n\\[\n\\frac{\\sigma_k^2}{\\sum_i \\sigma_i^2}.\n\\]\nThis quantity is used in practice to choose the number of components.",
    "crumbs": [
      "Data Science Applications",
      "Principal Component Analysis (PCA): A Spectral and SVD-Based Application"
    ]
  },
  {
    "objectID": "chapters/06-pca-application.html#geometric-interpretation",
    "href": "chapters/06-pca-application.html#geometric-interpretation",
    "title": "Principal Component Analysis (PCA): A Spectral and SVD-Based Application",
    "section": "10 Geometric Interpretation",
    "text": "10 Geometric Interpretation\n\nPCA finds orthogonal directions of maximal variance.\nData is projected onto a lower-dimensional linear subspace.\nNoise typically concentrates in directions with small singular values.\n\nGeometrically, PCA fits a best affine subspace to the data.",
    "crumbs": [
      "Data Science Applications",
      "Principal Component Analysis (PCA): A Spectral and SVD-Based Application"
    ]
  },
  {
    "objectID": "chapters/06-pca-application.html#when-pca-works-well-and-when-it-does-not",
    "href": "chapters/06-pca-application.html#when-pca-works-well-and-when-it-does-not",
    "title": "Principal Component Analysis (PCA): A Spectral and SVD-Based Application",
    "section": "11 When PCA Works Well (and When It Does Not)",
    "text": "11 When PCA Works Well (and When It Does Not)\n\n11.1 Works well when:\n\ndata lies near a linear subspace,\nnoise is approximately isotropic,\nvariance is a meaningful notion of information.\n\n\n\n11.2 Limitations:\n\nsensitive to outliers,\ncaptures only linear structure,\ndepends on feature scaling.",
    "crumbs": [
      "Data Science Applications",
      "Principal Component Analysis (PCA): A Spectral and SVD-Based Application"
    ]
  },
  {
    "objectID": "chapters/06-pca-application.html#connections-to-other-methods",
    "href": "chapters/06-pca-application.html#connections-to-other-methods",
    "title": "Principal Component Analysis (PCA): A Spectral and SVD-Based Application",
    "section": "12 Connections to Other Methods",
    "text": "12 Connections to Other Methods\n\nTruncated SVD: computational backbone of PCA\n\nKernel PCA: nonlinear extension using kernels\n\nAutoencoders: nonlinear generalization of PCA\n\nFactor Analysis: probabilistic variant",
    "crumbs": [
      "Data Science Applications",
      "Principal Component Analysis (PCA): A Spectral and SVD-Based Application"
    ]
  },
  {
    "objectID": "chapters/06-pca-application.html#summary",
    "href": "chapters/06-pca-application.html#summary",
    "title": "Principal Component Analysis (PCA): A Spectral and SVD-Based Application",
    "section": "13 Summary",
    "text": "13 Summary\n\nPCA is an application of spectral theory and SVD.\nPrincipal components are eigenvectors of the covariance matrix.\nSVD provides a stable and efficient way to compute PCA.\nPCA yields optimal low-rank approximations.\nUnderstanding PCA requires understanding linear algebra, not just algorithms.",
    "crumbs": [
      "Data Science Applications",
      "Principal Component Analysis (PCA): A Spectral and SVD-Based Application"
    ]
  },
  {
    "objectID": "chapters/06-pca-application.html#preview-regularization-and-truncated-svd",
    "href": "chapters/06-pca-application.html#preview-regularization-and-truncated-svd",
    "title": "Principal Component Analysis (PCA): A Spectral and SVD-Based Application",
    "section": "14 Preview: Regularization and Truncated SVD",
    "text": "14 Preview: Regularization and Truncated SVD\nSmall singular values amplify noise. In the next application chapter, we study:\n\ntruncated SVD as a denoising method,\nridge regression as spectral regularization,\nbias–variance trade-offs.",
    "crumbs": [
      "Data Science Applications",
      "Principal Component Analysis (PCA): A Spectral and SVD-Based Application"
    ]
  },
  {
    "objectID": "chapters/02-linear-maps.html",
    "href": "chapters/02-linear-maps.html",
    "title": "Linear Maps and Matrices",
    "section": "",
    "text": "Linear maps formalize the idea of transforming vectors in a linear way. They are the mathematical objects behind:\n\nfeature transformations,\nlinear models,\ndimensionality reduction,\nneural network layers (locally).\n\nIn practice, linear maps are represented by matrices.",
    "crumbs": [
      "Foundations",
      "Linear Maps and Matrices"
    ]
  },
  {
    "objectID": "chapters/02-linear-maps.html#motivation",
    "href": "chapters/02-linear-maps.html#motivation",
    "title": "Linear Maps and Matrices",
    "section": "",
    "text": "Linear maps formalize the idea of transforming vectors in a linear way. They are the mathematical objects behind:\n\nfeature transformations,\nlinear models,\ndimensionality reduction,\nneural network layers (locally).\n\nIn practice, linear maps are represented by matrices.",
    "crumbs": [
      "Foundations",
      "Linear Maps and Matrices"
    ]
  },
  {
    "objectID": "chapters/02-linear-maps.html#linear-maps",
    "href": "chapters/02-linear-maps.html#linear-maps",
    "title": "Linear Maps and Matrices",
    "section": "2 Linear Maps",
    "text": "2 Linear Maps\n\n\nDefinition (Linear map)\n\nLet \\(V\\) and \\(W\\) be vector spaces over \\(\\mathbb{R}\\). A function \\(T : V \\to W\\) is a linear map if for all \\(u,v \\in V\\) and \\(\\alpha \\in \\mathbb{R}\\):\n\n\\(T(u+v) = T(u) + T(v)\\)\n\n\\(T(\\alpha u) = \\alpha T(u)\\)\n\n\nEquivalently: \\[\nT(\\alpha u + \\beta v) = \\alpha T(u) + \\beta T(v).\n\\]",
    "crumbs": [
      "Foundations",
      "Linear Maps and Matrices"
    ]
  },
  {
    "objectID": "chapters/02-linear-maps.html#examples-of-linear-maps",
    "href": "chapters/02-linear-maps.html#examples-of-linear-maps",
    "title": "Linear Maps and Matrices",
    "section": "3 Examples of Linear Maps",
    "text": "3 Examples of Linear Maps\n\n\nExample (Matrix multiplication)\n\nLet \\(A \\in \\mathbb{R}^{m \\times n}\\).\nThe map \\(T(x) = Ax\\) from \\(\\mathbb{R}^n\\) to \\(\\mathbb{R}^m\\) is linear.\n\n\n\nExample (Projection)\n\nThe map projecting vectors in \\(\\mathbb{R}^2\\) onto the \\(x\\)-axis is linear.\n\n\n\nExample (Differentiation)\n\nThe map \\(T(f) = f'\\) on the space of polynomials is linear.",
    "crumbs": [
      "Foundations",
      "Linear Maps and Matrices"
    ]
  },
  {
    "objectID": "chapters/02-linear-maps.html#kernel-and-image",
    "href": "chapters/02-linear-maps.html#kernel-and-image",
    "title": "Linear Maps and Matrices",
    "section": "4 Kernel and Image",
    "text": "4 Kernel and Image\n\n\nDefinition (Kernel)\n\nThe kernel of a linear map \\(T : V \\to W\\) is: \\[\n\\ker(T) := \\{v \\in V : T(v) = 0\\}.\n\\]\n\n\n\nDefinition (Image)\n\nThe image of \\(T\\) is: \\[\n\\mathrm{Im}(T) := \\{T(v) : v \\in V\\}.\n\\]",
    "crumbs": [
      "Foundations",
      "Linear Maps and Matrices"
    ]
  },
  {
    "objectID": "chapters/02-linear-maps.html#ranknullity-theorem",
    "href": "chapters/02-linear-maps.html#ranknullity-theorem",
    "title": "Linear Maps and Matrices",
    "section": "5 Rank–Nullity Theorem",
    "text": "5 Rank–Nullity Theorem\n\n\nTheorem (Rank–Nullity)\n\nLet \\(T:V\\to W\\) be a linear map with \\(V\\) finite-dimensional. Then:\n\\[\n\\dim(V) = \\dim(\\ker T) + \\dim(\\mathrm{Im}\\, T).\n\\]\n\n\n\nProof (click to expand)\n\n\nProof. \n\nProof\n\nLet \\(\\{k_1,\\dots,k_r\\}\\) be a basis of \\(\\ker(T)\\), so \\(r=\\dim(\\ker T)\\).\nExtend this basis to a basis of \\(V\\): \\[\n\\{k_1,\\dots,k_r, v_{r+1},\\dots,v_n\\},\n\\] where \\(n=\\dim(V)\\) (this extension is always possible in finite-dimensional spaces).\nWe claim that \\(\\{T(v_{r+1}),\\dots,T(v_n)\\}\\) is a basis of \\(\\mathrm{Im}(T)\\).\n(1) Spanning.\nTake any \\(y\\in\\mathrm{Im}(T)\\). Then \\(y=T(v)\\) for some \\(v\\in V\\). Write \\(v\\) in the chosen basis: \\[\nv=\\sum_{i=1}^r a_i k_i + \\sum_{j=r+1}^n b_j v_j.\n\\] By linearity and the fact that \\(T(k_i)=0\\) for all \\(i\\), \\[\nT(v)=\\sum_{i=1}^r a_i T(k_i) + \\sum_{j=r+1}^n b_j T(v_j)\n= \\sum_{j=r+1}^n b_j T(v_j).\n\\] Hence \\(y\\) is in the span of \\(\\{T(v_{r+1}),\\dots,T(v_n)\\}\\).\n(2) Linear independence.\nAssume \\[\n\\sum_{j=r+1}^n c_j\\,T(v_j)=0.\n\\] Then \\[\nT\\Big(\\sum_{j=r+1}^n c_j v_j\\Big)=0,\n\\] so \\(\\sum_{j=r+1}^n c_j v_j \\in \\ker(T)\\). Therefore there exist scalars \\(\\alpha_1,\\dots,\\alpha_r\\) such that \\[\n\\sum_{j=r+1}^n c_j v_j = \\sum_{i=1}^r \\alpha_i k_i.\n\\] Rearranging gives \\[\n\\sum_{i=1}^r (-\\alpha_i)k_i + \\sum_{j=r+1}^n c_j v_j = 0.\n\\] But \\(\\{k_1,\\dots,k_r, v_{r+1},\\dots,v_n\\}\\) is a basis of \\(V\\), hence linearly independent, so all coefficients must be zero. In particular, \\(c_j=0\\) for every \\(j=r+1,\\dots,n\\). Thus \\(\\{T(v_{r+1}),\\dots,T(v_n)\\}\\) is linearly independent.\nSo \\(\\{T(v_{r+1}),\\dots,T(v_n)\\}\\) is a basis of \\(\\mathrm{Im}(T)\\) and has \\(n-r\\) elements, meaning \\[\n\\dim(\\mathrm{Im}(T)) = n-r.\n\\] Therefore \\[\n\\dim(V)=n=r+(n-r)=\\dim(\\ker T)+\\dim(\\mathrm{Im}(T)).\n\\]\n□\n\n\nThis theorem explains the trade-off between: - degrees of freedom lost (kernel), - information preserved (image).",
    "crumbs": [
      "Foundations",
      "Linear Maps and Matrices"
    ]
  },
  {
    "objectID": "chapters/02-linear-maps.html#matrices-as-representations-of-linear-maps",
    "href": "chapters/02-linear-maps.html#matrices-as-representations-of-linear-maps",
    "title": "Linear Maps and Matrices",
    "section": "6 Matrices as Representations of Linear Maps",
    "text": "6 Matrices as Representations of Linear Maps\nLet \\(T : \\mathbb{R}^n \\to \\mathbb{R}^m\\) be linear. There exists a unique matrix \\(A \\in \\mathbb{R}^{m \\times n}\\) such that: \\[\nT(x) = Ax.\n\\]\nThe columns of \\(A\\) are the images of the canonical basis vectors.",
    "crumbs": [
      "Foundations",
      "Linear Maps and Matrices"
    ]
  },
  {
    "objectID": "chapters/02-linear-maps.html#change-of-basis",
    "href": "chapters/02-linear-maps.html#change-of-basis",
    "title": "Linear Maps and Matrices",
    "section": "7 Change of Basis",
    "text": "7 Change of Basis\n\n\nDefinition (Change of basis)\n\nLet \\(B\\) and \\(C\\) be bases of \\(V\\) and \\(W\\), respectively. The matrix of \\(T\\) depends on the chosen bases.\nDifferent bases yield different matrices, but the same linear map.\n\nThis explains why feature scaling and basis choice matter in practice.",
    "crumbs": [
      "Foundations",
      "Linear Maps and Matrices"
    ]
  },
  {
    "objectID": "chapters/02-linear-maps.html#geometry-of-linear-maps",
    "href": "chapters/02-linear-maps.html#geometry-of-linear-maps",
    "title": "Linear Maps and Matrices",
    "section": "8 Geometry of Linear Maps",
    "text": "8 Geometry of Linear Maps\nLinear maps can: - rotate, - reflect, - scale, - shear, - project.\nAll linear models are combinations of these effects.",
    "crumbs": [
      "Foundations",
      "Linear Maps and Matrices"
    ]
  },
  {
    "objectID": "chapters/02-linear-maps.html#computational-example-python",
    "href": "chapters/02-linear-maps.html#computational-example-python",
    "title": "Linear Maps and Matrices",
    "section": "9 Computational Example (Python)",
    "text": "9 Computational Example (Python)\nimport numpy as np\n\nA = np.array([[1, 2],\n              [0, 1]])\n\nx = np.array([1, 1])\n\nA @ x\nThe matrix \\(A\\) maps the vector \\(x\\) to a new vector in \\(\\mathbb{R}^2\\).",
    "crumbs": [
      "Foundations",
      "Linear Maps and Matrices"
    ]
  },
  {
    "objectID": "chapters/02-linear-maps.html#rank-and-data",
    "href": "chapters/02-linear-maps.html#rank-and-data",
    "title": "Linear Maps and Matrices",
    "section": "10 Rank and Data",
    "text": "10 Rank and Data\nnp.linalg.matrix_rank(A)\n\nThe rank measures the dimension of the image.\nLow rank implies redundancy or collinearity.",
    "crumbs": [
      "Foundations",
      "Linear Maps and Matrices"
    ]
  },
  {
    "objectID": "chapters/02-linear-maps.html#linear-models-in-data-science",
    "href": "chapters/02-linear-maps.html#linear-models-in-data-science",
    "title": "Linear Maps and Matrices",
    "section": "11 Linear Models in Data Science",
    "text": "11 Linear Models in Data Science\nA linear regression model is:\n\\[\ny = Xw\n\\]\nwhere: - rows of \\(X\\) are data points, - columns of \\(X\\) are features, - \\(w\\) is a parameter vector.\nPrediction is simply applying a linear map.",
    "crumbs": [
      "Foundations",
      "Linear Maps and Matrices"
    ]
  },
  {
    "objectID": "chapters/02-linear-maps.html#preview-least-squares",
    "href": "chapters/02-linear-maps.html#preview-least-squares",
    "title": "Linear Maps and Matrices",
    "section": "12 Preview: Least Squares",
    "text": "12 Preview: Least Squares\nWhen \\(X\\) is not invertible, we cannot solve \\(Xw = y\\) exactly. Instead, we solve:\n\\[\n\\min_{w \\in \\mathbb{R}^n} \\|Xw - y\\|_2^2\n\\]\nThis is a geometric projection problem, studied in the next chapter.",
    "crumbs": [
      "Foundations",
      "Linear Maps and Matrices"
    ]
  },
  {
    "objectID": "chapters/02-linear-maps.html#summary",
    "href": "chapters/02-linear-maps.html#summary",
    "title": "Linear Maps and Matrices",
    "section": "13 Summary",
    "text": "13 Summary\n\nLinear maps preserve vector space structure.\nMatrices represent linear maps.\nKernel and image explain information loss.\nRank measures effective dimensionality.\nLinear models are applications of linear maps.",
    "crumbs": [
      "Foundations",
      "Linear Maps and Matrices"
    ]
  },
  {
    "objectID": "chapters/100A-appendix.html",
    "href": "chapters/100A-appendix.html",
    "title": "Appendix: Computational Cost, Complexity, and Counting Operations",
    "section": "",
    "text": "Throughout this course we will repeatedly say things like “this algorithm costs \\(\\mathcal{O}(n^2)\\)” or “this factorization costs \\(\\mathcal{O}(n^3)\\)”.\nThis addendum makes those statements precise by introducing:\n\nwhat we mean by computational cost,\nhow to count floating-point operations (flops),\nasymptotic notation (\\(\\mathcal{O}\\), \\(\\Theta\\), \\(\\Omega\\)),\ntypical cost patterns in linear algebra.\n\nThe goal is to be able to justify the complexity claims used in chapters on LU, Cholesky, QR, SVD, least squares, etc.",
    "crumbs": [
      "Appendix",
      "Appendix: Computational Cost, Complexity, and Counting Operations"
    ]
  },
  {
    "objectID": "chapters/100A-appendix.html#motivation",
    "href": "chapters/100A-appendix.html#motivation",
    "title": "Appendix: Computational Cost, Complexity, and Counting Operations",
    "section": "",
    "text": "Throughout this course we will repeatedly say things like “this algorithm costs \\(\\mathcal{O}(n^2)\\)” or “this factorization costs \\(\\mathcal{O}(n^3)\\)”.\nThis addendum makes those statements precise by introducing:\n\nwhat we mean by computational cost,\nhow to count floating-point operations (flops),\nasymptotic notation (\\(\\mathcal{O}\\), \\(\\Theta\\), \\(\\Omega\\)),\ntypical cost patterns in linear algebra.\n\nThe goal is to be able to justify the complexity claims used in chapters on LU, Cholesky, QR, SVD, least squares, etc.",
    "crumbs": [
      "Appendix",
      "Appendix: Computational Cost, Complexity, and Counting Operations"
    ]
  },
  {
    "objectID": "chapters/100A-appendix.html#models-of-computation",
    "href": "chapters/100A-appendix.html#models-of-computation",
    "title": "Appendix: Computational Cost, Complexity, and Counting Operations",
    "section": "2 1. Models of Computation",
    "text": "2 1. Models of Computation\n\n2.1 1.1 Floating-point operation (flop)\n\n\nDefinition (Flop)\n\nA floating-point operation (flop) is typically counted as one basic arithmetic operation: addition, subtraction, multiplication, or division performed on floating-point numbers.\n\nIn numerical linear algebra, flop counts are a standard way to compare algorithms.\n\nRemark. \n\nRemark\n\nDifferent hardware may execute additions and multiplications at different speeds, and memory access can dominate runtime. Still, flop counts are a useful first-order approximation and correlate well with practical runtime for dense computations.\n\n\n\n\n2.2 1.2 Dense vs. sparse cost\n\n\nDefinition (Dense vs. sparse)\n\nA matrix is dense if most entries are nonzero (or treated as such).\nA matrix is sparse if most entries are zero and we exploit that structure to save work.\n\nIn this addendum we focus on dense costs, which are the classical “textbook” complexities.",
    "crumbs": [
      "Appendix",
      "Appendix: Computational Cost, Complexity, and Counting Operations"
    ]
  },
  {
    "objectID": "chapters/100A-appendix.html#asymptotic-notation",
    "href": "chapters/100A-appendix.html#asymptotic-notation",
    "title": "Appendix: Computational Cost, Complexity, and Counting Operations",
    "section": "3 2. Asymptotic Notation",
    "text": "3 2. Asymptotic Notation\n\n3.1 2.1 Big-O\n\n\nDefinition (Big-O)\n\nLet \\(f,g:\\mathbb{N}\\to \\mathbb{R}_{\\ge 0}\\). We say \\(f(n)=\\mathcal{O}(g(n))\\) if there exist constants \\(C&gt;0\\) and \\(n_0\\) such that for all \\(n\\ge n_0\\),\n\\[\nf(n) \\le C\\,g(n).\n\\]\n\nIntuition: \\(f\\) grows at most on the order of \\(g\\).\n\n\n\n3.2 2.2 Big-Omega and Theta\n\n\nDefinition (Big-\\(\\Omega\\))\n\n\\(f(n)=\\Omega(g(n))\\) if there exist \\(c&gt;0\\) and \\(n_0\\) such that for all \\(n\\ge n_0\\),\n\\[\nf(n) \\ge c\\,g(n).\n\\]\n\n\n\nDefinition (Big-\\(\\Theta\\))\n\n\\(f(n)=\\Theta(g(n))\\) if \\(f(n)=\\mathcal{O}(g(n))\\) and \\(f(n)=\\Omega(g(n))\\).\n\nSo \\(\\Theta\\) means “same growth rate up to constants.”\n\n\n\n3.3 2.3 Practical rule: keep the leading term\nIn dense linear algebra, costs often look like:\n\\[\nf(n)=\\frac{1}{3}n^3 + 2n^2 + 7n.\n\\]\nAs \\(n\\to\\infty\\), the \\(n^3\\) term dominates, so \\(f(n)=\\Theta(n^3)\\).",
    "crumbs": [
      "Appendix",
      "Appendix: Computational Cost, Complexity, and Counting Operations"
    ]
  },
  {
    "objectID": "chapters/100A-appendix.html#counting-operations-core-patterns",
    "href": "chapters/100A-appendix.html#counting-operations-core-patterns",
    "title": "Appendix: Computational Cost, Complexity, and Counting Operations",
    "section": "4 3. Counting Operations: Core Patterns",
    "text": "4 3. Counting Operations: Core Patterns\n\n4.1 3.1 Summation growth\n\n\nProposition\n\nThe following sums satisfy:\n\\[\n\\sum_{k=1}^n k = \\frac{n(n+1)}{2} = \\Theta(n^2),\n\\qquad\n\\sum_{k=1}^n k^2 = \\frac{n(n+1)(2n+1)}{6} = \\Theta(n^3).\n\\]\n\nThese two formulas are the backbone of most complexity derivations.\n\n\nProof of the sum formulas (click to expand)\n\n\nProof. \n\nProof (sketch)\n\nThe first identity is classical: pair terms \\((1+n),(2+(n-1)),\\dots\\) to get \\(\\frac{n}{2}(n+1)\\) (or use induction).\nFor the second, one can use induction or the telescoping identity \\((k+1)^3-k^3=3k^2+3k+1\\) and sum both sides from \\(k=1\\) to \\(n\\). □\n\n\n\n\n\n4.2 3.2 Dot product cost\n\n\nProposition (Dot product)\n\nComputing the dot product of two vectors \\(x,y\\in\\mathbb{R}^n\\):\n\\[\nx^\\top y = \\sum_{i=1}^n x_i y_i\n\\]\nrequires \\(n\\) multiplications and \\((n-1)\\) additions, hence \\(\\Theta(n)\\) flops.\n\n\n\nExample: explicit flop count for a dot product\n\n\n\nExample\n\nTo compute \\(x^\\top y\\): - multiply \\(x_i y_i\\) for \\(i=1,\\dots,n\\) → \\(n\\) multiplications, - sum them → \\((n-1)\\) additions.\nTotal \\(\\approx 2n\\) flops, i.e. \\(\\Theta(n)\\).\n\n\n\n\n\n4.3 3.3 Matrix–vector multiplication\n\n\nProposition (Matrix–vector product)\n\nComputing \\(Ax\\) with \\(A\\in\\mathbb{R}^{m\\times n}\\) and \\(x\\in\\mathbb{R}^n\\) costs \\(\\Theta(mn)\\) flops.\n\nReason: \\(Ax\\) produces \\(m\\) dot products of length \\(n\\).\n\n\nExample: flop count for \\(Ax\\)\n\n\n\nExample\n\nEach entry of \\(Ax\\) is \\[\n(Ax)_i = \\sum_{j=1}^n a_{ij}x_j,\n\\] a dot product of length \\(n\\) → \\(\\approx 2n\\) flops.\nThere are \\(m\\) rows → \\(\\approx 2mn\\) flops, hence \\(\\Theta(mn)\\).\n\n\n\n\n\n4.4 3.4 Matrix–matrix multiplication (naïve)\n\n\nProposition (Matrix–matrix product)\n\nThe standard algorithm to compute \\(C=AB\\) with \\(A\\in\\mathbb{R}^{m\\times n}\\) and \\(B\\in\\mathbb{R}^{n\\times p}\\) costs \\(\\Theta(mnp)\\) flops.\n\nEach entry \\(c_{ij}\\) is a dot product of length \\(n\\).\n\n\nExample: flop count for \\(AB\\)\n\n\n\nExample\n\nThere are \\(mp\\) entries in \\(C\\).\nEach entry is \\[\nc_{ij}=\\sum_{k=1}^n a_{ik}b_{kj},\n\\] costing \\(\\approx 2n\\) flops.\nTotal \\(\\approx 2mnp\\) flops → \\(\\Theta(mnp)\\).",
    "crumbs": [
      "Appendix",
      "Appendix: Computational Cost, Complexity, and Counting Operations"
    ]
  },
  {
    "objectID": "chapters/100A-appendix.html#triangular-systems-forward-and-back-substitution",
    "href": "chapters/100A-appendix.html#triangular-systems-forward-and-back-substitution",
    "title": "Appendix: Computational Cost, Complexity, and Counting Operations",
    "section": "5 4. Triangular Systems: Forward and Back Substitution",
    "text": "5 4. Triangular Systems: Forward and Back Substitution\nTriangular solves are fundamental because LU and Cholesky reduce general solves to them.\n\n5.1 4.1 Forward substitution (lower triangular)\nLet \\(L\\in\\mathbb{R}^{n\\times n}\\) be lower triangular with nonzero diagonal.\nSolve\n\\[\nLy = b.\n\\]\nThe equations are:\n\\[\n\\ell_{11}y_1=b_1,\n\\qquad\n\\ell_{21}y_1+\\ell_{22}y_2=b_2,\n\\quad \\dots \\quad\n\\sum_{j=1}^i \\ell_{ij}y_j=b_i.\n\\]\nSo we compute \\(y_1\\) first, then \\(y_2\\), etc.\n\n\nProposition (Cost of forward substitution)\n\nSolving a lower triangular system \\(Ly=b\\) by forward substitution costs \\(\\Theta(n^2)\\) flops.\n\n\n\nProof and explicit cost calculation (click to expand)\n\n\nProof. \n\nProof\n\nFor row \\(i\\), we compute\n\\[\ny_i=\\frac{1}{\\ell_{ii}}\\left(b_i-\\sum_{j=1}^{i-1}\\ell_{ij}y_j\\right).\n\\]\nThe inner sum has \\((i-1)\\) multiplications and \\((i-2)\\) additions, plus one subtraction and one division. Up to constants, row \\(i\\) costs \\(\\Theta(i)\\) flops.\nTherefore, total cost is\n\\[\n\\sum_{i=1}^n \\Theta(i) = \\Theta\\!\\left(\\sum_{i=1}^n i\\right)=\\Theta(n^2).\n\\]\n□\n\n\n\n\n\n5.2 4.2 Back substitution (upper triangular)\nLet \\(U\\in\\mathbb{R}^{n\\times n}\\) be upper triangular with nonzero diagonal.\nSolve\n\\[\nUx = y.\n\\]\nWe compute \\(x_n\\) first, then \\(x_{n-1}\\), etc.\n\n\nProposition (Cost of back substitution)\n\nSolving an upper triangular system \\(Ux=y\\) by back substitution costs \\(\\Theta(n^2)\\) flops.\n\n\n\nProof and explicit cost calculation (click to expand)\n\n\nProof. \n\nProof\n\nFor row \\(i\\) (counting from the bottom), we compute\n\\[\nx_i=\\frac{1}{u_{ii}}\\left(y_i-\\sum_{j=i+1}^{n}u_{ij}x_j\\right).\n\\]\nThe sum has \\((n-i)\\) multiplications and \\((n-i-1)\\) additions, plus one subtraction and one division. So row \\(i\\) costs \\(\\Theta(n-i)\\) flops.\nTotal:\n\\[\n\\sum_{i=1}^n \\Theta(n-i)=\\Theta\\!\\left(\\sum_{k=0}^{n-1}k\\right)=\\Theta(n^2).\n\\]\n□",
    "crumbs": [
      "Appendix",
      "Appendix: Computational Cost, Complexity, and Counting Operations"
    ]
  },
  {
    "objectID": "chapters/100A-appendix.html#lu-factorization-cost-dense",
    "href": "chapters/100A-appendix.html#lu-factorization-cost-dense",
    "title": "Appendix: Computational Cost, Complexity, and Counting Operations",
    "section": "6 5. LU Factorization Cost (Dense)",
    "text": "6 5. LU Factorization Cost (Dense)\nLU factorization itself costs \\(\\Theta(n^3)\\) flops (for dense matrices), but once computed, each solve costs only \\(\\Theta(n^2)\\).\n\n\nProposition (LU factorization cost, informal)\n\nFor dense \\(A\\in\\mathbb{R}^{n\\times n}\\), computing an LU factorization costs \\(\\Theta(n^3)\\) flops. After factorization, each solve \\(Ax=b\\) costs \\(\\Theta(n^2)\\) flops.\n\n\n\nWhy LU costs \\(\\Theta(n^3)\\) (click to expand)\n\n\nProof. \n\nProof (counting idea)\n\nGaussian elimination proceeds column by column. At elimination step \\(k\\), we update the trailing submatrix of size \\((n-k)\\times(n-k)\\). This is roughly a rank-1 update costing \\(\\Theta((n-k)^2)\\) operations.\nHence total elimination cost scales like\n\\[\n\\sum_{k=1}^{n-1} (n-k)^2 = \\sum_{s=1}^{n-1} s^2 = \\Theta(n^3).\n\\]\nMore precise counting gives approximately \\(\\frac{2}{3}n^3\\) flops for the elimination part. □",
    "crumbs": [
      "Appendix",
      "Appendix: Computational Cost, Complexity, and Counting Operations"
    ]
  },
  {
    "objectID": "chapters/100A-appendix.html#cholesky-factorization-cost-dense",
    "href": "chapters/100A-appendix.html#cholesky-factorization-cost-dense",
    "title": "Appendix: Computational Cost, Complexity, and Counting Operations",
    "section": "7 6. Cholesky Factorization Cost (Dense)",
    "text": "7 6. Cholesky Factorization Cost (Dense)\nCholesky exploits symmetry and costs about half of LU.\n\n\nProposition (Cholesky factorization cost, informal)\n\nFor dense symmetric positive definite \\(A\\in\\mathbb{R}^{n\\times n}\\), computing \\(A=LL^\\top\\) costs \\(\\Theta(n^3)\\) flops, with a leading constant about half of LU.\n\n\n\nWhy Cholesky is roughly half of LU (click to expand)\n\n\nProof. \n\nProof (counting idea)\n\nCholesky only computes the lower triangular factor \\(L\\), and symmetry means we do not process the entire matrix independently. The work at step \\(k\\) updates a trailing submatrix but only its lower-triangular part, reducing the constant factor by about \\(1/2\\).\nA standard flop estimate is approximately \\(\\frac{1}{3}n^3\\) flops. □",
    "crumbs": [
      "Appendix",
      "Appendix: Computational Cost, Complexity, and Counting Operations"
    ]
  },
  {
    "objectID": "chapters/100A-appendix.html#why-constants-matter-but-not-too-much",
    "href": "chapters/100A-appendix.html#why-constants-matter-but-not-too-much",
    "title": "Appendix: Computational Cost, Complexity, and Counting Operations",
    "section": "8 7. Why Constants Matter (but not too much)",
    "text": "8 7. Why Constants Matter (but not too much)\nTwo algorithms can both be \\(\\Theta(n^3)\\) but differ significantly in practice:\n\nLU: about \\(\\frac{2}{3}n^3\\) flops\nCholesky: about \\(\\frac{1}{3}n^3\\) flops\n\nSo Cholesky is often close to 2× faster when applicable.",
    "crumbs": [
      "Appendix",
      "Appendix: Computational Cost, Complexity, and Counting Operations"
    ]
  },
  {
    "objectID": "chapters/100A-appendix.html#worked-example-triangular-solve-by-hand",
    "href": "chapters/100A-appendix.html#worked-example-triangular-solve-by-hand",
    "title": "Appendix: Computational Cost, Complexity, and Counting Operations",
    "section": "9 8. Worked Example: Triangular Solve by Hand",
    "text": "9 8. Worked Example: Triangular Solve by Hand\n\n\nExample: forward substitution step-by-step (click to expand)\n\n\n\nExample\n\nSolve \\(Ly=b\\) where\n\\[\nL=\\begin{pmatrix}\n2 & 0 & 0\\\\\n-1 & 3 & 0\\\\\n4 & 2 & 1\n\\end{pmatrix},\n\\qquad\nb=\\begin{pmatrix}\n2\\\\\n5\\\\\n1\n\\end{pmatrix}.\n\\]\nRow 1: \\[\n2y_1 = 2 \\;\\Rightarrow\\; y_1=1.\n\\]\nRow 2: \\[\n-1\\cdot y_1 + 3y_2 = 5\n\\;\\Rightarrow\\;\n-1 + 3y_2 = 5\n\\;\\Rightarrow\\;\ny_2 = 2.\n\\]\nRow 3: \\[\n4y_1 + 2y_2 + 1\\cdot y_3 = 1\n\\;\\Rightarrow\\;\n4 + 4 + y_3 = 1\n\\;\\Rightarrow\\;\ny_3=-7.\n\\]\nSo\n\\[\ny=\\begin{pmatrix}1\\\\2\\\\-7\\end{pmatrix}.\n\\]",
    "crumbs": [
      "Appendix",
      "Appendix: Computational Cost, Complexity, and Counting Operations"
    ]
  },
  {
    "objectID": "chapters/100A-appendix.html#summary-checklist",
    "href": "chapters/100A-appendix.html#summary-checklist",
    "title": "Appendix: Computational Cost, Complexity, and Counting Operations",
    "section": "10 9. Summary Checklist",
    "text": "10 9. Summary Checklist\n\nUse Big-O / Theta to describe growth rates.\nDot products cost \\(\\Theta(n)\\).\nMatrix–vector products cost \\(\\Theta(mn)\\).\nTriangular solves cost \\(\\Theta(n^2)\\).\nDense LU factorization costs \\(\\Theta(n^3)\\).\nDense Cholesky costs \\(\\Theta(n^3)\\) with a smaller constant.\nOnce a factorization is computed, solving for many right-hand sides is cheap.",
    "crumbs": [
      "Appendix",
      "Appendix: Computational Cost, Complexity, and Counting Operations"
    ]
  },
  {
    "objectID": "chapters/100A-appendix.html#exercises-optional",
    "href": "chapters/100A-appendix.html#exercises-optional",
    "title": "Appendix: Computational Cost, Complexity, and Counting Operations",
    "section": "11 Exercises (Optional)",
    "text": "11 Exercises (Optional)\n\n\nExercise 1: compute a flop estimate (click to expand)\n\n\n\nExercise\n\nSuppose you solve \\(Ax=b\\) for \\(100\\) different vectors \\(b\\).\n\nIf you compute LU once, what is the total cost as a function of \\(n\\)?\nCompare it with computing \\(A^{-1}\\) explicitly and multiplying.\n\nGive the result using \\(\\Theta(\\cdot)\\) notation.\n\n\n\n\nExercise 2: rank-1 update cost (click to expand)\n\n\n\nExercise\n\nShow that updating a dense \\((n-k)\\times(n-k)\\) matrix by\n\\[\nB \\leftarrow B - uv^\\top\n\\]\ncosts \\(\\Theta((n-k)^2)\\) flops.\n(Hint: each entry of \\(B\\) is updated by one multiplication and one subtraction.)",
    "crumbs": [
      "Appendix",
      "Appendix: Computational Cost, Complexity, and Counting Operations"
    ]
  },
  {
    "objectID": "chapters/05-svd.html",
    "href": "chapters/05-svd.html",
    "title": "Singular Value Decomposition (SVD)",
    "section": "",
    "text": "The Singular Value Decomposition (SVD) is the most important matrix factorization in modern applied linear algebra.\nUnlike eigenvalue decompositions, SVD:\n\napplies to any matrix (square or rectangular),\nis numerically stable,\nreveals intrinsic dimensionality,\nprovides optimal low-rank approximations.\n\nIn Data Science, SVD underlies: - PCA, - dimensionality reduction, - data compression, - recommender systems, - latent semantic analysis.",
    "crumbs": [
      "Spectral theory & decompositions",
      "Singular Value Decomposition (SVD)"
    ]
  },
  {
    "objectID": "chapters/05-svd.html#motivation",
    "href": "chapters/05-svd.html#motivation",
    "title": "Singular Value Decomposition (SVD)",
    "section": "",
    "text": "The Singular Value Decomposition (SVD) is the most important matrix factorization in modern applied linear algebra.\nUnlike eigenvalue decompositions, SVD:\n\napplies to any matrix (square or rectangular),\nis numerically stable,\nreveals intrinsic dimensionality,\nprovides optimal low-rank approximations.\n\nIn Data Science, SVD underlies: - PCA, - dimensionality reduction, - data compression, - recommender systems, - latent semantic analysis.",
    "crumbs": [
      "Spectral theory & decompositions",
      "Singular Value Decomposition (SVD)"
    ]
  },
  {
    "objectID": "chapters/05-svd.html#statement-of-the-svd",
    "href": "chapters/05-svd.html#statement-of-the-svd",
    "title": "Singular Value Decomposition (SVD)",
    "section": "2 Statement of the SVD",
    "text": "2 Statement of the SVD\n\n\nTheorem (Singular Value Decomposition)\n\nLet \\(A \\in \\mathbb{R}^{m \\times n}\\). There exist orthogonal matrices\n\\[\nU \\in \\mathbb{R}^{m \\times m}, \\qquad\nV \\in \\mathbb{R}^{n \\times n},\n\\]\nand a diagonal matrix\n\\[\n\\Sigma \\in \\mathbb{R}^{m \\times n},\n\\]\nwith nonnegative diagonal entries\n\\[\n\\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge \\sigma_r &gt; 0,\n\\]\nsuch that\n\\[\nA = U \\Sigma V^\\top.\n\\]\nThe numbers \\(\\sigma_i\\) are the singular values of \\(A\\).",
    "crumbs": [
      "Spectral theory & decompositions",
      "Singular Value Decomposition (SVD)"
    ]
  },
  {
    "objectID": "chapters/05-svd.html#structure-of-the-decomposition",
    "href": "chapters/05-svd.html#structure-of-the-decomposition",
    "title": "Singular Value Decomposition (SVD)",
    "section": "3 Structure of the Decomposition",
    "text": "3 Structure of the Decomposition\n\nColumns of \\(V\\): right singular vectors\nColumns of \\(U\\): left singular vectors\n\\(\\Sigma\\): scaling along orthogonal directions\n\nOnly the first \\(r = \\mathrm{rank}(A)\\) singular values are nonzero.",
    "crumbs": [
      "Spectral theory & decompositions",
      "Singular Value Decomposition (SVD)"
    ]
  },
  {
    "objectID": "chapters/05-svd.html#geometric-interpretation",
    "href": "chapters/05-svd.html#geometric-interpretation",
    "title": "Singular Value Decomposition (SVD)",
    "section": "4 Geometric Interpretation",
    "text": "4 Geometric Interpretation\nThe SVD describes the action of \\(A\\) as:\n\nrotation/reflection by \\(V^\\top\\),\nscaling by \\(\\Sigma\\),\nrotation/reflection by \\(U\\).\n\nThus, \\(A\\) maps the unit sphere in \\(\\mathbb{R}^n\\) to an ellipsoid in \\(\\mathbb{R}^m\\).",
    "crumbs": [
      "Spectral theory & decompositions",
      "Singular Value Decomposition (SVD)"
    ]
  },
  {
    "objectID": "chapters/05-svd.html#rank-one-decomposition",
    "href": "chapters/05-svd.html#rank-one-decomposition",
    "title": "Singular Value Decomposition (SVD)",
    "section": "5 Rank-One Decomposition",
    "text": "5 Rank-One Decomposition\nThe SVD can be written as a sum of rank-one operators:\n\\[\nA = \\sum_{i=1}^r \\sigma_i \\, u_i v_i^\\top.\n\\]\nEach term: - projects onto \\(v_i\\), - scales by \\(\\sigma_i\\), - maps to direction \\(u_i\\).",
    "crumbs": [
      "Spectral theory & decompositions",
      "Singular Value Decomposition (SVD)"
    ]
  },
  {
    "objectID": "chapters/05-svd.html#proof-sketch-of-existence",
    "href": "chapters/05-svd.html#proof-sketch-of-existence",
    "title": "Singular Value Decomposition (SVD)",
    "section": "6 Proof Sketch of Existence",
    "text": "6 Proof Sketch of Existence\n\n\nProof sketch (click to expand)\n\n\nProof. \n\nProof (sketch)\n\n\nConsider the symmetric matrix \\(A^\\top A\\).\nBy the Spectral Theorem, there exists an orthonormal basis of eigenvectors \\(\\{v_i\\}\\) with eigenvalues \\(\\lambda_i \\ge 0\\).\nDefine singular values \\(\\sigma_i = \\sqrt{\\lambda_i}\\).\nFor \\(\\sigma_i &gt; 0\\), define \\(u_i = \\frac{1}{\\sigma_i} A v_i\\).\nShow that \\(\\{u_i\\}\\) is orthonormal and that\n\\[\nA v_i = \\sigma_i u_i.\n\\]\nExtend \\(\\{u_i\\}\\) and \\(\\{v_i\\}\\) to orthonormal bases of \\(\\mathbb{R}^m\\) and \\(\\mathbb{R}^n\\).\n\nThis yields \\(A = U \\Sigma V^\\top\\). □",
    "crumbs": [
      "Spectral theory & decompositions",
      "Singular Value Decomposition (SVD)"
    ]
  },
  {
    "objectID": "chapters/05-svd.html#best-rank-k-approximation",
    "href": "chapters/05-svd.html#best-rank-k-approximation",
    "title": "Singular Value Decomposition (SVD)",
    "section": "7 Best Rank-\\(k\\) Approximation",
    "text": "7 Best Rank-\\(k\\) Approximation\nOne of the most important properties of SVD is optimal low-rank approximation (Eckart and Young 1936; Mirsky 1960).\n\n\nTheorem (Eckart–Young–Mirsky)\n\nLet \\(A = \\sum_{i=1}^r \\sigma_i u_i v_i^\\top\\) be the SVD of \\(A\\). For \\(k &lt; r\\), define\n\\[\nA_k := \\sum_{i=1}^k \\sigma_i u_i v_i^\\top.\n\\]\nThen \\(A_k\\) is the best rank-\\(k\\) approximation of \\(A\\) in both:\n\nFrobenius norm,\noperator (spectral) norm.\n\nThat is,\n\\[\n\\|A - A_k\\| = \\min_{\\mathrm{rank}(B)\\le k} \\|A - B\\|.\n\\]\n\n\n\n\nProof of the Eckart–Young–Mirsky Theorem (rigorous, click to expand)\n\n\nProof. \n\nProof (Eckart–Young–Mirsky)\n\nLet \\(A\\in\\mathbb{R}^{m\\times n}\\) have SVD \\[\nA=U\\Sigma V^\\top,\n\\qquad\n\\Sigma=\\mathrm{diag}(\\sigma_1,\\dots,\\sigma_r)\\in\\mathbb{R}^{m\\times n},\n\\qquad\n\\sigma_1\\ge \\cdots \\ge \\sigma_r&gt;0,\n\\] where \\(r=\\mathrm{rank}(A)\\) and we set \\(\\sigma_i=0\\) for \\(i&gt;r\\).\nFor \\(k&lt;r\\), define the truncated SVD \\[\nA_k := U\\Sigma_k V^\\top,\n\\qquad\n\\Sigma_k=\\mathrm{diag}(\\sigma_1,\\dots,\\sigma_k,0,\\dots,0).\n\\]\nWe prove that for both the Frobenius norm \\(\\|\\cdot\\|_F\\) and the operator (spectral) norm \\(\\|\\cdot\\|_2\\), \\[\n\\|A-A_k\\| = \\min_{\\mathrm{rank}(B)\\le k}\\|A-B\\|.\n\\]\n\n\n8 Step 1: Reduce to the diagonal case\nBoth \\(\\|\\cdot\\|_F\\) and \\(\\|\\cdot\\|_2\\) are orthogonally invariant: for orthogonal \\(Q_1,Q_2\\), \\[\n\\|Q_1MQ_2\\|_F=\\|M\\|_F,\n\\qquad\n\\|Q_1MQ_2\\|_2=\\|M\\|_2.\n\\]\nFix any matrix \\(B\\) with \\(\\mathrm{rank}(B)\\le k\\) and set \\[\nC := U^\\top B V.\n\\] Then \\(\\mathrm{rank}(C)=\\mathrm{rank}(B)\\le k\\) and \\[\n\\|A-B\\|\n=\\|U\\Sigma V^\\top - B\\|\n=\\|\\Sigma - U^\\top B V\\|\n=\\|\\Sigma - C\\|.\n\\] Therefore, \\[\n\\min_{\\mathrm{rank}(B)\\le k}\\|A-B\\|\n=\n\\min_{\\mathrm{rank}(C)\\le k}\\|\\Sigma-C\\|.\n\\]\nSo it suffices to prove the theorem for \\(\\Sigma\\) and its truncation \\(\\Sigma_k\\).\n\n\n\n9 Part A: Frobenius norm optimality\nLet \\(C\\) be any matrix with \\(\\mathrm{rank}(C)\\le k\\) and denote its singular values by \\[\n\\tau_1\\ge \\tau_2\\ge \\cdots \\ge \\tau_k\\ge 0,\n\\qquad\n\\tau_i=0 \\text{ for } i&gt;k.\n\\]\nWe use the identity \\[\n\\|\\Sigma-C\\|_F^2\n= \\|\\Sigma\\|_F^2 + \\|C\\|_F^2 - 2\\,\\mathrm{tr}(\\Sigma^\\top C).\n\\]\n\n9.1 Key inequality (von Neumann trace inequality)\nFor any matrices \\(X,Y\\) of the same size, the trace satisfies \\[\n\\mathrm{tr}(X^\\top Y)\\le \\sum_{i\\ge 1} s_i(X)\\,s_i(Y),\n\\] where \\(s_i(\\cdot)\\) denotes singular values in nonincreasing order. Applying this to \\(X=\\Sigma\\) and \\(Y=C\\) gives \\[\n\\mathrm{tr}(\\Sigma^\\top C)\n\\le\n\\sum_{i\\ge 1} \\sigma_i\\,\\tau_i\n=\n\\sum_{i=1}^k \\sigma_i\\,\\tau_i,\n\\] since \\(\\tau_i=0\\) for \\(i&gt;k\\).\nHence, \\[\\begin{align*}\n\\|\\Sigma-C\\|_F^2\n&\\ge\n\\sum_{i\\ge 1}\\sigma_i^2\n+\n\\sum_{i=1}^k \\tau_i^2\n-\n2\\sum_{i=1}^k \\sigma_i\\tau_i \\\\\n&=\n\\sum_{i=1}^k(\\sigma_i-\\tau_i)^2\n+\n\\sum_{i&gt;k}\\sigma_i^2 \\\\\n&\\ge\n\\sum_{i&gt;k}\\sigma_i^2.\n\\end{align*}\\]\nTherefore, \\[\n\\|\\Sigma-C\\|_F \\;\\ge\\; \\left(\\sum_{i&gt;k}\\sigma_i^2\\right)^{1/2}.\n\\]\n\n\n9.2 Achievability\nChoose \\(C=\\Sigma_k\\). Then \\[\n\\|\\Sigma-\\Sigma_k\\|_F^2 = \\sum_{i&gt;k}\\sigma_i^2,\n\\] so equality is achieved and \\(\\Sigma_k\\) is a best rank-\\(k\\) approximation in Frobenius norm.\nReturning to \\(A=U\\Sigma V^\\top\\), we obtain that \\(A_k=U\\Sigma_kV^\\top\\) is optimal and \\[\n\\min_{\\mathrm{rank}(B)\\le k}\\|A-B\\|_F = \\|A-A_k\\|_F = \\left(\\sum_{i&gt;k}\\sigma_i^2\\right)^{1/2}.\n\\]\n\n\n\n\n10 Part B: Spectral norm optimality\nWe now prove \\[\n\\min_{\\mathrm{rank}(C)\\le k}\\|\\Sigma-C\\|_2 = \\sigma_{k+1},\n\\] (where \\(\\sigma_{k+1}&gt;0\\) since \\(k&lt;r\\)).\nLet \\(C\\) have rank \\(\\le k\\). Then its \\((k+1)\\)-th singular value is zero: \\[\n\\tau_{k+1}(C)=0.\n\\]\n\n10.1 Key inequality (Weyl-type singular value inequality)\nFor any matrices \\(X,Y\\) of the same size and any indices \\(i,j\\) with \\(i+j-1\\) valid, \\[\ns_{i+j-1}(X+Y)\\le s_i(X)+s_j(Y).\n\\]\nApply this with \\(X=\\Sigma-C\\) and \\(Y=C\\), so that \\(X+Y=\\Sigma\\). Choose \\(i=1\\) and \\(j=k+1\\). Then \\[\ns_{k+1}(\\Sigma)\n\\le\ns_1(\\Sigma-C)+s_{k+1}(C)\n=\n\\|\\Sigma-C\\|_2 + 0.\n\\] Hence, \\[\n\\|\\Sigma-C\\|_2 \\ge s_{k+1}(\\Sigma)=\\sigma_{k+1}.\n\\]\n\n\n10.2 Achievability\nTake \\(C=\\Sigma_k\\). Then \\(\\Sigma-\\Sigma_k\\) is diagonal with largest diagonal entry \\(\\sigma_{k+1}\\), so \\[\n\\|\\Sigma-\\Sigma_k\\|_2 = \\sigma_{k+1}.\n\\]\nTherefore \\(\\Sigma_k\\) is a best rank-\\(k\\) approximation in spectral norm, and transferring back to \\(A\\) gives \\[\n\\min_{\\mathrm{rank}(B)\\le k}\\|A-B\\|_2 = \\|A-A_k\\|_2 = \\sigma_{k+1}.\n\\]\n\n\n\n\n11 Conclusion\nFor both norms, \\[\nA_k = U\\Sigma_kV^\\top \\in \\arg\\min_{\\mathrm{rank}(B)\\le k}\\|A-B\\|.\n\\] Moreover, \\[\n\\|A-A_k\\|_F = \\left(\\sum_{i&gt;k}\\sigma_i^2\\right)^{1/2},\n\\qquad\n\\|A-A_k\\|_2 = \\sigma_{k+1}.\n\\]\n□",
    "crumbs": [
      "Spectral theory & decompositions",
      "Singular Value Decomposition (SVD)"
    ]
  },
  {
    "objectID": "chapters/05-svd.html#step-1-reduce-to-the-diagonal-case",
    "href": "chapters/05-svd.html#step-1-reduce-to-the-diagonal-case",
    "title": "Singular Value Decomposition (SVD)",
    "section": "8 Step 1: Reduce to the diagonal case",
    "text": "8 Step 1: Reduce to the diagonal case\nBoth \\(\\|\\cdot\\|_F\\) and \\(\\|\\cdot\\|_2\\) are orthogonally invariant: for orthogonal \\(Q_1,Q_2\\), \\[\n\\|Q_1MQ_2\\|_F=\\|M\\|_F,\n\\qquad\n\\|Q_1MQ_2\\|_2=\\|M\\|_2.\n\\]\nFix any matrix \\(B\\) with \\(\\mathrm{rank}(B)\\le k\\) and set \\[\nC := U^\\top B V.\n\\] Then \\(\\mathrm{rank}(C)=\\mathrm{rank}(B)\\le k\\) and \\[\n\\|A-B\\|\n=\\|U\\Sigma V^\\top - B\\|\n=\\|\\Sigma - U^\\top B V\\|\n=\\|\\Sigma - C\\|.\n\\] Therefore, \\[\n\\min_{\\mathrm{rank}(B)\\le k}\\|A-B\\|\n=\n\\min_{\\mathrm{rank}(C)\\le k}\\|\\Sigma-C\\|.\n\\]\nSo it suffices to prove the theorem for \\(\\Sigma\\) and its truncation \\(\\Sigma_k\\).",
    "crumbs": [
      "Spectral theory & decompositions",
      "Singular Value Decomposition (SVD)"
    ]
  },
  {
    "objectID": "chapters/05-svd.html#part-a-frobenius-norm-optimality",
    "href": "chapters/05-svd.html#part-a-frobenius-norm-optimality",
    "title": "Singular Value Decomposition (SVD)",
    "section": "9 Part A: Frobenius norm optimality",
    "text": "9 Part A: Frobenius norm optimality\nLet \\(C\\) be any matrix with \\(\\mathrm{rank}(C)\\le k\\) and denote its singular values by \\[\n\\tau_1\\ge \\tau_2\\ge \\cdots \\ge \\tau_k\\ge 0,\n\\qquad\n\\tau_i=0 \\text{ for } i&gt;k.\n\\]\nWe use the identity \\[\n\\|\\Sigma-C\\|_F^2\n= \\|\\Sigma\\|_F^2 + \\|C\\|_F^2 - 2\\,\\mathrm{tr}(\\Sigma^\\top C).\n\\]\n\n9.1 Key inequality (von Neumann trace inequality)\nFor any matrices \\(X,Y\\) of the same size, the trace satisfies \\[\n\\mathrm{tr}(X^\\top Y)\\le \\sum_{i\\ge 1} s_i(X)\\,s_i(Y),\n\\] where \\(s_i(\\cdot)\\) denotes singular values in nonincreasing order. Applying this to \\(X=\\Sigma\\) and \\(Y=C\\) gives \\[\n\\mathrm{tr}(\\Sigma^\\top C)\n\\le\n\\sum_{i\\ge 1} \\sigma_i\\,\\tau_i\n=\n\\sum_{i=1}^k \\sigma_i\\,\\tau_i,\n\\] since \\(\\tau_i=0\\) for \\(i&gt;k\\).\nHence, \\[\\begin{align*}\n\\|\\Sigma-C\\|_F^2\n&\\ge\n\\sum_{i\\ge 1}\\sigma_i^2\n+\n\\sum_{i=1}^k \\tau_i^2\n-\n2\\sum_{i=1}^k \\sigma_i\\tau_i \\\\\n&=\n\\sum_{i=1}^k(\\sigma_i-\\tau_i)^2\n+\n\\sum_{i&gt;k}\\sigma_i^2 \\\\\n&\\ge\n\\sum_{i&gt;k}\\sigma_i^2.\n\\end{align*}\\]\nTherefore, \\[\n\\|\\Sigma-C\\|_F \\;\\ge\\; \\left(\\sum_{i&gt;k}\\sigma_i^2\\right)^{1/2}.\n\\]\n\n\n9.2 Achievability\nChoose \\(C=\\Sigma_k\\). Then \\[\n\\|\\Sigma-\\Sigma_k\\|_F^2 = \\sum_{i&gt;k}\\sigma_i^2,\n\\] so equality is achieved and \\(\\Sigma_k\\) is a best rank-\\(k\\) approximation in Frobenius norm.\nReturning to \\(A=U\\Sigma V^\\top\\), we obtain that \\(A_k=U\\Sigma_kV^\\top\\) is optimal and \\[\n\\min_{\\mathrm{rank}(B)\\le k}\\|A-B\\|_F = \\|A-A_k\\|_F = \\left(\\sum_{i&gt;k}\\sigma_i^2\\right)^{1/2}.\n\\]",
    "crumbs": [
      "Spectral theory & decompositions",
      "Singular Value Decomposition (SVD)"
    ]
  },
  {
    "objectID": "chapters/05-svd.html#part-b-spectral-norm-optimality",
    "href": "chapters/05-svd.html#part-b-spectral-norm-optimality",
    "title": "Singular Value Decomposition (SVD)",
    "section": "10 Part B: Spectral norm optimality",
    "text": "10 Part B: Spectral norm optimality\nWe now prove \\[\n\\min_{\\mathrm{rank}(C)\\le k}\\|\\Sigma-C\\|_2 = \\sigma_{k+1},\n\\] (where \\(\\sigma_{k+1}&gt;0\\) since \\(k&lt;r\\)).\nLet \\(C\\) have rank \\(\\le k\\). Then its \\((k+1)\\)-th singular value is zero: \\[\n\\tau_{k+1}(C)=0.\n\\]\n\n10.1 Key inequality (Weyl-type singular value inequality)\nFor any matrices \\(X,Y\\) of the same size and any indices \\(i,j\\) with \\(i+j-1\\) valid, \\[\ns_{i+j-1}(X+Y)\\le s_i(X)+s_j(Y).\n\\]\nApply this with \\(X=\\Sigma-C\\) and \\(Y=C\\), so that \\(X+Y=\\Sigma\\). Choose \\(i=1\\) and \\(j=k+1\\). Then \\[\ns_{k+1}(\\Sigma)\n\\le\ns_1(\\Sigma-C)+s_{k+1}(C)\n=\n\\|\\Sigma-C\\|_2 + 0.\n\\] Hence, \\[\n\\|\\Sigma-C\\|_2 \\ge s_{k+1}(\\Sigma)=\\sigma_{k+1}.\n\\]\n\n\n10.2 Achievability\nTake \\(C=\\Sigma_k\\). Then \\(\\Sigma-\\Sigma_k\\) is diagonal with largest diagonal entry \\(\\sigma_{k+1}\\), so \\[\n\\|\\Sigma-\\Sigma_k\\|_2 = \\sigma_{k+1}.\n\\]\nTherefore \\(\\Sigma_k\\) is a best rank-\\(k\\) approximation in spectral norm, and transferring back to \\(A\\) gives \\[\n\\min_{\\mathrm{rank}(B)\\le k}\\|A-B\\|_2 = \\|A-A_k\\|_2 = \\sigma_{k+1}.\n\\]",
    "crumbs": [
      "Spectral theory & decompositions",
      "Singular Value Decomposition (SVD)"
    ]
  },
  {
    "objectID": "chapters/05-svd.html#conclusion",
    "href": "chapters/05-svd.html#conclusion",
    "title": "Singular Value Decomposition (SVD)",
    "section": "11 Conclusion",
    "text": "11 Conclusion\nFor both norms, \\[\nA_k = U\\Sigma_kV^\\top \\in \\arg\\min_{\\mathrm{rank}(B)\\le k}\\|A-B\\|.\n\\] Moreover, \\[\n\\|A-A_k\\|_F = \\left(\\sum_{i&gt;k}\\sigma_i^2\\right)^{1/2},\n\\qquad\n\\|A-A_k\\|_2 = \\sigma_{k+1}.\n\\]\n□",
    "crumbs": [
      "Spectral theory & decompositions",
      "Singular Value Decomposition (SVD)"
    ]
  },
  {
    "objectID": "chapters/05-svd.html#computational-example-python",
    "href": "chapters/05-svd.html#computational-example-python",
    "title": "Singular Value Decomposition (SVD)",
    "section": "12 Computational Example (Python)",
    "text": "12 Computational Example (Python)\nimport numpy as np\n\nrng = np.random.default_rng(0)\nA = rng.normal(size=(6, 4))\n\nU, s, Vt = np.linalg.svd(A, full_matrices=False)\n\nU.shape, s, Vt.shape\nReconstruction:\nA_reconstructed = U @ np.diag(s) @ Vt\nnp.allclose(A, A_reconstructed)",
    "crumbs": [
      "Spectral theory & decompositions",
      "Singular Value Decomposition (SVD)"
    ]
  },
  {
    "objectID": "chapters/05-svd.html#low-rank-approximation-in-practice",
    "href": "chapters/05-svd.html#low-rank-approximation-in-practice",
    "title": "Singular Value Decomposition (SVD)",
    "section": "13 Low-Rank Approximation in Practice",
    "text": "13 Low-Rank Approximation in Practice\nk = 2\nA_k = U[:, :k] @ np.diag(s[:k]) @ Vt[:k, :]\n\nnp.linalg.norm(A - A_k, ord=\"fro\")",
    "crumbs": [
      "Spectral theory & decompositions",
      "Singular Value Decomposition (SVD)"
    ]
  },
  {
    "objectID": "chapters/05-svd.html#connection-to-pca",
    "href": "chapters/05-svd.html#connection-to-pca",
    "title": "Singular Value Decomposition (SVD)",
    "section": "14 Connection to PCA",
    "text": "14 Connection to PCA\nGiven a centered data matrix \\(X \\in \\mathbb{R}^{m \\times n}\\):\n\nSVD: \\(X = U \\Sigma V^\\top\\)\nCovariance: \\(X^\\top X = V \\Sigma^2 V^\\top\\)\n\nThus: - right singular vectors = principal directions, - squared singular values = explained variance (up to normalization).\nPCA can be computed directly via SVD, without forming the covariance matrix.",
    "crumbs": [
      "Spectral theory & decompositions",
      "Singular Value Decomposition (SVD)"
    ]
  },
  {
    "objectID": "chapters/05-svd.html#conditioning-and-stability",
    "href": "chapters/05-svd.html#conditioning-and-stability",
    "title": "Singular Value Decomposition (SVD)",
    "section": "15 Conditioning and Stability",
    "text": "15 Conditioning and Stability\nThe condition number of \\(A\\) is:\n\\[\n\\kappa(A) = \\frac{\\sigma_{\\max}}{\\sigma_{\\min}}.\n\\]\n\nLarge \\(\\kappa(A)\\) implies ill-conditioned problems.\nSmall singular values amplify noise and numerical errors.\n\nThis explains why SVD is crucial for: - diagnosing instability, - regularization (e.g. truncated SVD).",
    "crumbs": [
      "Spectral theory & decompositions",
      "Singular Value Decomposition (SVD)"
    ]
  },
  {
    "objectID": "chapters/05-svd.html#data-science-applications",
    "href": "chapters/05-svd.html#data-science-applications",
    "title": "Singular Value Decomposition (SVD)",
    "section": "16 Data Science Applications",
    "text": "16 Data Science Applications\n\n16.1 Dimensionality Reduction\nKeep only the first \\(k\\) singular values and vectors to obtain a low-dimensional representation.\n\n\n16.2 Compression\nStore \\(U_k\\), \\(\\Sigma_k\\), and \\(V_k\\) instead of the full matrix \\(A\\).\n\n\n16.3 Recommender Systems\nUser–item interaction matrices are approximated by low-rank SVD to uncover latent factors.\n\n\n16.4 Latent Semantic Analysis\nText–term matrices are decomposed to identify latent semantic structure in documents.",
    "crumbs": [
      "Spectral theory & decompositions",
      "Singular Value Decomposition (SVD)"
    ]
  },
  {
    "objectID": "chapters/05-svd.html#what-svd-does-not-assume",
    "href": "chapters/05-svd.html#what-svd-does-not-assume",
    "title": "Singular Value Decomposition (SVD)",
    "section": "17 What SVD Does Not Assume",
    "text": "17 What SVD Does Not Assume\n\nno symmetry,\nno invertibility,\nno square shape.\n\nThis universality makes SVD the central tool of applied linear algebra.",
    "crumbs": [
      "Spectral theory & decompositions",
      "Singular Value Decomposition (SVD)"
    ]
  },
  {
    "objectID": "chapters/05-svd.html#summary",
    "href": "chapters/05-svd.html#summary",
    "title": "Singular Value Decomposition (SVD)",
    "section": "18 Summary",
    "text": "18 Summary\n\nSVD generalizes the Spectral Theorem.\nAny matrix admits an orthogonal decomposition.\nSingular values quantify intrinsic dimensionality.\nTruncated SVD yields optimal low-rank approximations.\nPCA and many Data Science methods are special cases of SVD.",
    "crumbs": [
      "Spectral theory & decompositions",
      "Singular Value Decomposition (SVD)"
    ]
  },
  {
    "objectID": "chapters/05-svd.html#preview-regularization-and-truncated-svd",
    "href": "chapters/05-svd.html#preview-regularization-and-truncated-svd",
    "title": "Singular Value Decomposition (SVD)",
    "section": "19 Preview: Regularization and Truncated SVD",
    "text": "19 Preview: Regularization and Truncated SVD\nSmall singular values cause instability. In the next chapter, we study:\n\ntruncated SVD,\nTikhonov (ridge) regularization,\nbias–variance trade-offs.",
    "crumbs": [
      "Spectral theory & decompositions",
      "Singular Value Decomposition (SVD)"
    ]
  },
  {
    "objectID": "chapters/05-from-spectral-to-svd.html",
    "href": "chapters/05-from-spectral-to-svd.html",
    "title": "From the Spectral Theorem to the Singular Value Decomposition",
    "section": "",
    "text": "The Spectral Theorem gives a complete description of symmetric linear operators. However, most data matrices encountered in practice are:\n\nrectangular,\nnon-symmetric,\nnot even square.\n\nThe Singular Value Decomposition (SVD) extends the ideas of spectral theory to all matrices and preserves the geometric intuition of:\n\northogonal directions,\nscaling along independent axes,\nrank and low-dimensional structure.\n\nThis chapter explains why SVD is the natural generalization of the Spectral Theorem.",
    "crumbs": [
      "Spectral theory & decompositions",
      "From the Spectral Theorem to the Singular Value Decomposition"
    ]
  },
  {
    "objectID": "chapters/05-from-spectral-to-svd.html#motivation",
    "href": "chapters/05-from-spectral-to-svd.html#motivation",
    "title": "From the Spectral Theorem to the Singular Value Decomposition",
    "section": "",
    "text": "The Spectral Theorem gives a complete description of symmetric linear operators. However, most data matrices encountered in practice are:\n\nrectangular,\nnon-symmetric,\nnot even square.\n\nThe Singular Value Decomposition (SVD) extends the ideas of spectral theory to all matrices and preserves the geometric intuition of:\n\northogonal directions,\nscaling along independent axes,\nrank and low-dimensional structure.\n\nThis chapter explains why SVD is the natural generalization of the Spectral Theorem.",
    "crumbs": [
      "Spectral theory & decompositions",
      "From the Spectral Theorem to the Singular Value Decomposition"
    ]
  },
  {
    "objectID": "chapters/05-from-spectral-to-svd.html#what-the-spectral-theorem-requires",
    "href": "chapters/05-from-spectral-to-svd.html#what-the-spectral-theorem-requires",
    "title": "From the Spectral Theorem to the Singular Value Decomposition",
    "section": "2 What the Spectral Theorem Requires",
    "text": "2 What the Spectral Theorem Requires\nRecall the Spectral Theorem:\n\nthe matrix must be square,\nthe matrix must be symmetric (self-adjoint).\n\nOnly under these conditions do we obtain an orthogonal diagonalization\n\\[\nA = Q \\Lambda Q^\\top.\n\\]\nThis immediately raises two questions:\n\nWhat if \\(A\\) is not symmetric?\nWhat if \\(A\\) is rectangular?",
    "crumbs": [
      "Spectral theory & decompositions",
      "From the Spectral Theorem to the Singular Value Decomposition"
    ]
  },
  {
    "objectID": "chapters/05-from-spectral-to-svd.html#first-observation-symmetry-can-be-recovered",
    "href": "chapters/05-from-spectral-to-svd.html#first-observation-symmetry-can-be-recovered",
    "title": "From the Spectral Theorem to the Singular Value Decomposition",
    "section": "3 First Observation: Symmetry Can Be Recovered",
    "text": "3 First Observation: Symmetry Can Be Recovered\nGiven an arbitrary matrix \\(A \\in \\mathbb{R}^{m \\times n}\\), the matrices\n\\[\nA^\\top A \\in \\mathbb{R}^{n \\times n},\n\\qquad\nAA^\\top \\in \\mathbb{R}^{m \\times m}\n\\]\nare always:\n\nsymmetric,\npositive semidefinite.\n\nThus, spectral theory applies to both \\(A^\\top A\\) and \\(AA^\\top\\).",
    "crumbs": [
      "Spectral theory & decompositions",
      "From the Spectral Theorem to the Singular Value Decomposition"
    ]
  },
  {
    "objectID": "chapters/05-from-spectral-to-svd.html#eigenvalues-of-atop-a",
    "href": "chapters/05-from-spectral-to-svd.html#eigenvalues-of-atop-a",
    "title": "From the Spectral Theorem to the Singular Value Decomposition",
    "section": "4 Eigenvalues of \\(A^\\top A\\)",
    "text": "4 Eigenvalues of \\(A^\\top A\\)\n\n\nProposition\n\nLet \\(A \\in \\mathbb{R}^{m \\times n}\\). Then:\n\nAll eigenvalues of \\(A^\\top A\\) are real and nonnegative.\nIf \\(v\\) is an eigenvector of \\(A^\\top A\\) with eigenvalue \\(\\lambda &gt; 0\\), then \\(Av \\neq 0\\) and\n\\[\n\\|Av\\|^2 = \\lambda \\|v\\|^2.\n\\]\n\n\nThis motivates the definition:\n\n\nDefinition (Singular values)\n\nThe singular values of \\(A\\) are defined as\n\\[\n\\sigma_i := \\sqrt{\\lambda_i},\n\\]\nwhere \\(\\lambda_i\\) are the eigenvalues of \\(A^\\top A\\).",
    "crumbs": [
      "Spectral theory & decompositions",
      "From the Spectral Theorem to the Singular Value Decomposition"
    ]
  },
  {
    "objectID": "chapters/05-from-spectral-to-svd.html#geometric-interpretation",
    "href": "chapters/05-from-spectral-to-svd.html#geometric-interpretation",
    "title": "From the Spectral Theorem to the Singular Value Decomposition",
    "section": "5 Geometric Interpretation",
    "text": "5 Geometric Interpretation\nIf \\(v\\) is a unit eigenvector of \\(A^\\top A\\) with eigenvalue \\(\\lambda\\), then:\n\\[\n\\|Av\\| = \\sqrt{\\lambda} = \\sigma.\n\\]\nThus:\n\n\\(v\\) is a direction in the input space,\n\\(A\\) maps \\(v\\) to a vector of length \\(\\sigma\\),\n\\(\\sigma\\) measures the stretching factor of \\(A\\) in direction \\(v\\).\n\nThis is exactly the geometric content we expect from a spectral decomposition.",
    "crumbs": [
      "Spectral theory & decompositions",
      "From the Spectral Theorem to the Singular Value Decomposition"
    ]
  },
  {
    "objectID": "chapters/05-from-spectral-to-svd.html#left-and-right-singular-directions",
    "href": "chapters/05-from-spectral-to-svd.html#left-and-right-singular-directions",
    "title": "From the Spectral Theorem to the Singular Value Decomposition",
    "section": "6 Left and Right Singular Directions",
    "text": "6 Left and Right Singular Directions\nLet \\(v_i\\) be an eigenvector of \\(A^\\top A\\) with eigenvalue \\(\\lambda_i &gt; 0\\). Define:\n\\[\nu_i := \\frac{1}{\\sigma_i} A v_i.\n\\]\n\n\nProposition\n\nThe vectors \\(u_i\\) satisfy:\n\n\\(\\|u_i\\| = 1\\),\n\\(u_i^\\top u_j = 0\\) for \\(i \\neq j\\),\n\\(A v_i = \\sigma_i u_i\\).\n\n\nSo:\n\n\\(v_i\\) are right singular vectors,\n\\(u_i\\) are left singular vectors.\n\nBoth families are orthonormal.",
    "crumbs": [
      "Spectral theory & decompositions",
      "From the Spectral Theorem to the Singular Value Decomposition"
    ]
  },
  {
    "objectID": "chapters/05-from-spectral-to-svd.html#recovering-a-matrix-from-rank-one-operators",
    "href": "chapters/05-from-spectral-to-svd.html#recovering-a-matrix-from-rank-one-operators",
    "title": "From the Spectral Theorem to the Singular Value Decomposition",
    "section": "7 Recovering a Matrix from Rank-One Operators",
    "text": "7 Recovering a Matrix from Rank-One Operators\nUsing the pairs \\((u_i, v_i)\\), we can write:\n\\[\nA = \\sum_{i=1}^r \\sigma_i \\, u_i v_i^\\top,\n\\]\nwhere \\(r = \\mathrm{rank}(A)\\).\nThis expresses \\(A\\) as a sum of rank-one operators, generalizing the spectral decomposition of symmetric matrices.",
    "crumbs": [
      "Spectral theory & decompositions",
      "From the Spectral Theorem to the Singular Value Decomposition"
    ]
  },
  {
    "objectID": "chapters/05-from-spectral-to-svd.html#comparison-with-the-spectral-decomposition",
    "href": "chapters/05-from-spectral-to-svd.html#comparison-with-the-spectral-decomposition",
    "title": "From the Spectral Theorem to the Singular Value Decomposition",
    "section": "8 Comparison with the Spectral Decomposition",
    "text": "8 Comparison with the Spectral Decomposition\n\n\n\n\n\n\n\nSpectral Theorem\nSingular Value Decomposition\n\n\n\n\n\\(A = Q \\Lambda Q^\\top\\)\n\\(A = U \\Sigma V^\\top\\)\n\n\nsymmetric \\(A\\)\narbitrary \\(A\\)\n\n\neigenvalues \\(\\lambda_i\\)\nsingular values \\(\\sigma_i = \\sqrt{\\lambda_i}\\)\n\n\none orthonormal basis\ntwo orthonormal bases\n\n\nacts on same space\nmaps between different spaces\n\n\n\nThe SVD reduces to the Spectral Theorem when \\(A\\) is symmetric.",
    "crumbs": [
      "Spectral theory & decompositions",
      "From the Spectral Theorem to the Singular Value Decomposition"
    ]
  },
  {
    "objectID": "chapters/05-from-spectral-to-svd.html#computational-insight",
    "href": "chapters/05-from-spectral-to-svd.html#computational-insight",
    "title": "From the Spectral Theorem to the Singular Value Decomposition",
    "section": "9 Computational Insight",
    "text": "9 Computational Insight\nIn practice:\n\neigen-decomposition of \\(A^\\top A\\) is not computed explicitly,\nSVD algorithms compute \\(U, \\Sigma, V\\) directly for numerical stability.\n\nHowever, the theoretical construction via \\(A^\\top A\\) explains why SVD exists and why it is orthogonal.",
    "crumbs": [
      "Spectral theory & decompositions",
      "From the Spectral Theorem to the Singular Value Decomposition"
    ]
  },
  {
    "objectID": "chapters/05-from-spectral-to-svd.html#why-this-matters-for-data-science",
    "href": "chapters/05-from-spectral-to-svd.html#why-this-matters-for-data-science",
    "title": "From the Spectral Theorem to the Singular Value Decomposition",
    "section": "10 Why This Matters for Data Science",
    "text": "10 Why This Matters for Data Science\nMost datasets are rectangular matrices:\n\nrows = samples,\ncolumns = features.\n\nSVD provides:\n\northogonal directions of maximal variance,\na notion of rank and intrinsic dimension,\noptimal low-rank approximations.\n\nThis makes SVD the central decomposition in modern data analysis.",
    "crumbs": [
      "Spectral theory & decompositions",
      "From the Spectral Theorem to the Singular Value Decomposition"
    ]
  },
  {
    "objectID": "chapters/05-from-spectral-to-svd.html#summary",
    "href": "chapters/05-from-spectral-to-svd.html#summary",
    "title": "From the Spectral Theorem to the Singular Value Decomposition",
    "section": "11 Summary",
    "text": "11 Summary\n\nSpectral theory applies to symmetric operators.\nAny matrix induces symmetric operators \\(A^\\top A\\) and \\(AA^\\top\\).\nSingular values arise as square roots of eigenvalues.\nSVD generalizes the Spectral Theorem to arbitrary matrices.\nGeometry (orthogonality + scaling) is preserved.",
    "crumbs": [
      "Spectral theory & decompositions",
      "From the Spectral Theorem to the Singular Value Decomposition"
    ]
  },
  {
    "objectID": "chapters/05-from-spectral-to-svd.html#preview-singular-value-decomposition",
    "href": "chapters/05-from-spectral-to-svd.html#preview-singular-value-decomposition",
    "title": "From the Spectral Theorem to the Singular Value Decomposition",
    "section": "12 Preview: Singular Value Decomposition",
    "text": "12 Preview: Singular Value Decomposition\nWe now state and study the Singular Value Decomposition formally, prove its main properties, and explore its applications to low-rank approximation, PCA, and data compression.",
    "crumbs": [
      "Spectral theory & decompositions",
      "From the Spectral Theorem to the Singular Value Decomposition"
    ]
  },
  {
    "objectID": "chapters/01-vectors.html",
    "href": "chapters/01-vectors.html",
    "title": "Vectors and Vector Spaces",
    "section": "",
    "text": "Linear algebra starts with the notion of a vector space: a structure where objects can be added and scaled.\nIn Data Science, vectors appear everywhere:\n\na data point is a vector of features,\nan embedding is a vector in \\(\\mathbb{R}^d\\),\na model parameter vector lives in a vector space,\nrows or columns of a data matrix are vectors.\n\nUnderstanding vector spaces precisely is therefore essential.",
    "crumbs": [
      "Foundations",
      "Vectors and Vector Spaces"
    ]
  },
  {
    "objectID": "chapters/01-vectors.html#motivation",
    "href": "chapters/01-vectors.html#motivation",
    "title": "Vectors and Vector Spaces",
    "section": "",
    "text": "Linear algebra starts with the notion of a vector space: a structure where objects can be added and scaled.\nIn Data Science, vectors appear everywhere:\n\na data point is a vector of features,\nan embedding is a vector in \\(\\mathbb{R}^d\\),\na model parameter vector lives in a vector space,\nrows or columns of a data matrix are vectors.\n\nUnderstanding vector spaces precisely is therefore essential.",
    "crumbs": [
      "Foundations",
      "Vectors and Vector Spaces"
    ]
  },
  {
    "objectID": "chapters/01-vectors.html#vectors",
    "href": "chapters/01-vectors.html#vectors",
    "title": "Vectors and Vector Spaces",
    "section": "2 Vectors",
    "text": "2 Vectors\nIntuitively, a vector represents a direction and magnitude.\nAlgebraically, a vector is simply an element of a vector space.\n\n2.1 Examples\n\nA vector in \\(\\mathbb{R}^n\\):\n\\[\nx = (x_1, \\dots, x_n)\n\\]\nA polynomial:\n\\[\np(t) = a_0 + a_1 t + \\cdots + a_n t^n\n\\]\nA function \\(f : [0,1] \\to \\mathbb{R}\\)\n\nAll of these will be vectors once we define the appropriate operations.",
    "crumbs": [
      "Foundations",
      "Vectors and Vector Spaces"
    ]
  },
  {
    "objectID": "chapters/01-vectors.html#vector-spaces",
    "href": "chapters/01-vectors.html#vector-spaces",
    "title": "Vectors and Vector Spaces",
    "section": "3 Vector Spaces",
    "text": "3 Vector Spaces\n\n\nDefinition (Vector space)\n\nLet \\(\\mathbb{K}\\) be a field (typically \\(\\mathbb{R}\\) or \\(\\mathbb{C}\\)). A vector space over \\(\\mathbb{K}\\) is a set \\(V\\) equipped with:\n\nvector addition: \\(V \\times V \\to V\\)\n\nscalar multiplication: \\(\\mathbb{K} \\times V \\to V\\)\n\nsatisfying the following axioms for all \\(u,v,w \\in V\\) and \\(\\alpha,\\beta \\in \\mathbb{K}\\):\n\n\\(u + v = v + u\\) (commutativity)\n\n\\((u + v) + w = u + (v + w)\\) (associativity)\n\nThere exists \\(0 \\in V\\) such that \\(v + 0 = v\\)\n\nFor every \\(v \\in V\\) there exists \\(-v\\) with \\(v + (-v) = 0\\)\n\n\\(\\alpha (v + w) = \\alpha v + \\alpha w\\)\n\n\\((\\alpha + \\beta)v = \\alpha v + \\beta v\\)\n\n\\((\\alpha \\beta)v = \\alpha(\\beta v)\\)\n\n\\(1 v = v\\)",
    "crumbs": [
      "Foundations",
      "Vectors and Vector Spaces"
    ]
  },
  {
    "objectID": "chapters/01-vectors.html#canonical-examples",
    "href": "chapters/01-vectors.html#canonical-examples",
    "title": "Vectors and Vector Spaces",
    "section": "4 Canonical Examples",
    "text": "4 Canonical Examples\n\n4.1 Euclidean space\n\n\nExample (\\(\\mathbb{R}^n\\))\n\n\\(\\mathbb{R}^n\\) with componentwise addition and scalar multiplication is a vector space.\n\n\n\n4.2 Function spaces\n\n\nExample (Function space)\n\nLet \\(V\\) be the set of all real-valued functions on \\([0,1]\\). Define: \\[\n(f+g)(x) := f(x)+g(x), \\quad (\\alpha f)(x) := \\alpha f(x).\n\\] Then \\(V\\) is a vector space.\n\n\n\n4.3 Polynomial spaces\n\n\nExample (Polynomials)\n\nThe set \\(\\mathbb{P}_n\\) of polynomials of degree \\(\\le n\\) is a vector space.",
    "crumbs": [
      "Foundations",
      "Vectors and Vector Spaces"
    ]
  },
  {
    "objectID": "chapters/01-vectors.html#subspaces",
    "href": "chapters/01-vectors.html#subspaces",
    "title": "Vectors and Vector Spaces",
    "section": "5 Subspaces",
    "text": "5 Subspaces\n\n\nDefinition (Subspace)\n\nA subset \\(W \\subset V\\) is a subspace if: 1. \\(0 \\in W\\)\n2. \\(u,v \\in W \\Rightarrow u+v \\in W\\)\n3. \\(\\alpha \\in \\mathbb{K},\\; v \\in W \\Rightarrow \\alpha v \\in W\\)\n\n\n5.1 Subspace test\n\n\nProposition (Subspace test)\n\nA nonempty subset \\(W \\subset V\\) is a subspace if and only if it is closed under addition and scalar multiplication.",
    "crumbs": [
      "Foundations",
      "Vectors and Vector Spaces"
    ]
  },
  {
    "objectID": "chapters/01-vectors.html#linear-combinations-and-span",
    "href": "chapters/01-vectors.html#linear-combinations-and-span",
    "title": "Vectors and Vector Spaces",
    "section": "6 Linear combinations and span",
    "text": "6 Linear combinations and span\n\n\nDefinition (Linear combination)\n\nGiven vectors \\(v_1,\\dots,v_k \\in V\\), a linear combination is any vector of the form: \\[\n\\alpha_1 v_1 + \\cdots + \\alpha_k v_k,\n\\quad \\alpha_i \\in \\mathbb{K}.\n\\]\n\n\n\nDefinition (Span)\n\nThe span of \\(\\{v_1,\\dots,v_k\\}\\) is the set of all their linear combinations: \\[\n\\mathrm{span}(v_1,\\dots,v_k).\n\\]\n\n\n\nProposition\n\n\\(\\mathrm{span}(v_1,\\dots,v_k)\\) is a subspace of \\(V\\).",
    "crumbs": [
      "Foundations",
      "Vectors and Vector Spaces"
    ]
  },
  {
    "objectID": "chapters/01-vectors.html#linear-independence",
    "href": "chapters/01-vectors.html#linear-independence",
    "title": "Vectors and Vector Spaces",
    "section": "7 Linear independence",
    "text": "7 Linear independence\n\n\nDefinition (Linear independence)\n\nVectors \\(v_1,\\dots,v_k\\) are linearly independent if \\[\n\\alpha_1 v_1 + \\cdots + \\alpha_k v_k = 0\n\\quad \\Rightarrow \\quad\n\\alpha_1=\\cdots=\\alpha_k=0.\n\\]\n\nIntuition: no vector can be written as a combination of the others.",
    "crumbs": [
      "Foundations",
      "Vectors and Vector Spaces"
    ]
  },
  {
    "objectID": "chapters/01-vectors.html#basis-and-dimension",
    "href": "chapters/01-vectors.html#basis-and-dimension",
    "title": "Vectors and Vector Spaces",
    "section": "8 Basis and dimension",
    "text": "8 Basis and dimension\n\n\nDefinition (Basis)\n\nA basis of a vector space \\(V\\) is a linearly independent set that spans \\(V\\).\n\n\n\nDefinition (Dimension)\n\nThe dimension of \\(V\\) is the number of vectors in any basis of \\(V\\).\n\n\n\nTheorem\n\nAll bases of a finite-dimensional vector space have the same number of elements.",
    "crumbs": [
      "Foundations",
      "Vectors and Vector Spaces"
    ]
  },
  {
    "objectID": "chapters/01-vectors.html#coordinates",
    "href": "chapters/01-vectors.html#coordinates",
    "title": "Vectors and Vector Spaces",
    "section": "9 Coordinates",
    "text": "9 Coordinates\nGiven a basis \\(\\{e_1,\\dots,e_n\\}\\), any vector \\(v \\in V\\) can be written uniquely as: \\[\nv = x_1 e_1 + \\cdots + x_n e_n.\n\\]\nThe coefficients \\((x_1,\\dots,x_n)\\) are the coordinates of \\(v\\) in that basis.",
    "crumbs": [
      "Foundations",
      "Vectors and Vector Spaces"
    ]
  },
  {
    "objectID": "chapters/01-vectors.html#computational-example-python",
    "href": "chapters/01-vectors.html#computational-example-python",
    "title": "Vectors and Vector Spaces",
    "section": "10 Computational example (Python)",
    "text": "10 Computational example (Python)\nimport numpy as np\n\nv1 = np.array([1, 0, 1])\nv2 = np.array([0, 1, 1])\nv3 = np.array([1, 1, 2])\n\n# Check linear dependence\nM = np.column_stack([v1, v2, v3])\nnp.linalg.matrix_rank(M)\nIf the rank is smaller than the number of vectors, they are linearly dependent.",
    "crumbs": [
      "Foundations",
      "Vectors and Vector Spaces"
    ]
  },
  {
    "objectID": "chapters/01-vectors.html#connection-to-data-science",
    "href": "chapters/01-vectors.html#connection-to-data-science",
    "title": "Vectors and Vector Spaces",
    "section": "11 Connection to Data Science",
    "text": "11 Connection to Data Science\nA dataset with \\(n\\) features lives in \\(\\mathbb{R}^n\\).\nFeature engineering changes the basis.\nDimensionality reduction (PCA, SVD) finds lower-dimensional subspaces.\nEmbeddings (word2vec, image embeddings) are vectors in high-dimensional spaces.\nUnderstanding vector spaces is the first step toward understanding geometry of data.",
    "crumbs": [
      "Foundations",
      "Vectors and Vector Spaces"
    ]
  },
  {
    "objectID": "chapters/04-spectral.html",
    "href": "chapters/04-spectral.html",
    "title": "Spectral Theory of Linear Operators",
    "section": "",
    "text": "Spectral theory studies how a linear operator acts by decomposing it into independent directions that are scaled.\nIn finite-dimensional real vector spaces, this means understanding:\n\neigenvalues,\neigenvectors,\ndiagonalization,\northogonal decompositions.\n\nIn Data Science, spectral theory underlies: - PCA, - spectral clustering, - graph Laplacians, - low-rank approximations.",
    "crumbs": [
      "Spectral theory & decompositions",
      "Spectral Theory of Linear Operators"
    ]
  },
  {
    "objectID": "chapters/04-spectral.html#motivation",
    "href": "chapters/04-spectral.html#motivation",
    "title": "Spectral Theory of Linear Operators",
    "section": "",
    "text": "Spectral theory studies how a linear operator acts by decomposing it into independent directions that are scaled.\nIn finite-dimensional real vector spaces, this means understanding:\n\neigenvalues,\neigenvectors,\ndiagonalization,\northogonal decompositions.\n\nIn Data Science, spectral theory underlies: - PCA, - spectral clustering, - graph Laplacians, - low-rank approximations.",
    "crumbs": [
      "Spectral theory & decompositions",
      "Spectral Theory of Linear Operators"
    ]
  },
  {
    "objectID": "chapters/04-spectral.html#eigenvalues-and-eigenvectors",
    "href": "chapters/04-spectral.html#eigenvalues-and-eigenvectors",
    "title": "Spectral Theory of Linear Operators",
    "section": "2 Eigenvalues and Eigenvectors",
    "text": "2 Eigenvalues and Eigenvectors\n\n\nDefinition (Eigenvalue and eigenvector)\n\nLet \\(T:V\\to V\\) be a linear operator on a vector space \\(V\\). A scalar \\(\\lambda\\in\\mathbb{R}\\) is an eigenvalue of \\(T\\) if there exists a nonzero vector \\(v\\in V\\) such that\n\\[\nT(v) = \\lambda v.\n\\]\nThe vector \\(v\\) is called an eigenvector associated with \\(\\lambda\\).\n\nFor a matrix \\(A\\in\\mathbb{R}^{n\\times n}\\), this reads:\n\\[\nAv = \\lambda v.\n\\]",
    "crumbs": [
      "Spectral theory & decompositions",
      "Spectral Theory of Linear Operators"
    ]
  },
  {
    "objectID": "chapters/04-spectral.html#the-characteristic-polynomial",
    "href": "chapters/04-spectral.html#the-characteristic-polynomial",
    "title": "Spectral Theory of Linear Operators",
    "section": "3 The Characteristic Polynomial",
    "text": "3 The Characteristic Polynomial\n\n\nDefinition (Characteristic polynomial)\n\nThe characteristic polynomial of \\(A\\in\\mathbb{R}^{n\\times n}\\) is\n\\[\np_A(\\lambda) := \\det(A - \\lambda I).\n\\]\n\nEigenvalues are the roots of \\(p_A\\).",
    "crumbs": [
      "Spectral theory & decompositions",
      "Spectral Theory of Linear Operators"
    ]
  },
  {
    "objectID": "chapters/04-spectral.html#algebraic-vs.-geometric-multiplicity",
    "href": "chapters/04-spectral.html#algebraic-vs.-geometric-multiplicity",
    "title": "Spectral Theory of Linear Operators",
    "section": "4 Algebraic vs. Geometric Multiplicity",
    "text": "4 Algebraic vs. Geometric Multiplicity\n\n\nDefinition\n\nLet \\(\\lambda\\) be an eigenvalue of \\(A\\).\n\nIts algebraic multiplicity is its multiplicity as a root of \\(p_A\\).\nIts geometric multiplicity is \\[\n\\dim \\ker(A - \\lambda I).\n\\]\n\n\nAlways: \\[\n1 \\le \\text{geometric multiplicity} \\le \\text{algebraic multiplicity}.\n\\]",
    "crumbs": [
      "Spectral theory & decompositions",
      "Spectral Theory of Linear Operators"
    ]
  },
  {
    "objectID": "chapters/04-spectral.html#diagonalization",
    "href": "chapters/04-spectral.html#diagonalization",
    "title": "Spectral Theory of Linear Operators",
    "section": "5 Diagonalization",
    "text": "5 Diagonalization\n\n\nDefinition (Diagonalizable matrix)\n\nA matrix \\(A\\in\\mathbb{R}^{n\\times n}\\) is diagonalizable if there exists an invertible matrix \\(P\\) such that\n\\[\nA = P D P^{-1},\n\\]\nwhere \\(D\\) is diagonal.\n\nDiagonal entries of \\(D\\) are eigenvalues of \\(A\\).",
    "crumbs": [
      "Spectral theory & decompositions",
      "Spectral Theory of Linear Operators"
    ]
  },
  {
    "objectID": "chapters/04-spectral.html#why-diagonalization-matters",
    "href": "chapters/04-spectral.html#why-diagonalization-matters",
    "title": "Spectral Theory of Linear Operators",
    "section": "6 Why Diagonalization Matters",
    "text": "6 Why Diagonalization Matters\nIf \\(A = P D P^{-1}\\), then:\n\npowers: \\(A^k = P D^k P^{-1}\\)\nexponentials: \\(e^{tA} = P e^{tD} P^{-1}\\)\ndynamics decouple into independent modes\n\nThis is why spectral theory is fundamental in: - linear dynamical systems, - diffusion processes, - graph methods.",
    "crumbs": [
      "Spectral theory & decompositions",
      "Spectral Theory of Linear Operators"
    ]
  },
  {
    "objectID": "chapters/04-spectral.html#self-adjoint-symmetric-operators",
    "href": "chapters/04-spectral.html#self-adjoint-symmetric-operators",
    "title": "Spectral Theory of Linear Operators",
    "section": "7 Self-Adjoint (Symmetric) Operators",
    "text": "7 Self-Adjoint (Symmetric) Operators\n\n\nDefinition (Self-adjoint operator)\n\nA linear operator \\(T:V\\to V\\) on an inner product space is self-adjoint if\n\\[\n\\langle T u, v \\rangle = \\langle u, T v \\rangle\n\\quad \\text{for all } u,v\\in V.\n\\]\n\nIn matrix form (Euclidean inner product):\n\\[\nA = A^\\top.\n\\]",
    "crumbs": [
      "Spectral theory & decompositions",
      "Spectral Theory of Linear Operators"
    ]
  },
  {
    "objectID": "chapters/04-spectral.html#the-spectral-theorem-finite-dimensional",
    "href": "chapters/04-spectral.html#the-spectral-theorem-finite-dimensional",
    "title": "Spectral Theory of Linear Operators",
    "section": "8 The Spectral Theorem (Finite-Dimensional)",
    "text": "8 The Spectral Theorem (Finite-Dimensional)\n\n\nTheorem (Spectral Theorem)\n\nLet \\(A\\in\\mathbb{R}^{n\\times n}\\) be symmetric. Then:\n\nAll eigenvalues of \\(A\\) are real.\nEigenvectors corresponding to distinct eigenvalues are orthogonal.\nThere exists an orthogonal matrix \\(Q\\) such that\n\n\\[\nA = Q \\Lambda Q^\\top,\n\\]\nwhere \\(\\Lambda\\) is diagonal.\n\n\n\n\nProof idea (click to expand)\n\n\nProof. \n\nProof (idea)\n\nThe proof uses: - the fact that symmetric matrices define real quadratic forms, - maximization of the Rayleigh quotient on the unit sphere, - induction on dimension.\nOrthogonality follows from symmetry: if \\(Av=\\lambda v\\) and \\(Aw=\\mu w\\) with \\(\\lambda\\neq\\mu\\), then\n\\[\n\\lambda \\langle v,w\\rangle\n= \\langle Av,w\\rangle\n= \\langle v,Aw\\rangle\n= \\mu \\langle v,w\\rangle,\n\\]\nso \\(\\langle v,w\\rangle=0\\). □",
    "crumbs": [
      "Spectral theory & decompositions",
      "Spectral Theory of Linear Operators"
    ]
  },
  {
    "objectID": "chapters/04-spectral.html#quadratic-forms-and-rayleigh-quotient",
    "href": "chapters/04-spectral.html#quadratic-forms-and-rayleigh-quotient",
    "title": "Spectral Theory of Linear Operators",
    "section": "9 Quadratic Forms and Rayleigh Quotient",
    "text": "9 Quadratic Forms and Rayleigh Quotient\n\n\nDefinition (Rayleigh quotient)\n\nFor a symmetric matrix \\(A\\) and nonzero \\(x\\):\n\\[\nR_A(x) := \\frac{x^\\top A x}{x^\\top x}.\n\\]\n\n\n\nTheorem\n\nThe maximum and minimum values of \\(R_A(x)\\) over \\(x\\neq 0\\) are the largest and smallest eigenvalues of \\(A\\).\n\nThis result is the variational foundation of PCA.",
    "crumbs": [
      "Spectral theory & decompositions",
      "Spectral Theory of Linear Operators"
    ]
  },
  {
    "objectID": "chapters/04-spectral.html#computational-example-python",
    "href": "chapters/04-spectral.html#computational-example-python",
    "title": "Spectral Theory of Linear Operators",
    "section": "10 Computational Example (Python)",
    "text": "10 Computational Example (Python)\nimport numpy as np\n\nA = np.array([[2, 1],\n              [1, 3]])\n\neigvals, eigvecs = np.linalg.eigh(A)\neigvals, eigvecs\n\nnp.linalg.eigh exploits symmetry.\nEigenvectors returned are orthonormal.",
    "crumbs": [
      "Spectral theory & decompositions",
      "Spectral Theory of Linear Operators"
    ]
  },
  {
    "objectID": "chapters/04-spectral.html#geometry-of-spectral-decomposition",
    "href": "chapters/04-spectral.html#geometry-of-spectral-decomposition",
    "title": "Spectral Theory of Linear Operators",
    "section": "11 Geometry of Spectral Decomposition",
    "text": "11 Geometry of Spectral Decomposition\nThe decomposition\n\\[\nA = \\sum_{i=1}^n \\lambda_i\\, v_i v_i^\\top\n\\]\nshows that a symmetric linear operator acts as: - projection onto the eigenvector \\(v_i\\), - followed by scaling by the eigenvalue \\(\\lambda_i\\).\nThis representation expresses \\(A\\) as a sum of rank-1 operators.",
    "crumbs": [
      "Spectral theory & decompositions",
      "Spectral Theory of Linear Operators"
    ]
  },
  {
    "objectID": "chapters/04-spectral.html#data-science-connections",
    "href": "chapters/04-spectral.html#data-science-connections",
    "title": "Spectral Theory of Linear Operators",
    "section": "12 Data Science Connections",
    "text": "12 Data Science Connections\n\n12.1 Principal Component Analysis (PCA)\nGiven a centered data matrix \\(X \\in \\mathbb{R}^{m \\times n}\\), the empirical covariance matrix is\n\\[\n\\mathrm{Cov}(X) = \\frac{1}{m} X^\\top X,\n\\]\nwhich is symmetric and positive semidefinite.\nPCA consists of computing the eigen-decomposition of \\(\\mathrm{Cov}(X)\\) and selecting the eigenvectors associated with the largest eigenvalues.\n\n\n\n12.2 Spectral Clustering\nIn spectral clustering, data points are viewed as nodes of a graph. The (normalized) graph Laplacian is a symmetric matrix.\nClustering is performed by embedding nodes using eigenvectors associated with the smallest nonzero eigenvalues of the Laplacian.\n\n\n\n12.3 Optimization and Conditioning\nQuadratic objectives of the form\n\\[\nf(x) = x^\\top A x\n\\]\nare governed by the spectrum of \\(A\\):\n\n\\(f\\) is convex if and only if \\(A \\succeq 0\\),\nthe conditioning of the problem is controlled by the ratio \\(\\lambda_{\\max} / \\lambda_{\\min}\\).",
    "crumbs": [
      "Spectral theory & decompositions",
      "Spectral Theory of Linear Operators"
    ]
  },
  {
    "objectID": "chapters/04-spectral.html#summary",
    "href": "chapters/04-spectral.html#summary",
    "title": "Spectral Theory of Linear Operators",
    "section": "13 Summary",
    "text": "13 Summary\n\nSymmetric matrices admit orthogonal diagonalization.\nSpectral decomposition expresses operators as sums of rank-1 projections.\nPCA and spectral clustering are direct consequences of spectral theory.\nOptimization behavior is dictated by eigenvalues.",
    "crumbs": [
      "Spectral theory & decompositions",
      "Spectral Theory of Linear Operators"
    ]
  },
  {
    "objectID": "chapters/04-spectral.html#preview-singular-value-decomposition",
    "href": "chapters/04-spectral.html#preview-singular-value-decomposition",
    "title": "Spectral Theory of Linear Operators",
    "section": "14 Preview: Singular Value Decomposition",
    "text": "14 Preview: Singular Value Decomposition\nNot all matrices are symmetric. The Singular Value Decomposition (SVD) extends spectral ideas to arbitrary rectangular matrices and is the central decomposition in modern Data Science.",
    "crumbs": [
      "Spectral theory & decompositions",
      "Spectral Theory of Linear Operators"
    ]
  },
  {
    "objectID": "chapters/00-linear-systems.html",
    "href": "chapters/00-linear-systems.html",
    "title": "Solving Linear Systems: LU, Cholesky and Numerical Stability",
    "section": "",
    "text": "Many problems in Linear Algebra — and most optimization problems in Data Science — boil down to solving a linear system\n\\[\nAx = b\n\\]\nefficiently and numerically stably.\nDifferent matrix factorizations exist for this task.\nThey do not all serve the same purpose:\n\nsome are fast but fragile,\nsome are slower but stable,\nsome exploit special structure (symmetry, positivity).\n\nIn this chapter we study LU, PA = LU, and Cholesky decompositions, and clarify their role in Data Science.",
    "crumbs": [
      "Foundations",
      "Solving Linear Systems: LU, Cholesky and Numerical Stability"
    ]
  },
  {
    "objectID": "chapters/00-linear-systems.html#motivation",
    "href": "chapters/00-linear-systems.html#motivation",
    "title": "Solving Linear Systems: LU, Cholesky and Numerical Stability",
    "section": "",
    "text": "Many problems in Linear Algebra — and most optimization problems in Data Science — boil down to solving a linear system\n\\[\nAx = b\n\\]\nefficiently and numerically stably.\nDifferent matrix factorizations exist for this task.\nThey do not all serve the same purpose:\n\nsome are fast but fragile,\nsome are slower but stable,\nsome exploit special structure (symmetry, positivity).\n\nIn this chapter we study LU, PA = LU, and Cholesky decompositions, and clarify their role in Data Science.",
    "crumbs": [
      "Foundations",
      "Solving Linear Systems: LU, Cholesky and Numerical Stability"
    ]
  },
  {
    "objectID": "chapters/00-linear-systems.html#solving-linear-systems-the-big-picture",
    "href": "chapters/00-linear-systems.html#solving-linear-systems-the-big-picture",
    "title": "Solving Linear Systems: LU, Cholesky and Numerical Stability",
    "section": "2 Solving Linear Systems: the Big Picture",
    "text": "2 Solving Linear Systems: the Big Picture\nGiven \\(A \\in \\mathbb{R}^{n \\times n}\\), there are three classical approaches:\n\nCompute \\(A^{-1}\\) explicitly (❌ almost never recommended)\nFactorize \\(A\\) once, then solve cheaply for many right-hand sides\nReformulate the problem to exploit structure (symmetry, positivity)\n\nMatrix factorizations implement (2) and (3).",
    "crumbs": [
      "Foundations",
      "Solving Linear Systems: LU, Cholesky and Numerical Stability"
    ]
  },
  {
    "objectID": "chapters/00-linear-systems.html#lu-decomposition",
    "href": "chapters/00-linear-systems.html#lu-decomposition",
    "title": "Solving Linear Systems: LU, Cholesky and Numerical Stability",
    "section": "3 LU Decomposition",
    "text": "3 LU Decomposition\n\n\nDefinition (LU decomposition)\n\nA matrix \\(A \\in \\mathbb{R}^{n \\times n}\\) admits an LU decomposition if it can be written as\n\\[\nA = LU\n\\]\nwhere: - \\(L\\) is lower triangular with ones on the diagonal, - \\(U\\) is upper triangular.\n\n\n\n3.1 Solving a system with LU\nIf \\(A = LU\\), then solving \\(Ax=b\\) reduces to:\n\nSolve \\(Ly = b\\) (forward substitution)\nSolve \\(Ux = y\\) (back substitution)\n\nBoth steps cost \\(\\mathcal{O}(n^2)\\) operations.\n\n\nProposition\n\nLet \\(A \\in \\mathbb{R}^{n \\times n}\\) admit an LU decomposition \\(A = LU\\), where \\(L\\) is lower triangular with ones on the diagonal and \\(U\\) is upper triangular. Then solving the linear system\n\\[\nAx = b\n\\]\nreduces to solving the two triangular systems:\n\n\\(Ly = b\\) (forward substitution)\n\n\\(Ux = y\\) (back substitution)\n\nEach step requires \\(\\mathcal{O}(n^2)\\) operations.\n\n\n\nProof (click to expand)\n\n\nProof. \n\nProof\n\nAssume that \\(A = LU\\) with \\(L\\) lower triangular and \\(U\\) upper triangular. Then the system \\(Ax = b\\) can be written as\n\\[\nLUx = b.\n\\]\nDefine an auxiliary variable \\(y := Ux\\). The system becomes\n\\[\nLy = b.\n\\]\nSince \\(L\\) is lower triangular with nonzero diagonal entries, this system can be solved uniquely by forward substitution, computing \\(y_1, y_2, \\dots, y_n\\) sequentially.\nOnce \\(y\\) is known, we solve\n\\[\nUx = y.\n\\]\nBecause \\(U\\) is upper triangular with nonzero diagonal entries, this system can be solved uniquely by back substitution, computing \\(x_n, x_{n-1}, \\dots, x_1\\) sequentially.\nEach substitution step involves summations over at most \\(n\\) terms, and there are \\(n\\) unknowns. Therefore, each triangular solve costs \\(\\mathcal{O}(n^2)\\) operations.\nThis proves that solving \\(Ax=b\\) using an LU decomposition requires two triangular solves, each with quadratic complexity.\n□\n\n\n\n\n\n3.2 Limitations of LU\nLU decomposition may fail or be numerically unstable if: - a pivot element is zero or very small, - the matrix is ill-conditioned.\nThis motivates pivoting.",
    "crumbs": [
      "Foundations",
      "Solving Linear Systems: LU, Cholesky and Numerical Stability"
    ]
  },
  {
    "objectID": "chapters/00-linear-systems.html#lu-with-pivoting-pa-lu",
    "href": "chapters/00-linear-systems.html#lu-with-pivoting-pa-lu",
    "title": "Solving Linear Systems: LU, Cholesky and Numerical Stability",
    "section": "4 LU with Pivoting (PA = LU)",
    "text": "4 LU with Pivoting (PA = LU)\n\n\nDefinition (LU with partial pivoting)\n\nFor any square matrix \\(A\\), there exists a permutation matrix \\(P\\) such that\n\\[\nPA = LU\n\\]\nwhere \\(L\\) and \\(U\\) are triangular matrices.\n\n\n\\(P\\) reorders the rows of \\(A\\) to ensure numerical stability.\nThis is the standard LU decomposition used in practice.\n\n\nRemark. \n\nRemark\n\nWhen numerical libraries say “LU”, they almost always mean PA = LU.",
    "crumbs": [
      "Foundations",
      "Solving Linear Systems: LU, Cholesky and Numerical Stability"
    ]
  },
  {
    "objectID": "chapters/00-linear-systems.html#computational-example-python",
    "href": "chapters/00-linear-systems.html#computational-example-python",
    "title": "Solving Linear Systems: LU, Cholesky and Numerical Stability",
    "section": "5 Computational Example (Python)",
    "text": "5 Computational Example (Python)\nimport numpy as np\nfrom scipy.linalg import lu\n\nA = np.array([[0., 2., 1.],\n              [1., 1., 0.],\n              [2., 1., 1.]])\n\nP, L, U = lu(A)\nP, L, U\n#Verification\n\nnp.allclose(P @ A, L @ U)",
    "crumbs": [
      "Foundations",
      "Solving Linear Systems: LU, Cholesky and Numerical Stability"
    ]
  },
  {
    "objectID": "chapters/00-linear-systems.html#cholesky-decomposition",
    "href": "chapters/00-linear-systems.html#cholesky-decomposition",
    "title": "Solving Linear Systems: LU, Cholesky and Numerical Stability",
    "section": "6 Cholesky Decomposition",
    "text": "6 Cholesky Decomposition\nLU decomposition does not exploit matrix structure. When a matrix is symmetric positive definite, a more efficient factorization exists.\n\n\nDefinition (Cholesky decomposition)\n\nLet \\(A \\in \\mathbb{R}^{n \\times n}\\) be symmetric positive definite. Then there exists a unique lower triangular matrix \\(L\\) with positive diagonal entries such that\n\\[\nA = LL^\\top\n\\]",
    "crumbs": [
      "Foundations",
      "Solving Linear Systems: LU, Cholesky and Numerical Stability"
    ]
  },
  {
    "objectID": "chapters/00-linear-systems.html#why-cholesky-is-special",
    "href": "chapters/00-linear-systems.html#why-cholesky-is-special",
    "title": "Solving Linear Systems: LU, Cholesky and Numerical Stability",
    "section": "7 Why Cholesky Is Special",
    "text": "7 Why Cholesky Is Special\nCompared to LU decomposition:\n\nit is about twice as fast,\nit is more numerically stable,\nit uses roughly half the memory,\nit exploits symmetry and positivity.\n\nHowever:\n\n❌ it fails if \\(A\\) is not positive definite.",
    "crumbs": [
      "Foundations",
      "Solving Linear Systems: LU, Cholesky and Numerical Stability"
    ]
  },
  {
    "objectID": "chapters/00-linear-systems.html#cholesky-and-quadratic-problems",
    "href": "chapters/00-linear-systems.html#cholesky-and-quadratic-problems",
    "title": "Solving Linear Systems: LU, Cholesky and Numerical Stability",
    "section": "8 Cholesky and Quadratic Problems",
    "text": "8 Cholesky and Quadratic Problems\nMany optimization problems in mathematics and Data Science involve minimizing a quadratic form:\n\\[\n\\min_x \\; \\frac{1}{2} x^\\top Q x - b^\\top x\n\\]\nwhere \\(Q \\succ 0\\).\nThe first-order optimality condition is:\n\\[\nQx = b\n\\]\nThis linear system is solved directly using the Cholesky decomposition of \\(Q\\).",
    "crumbs": [
      "Foundations",
      "Solving Linear Systems: LU, Cholesky and Numerical Stability"
    ]
  },
  {
    "objectID": "chapters/00-linear-systems.html#data-science-applications",
    "href": "chapters/00-linear-systems.html#data-science-applications",
    "title": "Solving Linear Systems: LU, Cholesky and Numerical Stability",
    "section": "9 Data Science Applications",
    "text": "9 Data Science Applications\n\n9.1 Least Squares via Normal Equations\nGiven a data matrix \\(X \\in \\mathbb{R}^{m \\times n}\\) and a target vector \\(y \\in \\mathbb{R}^m\\), linear regression solves:\n\\[\n\\min_w \\|Xw - y\\|_2^2\n\\]\nThis leads to the normal equations:\n\\[\nX^\\top X w = X^\\top y\n\\]\nIf \\(X\\) has full column rank, then \\(X^\\top X\\) is symmetric positive definite and can be solved efficiently using Cholesky decomposition.\n\n\n\n9.2 Gaussian Models\nIn multivariate Gaussian models, we often need to compute:\n\nthe inverse of the covariance matrix \\(\\Sigma^{-1}\\),\nthe log-determinant \\(\\log \\det \\Sigma\\),\nsamples from \\(\\mathcal{N}(0, \\Sigma)\\).\n\nAll of these operations rely on the factorization\n\\[\n\\Sigma = LL^\\top\n\\]\nThis appears in:\n\nGaussian Processes,\nKalman filters,\nBayesian linear regression.",
    "crumbs": [
      "Foundations",
      "Solving Linear Systems: LU, Cholesky and Numerical Stability"
    ]
  },
  {
    "objectID": "chapters/00-linear-systems.html#numerical-stability-matters",
    "href": "chapters/00-linear-systems.html#numerical-stability-matters",
    "title": "Solving Linear Systems: LU, Cholesky and Numerical Stability",
    "section": "10 Numerical Stability Matters",
    "text": "10 Numerical Stability Matters\nTwo mathematically equivalent formulations can behave very differently numerically.\n\nRemark. \n\nRemark\n\nComputing \\(A^{-1}b\\) explicitly is almost always worse than solving \\(Ax = b\\) using a matrix factorization.\n\nStability considerations explain why:\n\nLU is used with pivoting,\nCholesky is preferred whenever applicable,\nQR or SVD are used when rank deficiency is possible.",
    "crumbs": [
      "Foundations",
      "Solving Linear Systems: LU, Cholesky and Numerical Stability"
    ]
  },
  {
    "objectID": "chapters/00-linear-systems.html#what-these-decompositions-do-not-do",
    "href": "chapters/00-linear-systems.html#what-these-decompositions-do-not-do",
    "title": "Solving Linear Systems: LU, Cholesky and Numerical Stability",
    "section": "11 What These Decompositions Do Not Do",
    "text": "11 What These Decompositions Do Not Do\nLU and Cholesky decompositions:\n\ndo not analyze the geometry of data,\ndo not reduce dimensionality,\ndo not reveal latent structure.\n\nThese tasks belong to spectral decompositions such as SVD and eigenvalue decompositions.",
    "crumbs": [
      "Foundations",
      "Solving Linear Systems: LU, Cholesky and Numerical Stability"
    ]
  },
  {
    "objectID": "chapters/00-linear-systems.html#summary",
    "href": "chapters/00-linear-systems.html#summary",
    "title": "Solving Linear Systems: LU, Cholesky and Numerical Stability",
    "section": "12 Summary",
    "text": "12 Summary\n\nSolving linear systems is central to optimization and learning.\nCholesky decomposition is optimal for symmetric positive definite matrices.\nIt is a key computational tool behind regression and Gaussian models.\nNumerical stability dictates which decomposition should be used.\nUnderstanding these methods is essential before studying spectral techniques.",
    "crumbs": [
      "Foundations",
      "Solving Linear Systems: LU, Cholesky and Numerical Stability"
    ]
  },
  {
    "objectID": "chapters/00-linear-systems.html#preview-qr-and-least-squares",
    "href": "chapters/00-linear-systems.html#preview-qr-and-least-squares",
    "title": "Solving Linear Systems: LU, Cholesky and Numerical Stability",
    "section": "13 Preview: QR and Least Squares",
    "text": "13 Preview: QR and Least Squares\nWhen \\(X\\) is rectangular or ill-conditioned, solving the normal equations is unstable.\nA more robust approach is based on the QR decomposition:\n\\[\nX = QR\n\\]\nThis leads to numerically stable least squares solvers and prepares the ground for the Singular Value Decomposition.",
    "crumbs": [
      "Foundations",
      "Solving Linear Systems: LU, Cholesky and Numerical Stability"
    ]
  },
  {
    "objectID": "chapters/03-inner-products.html",
    "href": "chapters/03-inner-products.html",
    "title": "Inner Products and Geometry",
    "section": "",
    "text": "Vector spaces become geometric once we introduce an inner product. This allows us to measure:\n\nlengths (norms),\nangles,\ndistances,\northogonality and projections.\n\nIn Data Science, these notions appear as: - Euclidean distance, - cosine similarity, - least squares, - PCA geometry.",
    "crumbs": [
      "Foundations",
      "Inner Products and Geometry"
    ]
  },
  {
    "objectID": "chapters/03-inner-products.html#motivation",
    "href": "chapters/03-inner-products.html#motivation",
    "title": "Inner Products and Geometry",
    "section": "",
    "text": "Vector spaces become geometric once we introduce an inner product. This allows us to measure:\n\nlengths (norms),\nangles,\ndistances,\northogonality and projections.\n\nIn Data Science, these notions appear as: - Euclidean distance, - cosine similarity, - least squares, - PCA geometry.",
    "crumbs": [
      "Foundations",
      "Inner Products and Geometry"
    ]
  },
  {
    "objectID": "chapters/03-inner-products.html#inner-products",
    "href": "chapters/03-inner-products.html#inner-products",
    "title": "Inner Products and Geometry",
    "section": "2 Inner Products",
    "text": "2 Inner Products\n\n\nDefinition (Inner product)\n\nLet \\(V\\) be a real vector space. An inner product on \\(V\\) is a map \\(\\langle \\cdot,\\cdot\\rangle : V\\times V \\to \\mathbb{R}\\) such that for all \\(u,v,w\\in V\\) and \\(\\alpha\\in\\mathbb{R}\\):\n\n\\(\\langle u,v\\rangle = \\langle v,u\\rangle\\) (symmetry)\n\\(\\langle \\alpha u + v, w\\rangle = \\alpha\\langle u,w\\rangle + \\langle v,w\\rangle\\) (linearity)\n\\(\\langle v,v\\rangle \\ge 0\\), with equality iff \\(v=0\\) (positive definiteness)",
    "crumbs": [
      "Foundations",
      "Inner Products and Geometry"
    ]
  },
  {
    "objectID": "chapters/03-inner-products.html#canonical-examples",
    "href": "chapters/03-inner-products.html#canonical-examples",
    "title": "Inner Products and Geometry",
    "section": "3 Canonical Examples",
    "text": "3 Canonical Examples\n\n3.1 Euclidean inner product\n\n\nExample (Euclidean inner product)\n\nOn \\(\\mathbb{R}^n\\): \\[\n\\langle x,y\\rangle := x^\\top y = \\sum_{i=1}^n x_i y_i.\n\\]\n\n\n\n3.2 Weighted inner product\n\n\nExample (Weighted inner product)\n\nLet \\(W\\) be a symmetric positive definite matrix. Define: \\[\n\\langle x,y\\rangle_W := x^\\top W y.\n\\]\nThis appears in generalized least squares.",
    "crumbs": [
      "Foundations",
      "Inner Products and Geometry"
    ]
  },
  {
    "objectID": "chapters/03-inner-products.html#norms-and-distances",
    "href": "chapters/03-inner-products.html#norms-and-distances",
    "title": "Inner Products and Geometry",
    "section": "4 Norms and Distances",
    "text": "4 Norms and Distances\n\n\nDefinition (Norm induced by an inner product)\n\nThe norm induced by \\(\\langle\\cdot,\\cdot\\rangle\\) is: \\[\n\\|v\\| := \\sqrt{\\langle v,v\\rangle}.\n\\]\n\nThe associated distance is: \\[\nd(u,v) := \\|u-v\\|.\n\\]",
    "crumbs": [
      "Foundations",
      "Inner Products and Geometry"
    ]
  },
  {
    "objectID": "chapters/03-inner-products.html#cauchyschwarz-inequality",
    "href": "chapters/03-inner-products.html#cauchyschwarz-inequality",
    "title": "Inner Products and Geometry",
    "section": "5 Cauchy–Schwarz Inequality",
    "text": "5 Cauchy–Schwarz Inequality\n\n\nTheorem (Cauchy–Schwarz)\n\nFor all \\(u,v\\in V\\): \\[\n|\\langle u,v\\rangle| \\le \\|u\\|\\,\\|v\\|.\n\\]\n\n\nProof. \n\nProof (idea)\n\nConsider the function \\(f(t)=\\|u-tv\\|^2 \\ge 0\\) for all \\(t\\in\\mathbb{R}\\). Expanding yields a quadratic polynomial in \\(t\\) whose discriminant must be non-positive, which implies the inequality. □",
    "crumbs": [
      "Foundations",
      "Inner Products and Geometry"
    ]
  },
  {
    "objectID": "chapters/03-inner-products.html#angles-and-cosine-similarity",
    "href": "chapters/03-inner-products.html#angles-and-cosine-similarity",
    "title": "Inner Products and Geometry",
    "section": "6 Angles and Cosine Similarity",
    "text": "6 Angles and Cosine Similarity\nFor nonzero vectors \\(u,v\\), define the angle \\(\\theta\\) by: \\[\n\\cos\\theta = \\frac{\\langle u,v\\rangle}{\\|u\\|\\|v\\|}.\n\\]\n\n\nExample (Cosine similarity)\n\nIn Data Science, the quantity \\[\n\\frac{\\langle u,v\\rangle}{\\|u\\|\\|v\\|}\n\\] is known as cosine similarity, widely used in NLP and recommender systems.",
    "crumbs": [
      "Foundations",
      "Inner Products and Geometry"
    ]
  },
  {
    "objectID": "chapters/03-inner-products.html#orthogonality",
    "href": "chapters/03-inner-products.html#orthogonality",
    "title": "Inner Products and Geometry",
    "section": "7 Orthogonality",
    "text": "7 Orthogonality\n\n\nDefinition (Orthogonal vectors)\n\nVectors \\(u\\) and \\(v\\) are orthogonal if \\[\n\\langle u,v\\rangle = 0.\n\\]\n\nOrthogonality generalizes the notion of “perpendicular”.",
    "crumbs": [
      "Foundations",
      "Inner Products and Geometry"
    ]
  },
  {
    "objectID": "chapters/03-inner-products.html#orthogonal-subspaces",
    "href": "chapters/03-inner-products.html#orthogonal-subspaces",
    "title": "Inner Products and Geometry",
    "section": "8 Orthogonal Subspaces",
    "text": "8 Orthogonal Subspaces\n\n\nDefinition (Orthogonal complement)\n\nGiven a subspace \\(W \\subset V\\), its orthogonal complement is: \\[\nW^\\perp := \\{v\\in V : \\langle v,w\\rangle = 0 \\text{ for all } w\\in W\\}.\n\\]\n\n\n\nTheorem\n\nIf \\(V\\) is finite-dimensional, then: \\[\nV = W \\oplus W^\\perp.\n\\]",
    "crumbs": [
      "Foundations",
      "Inner Products and Geometry"
    ]
  },
  {
    "objectID": "chapters/03-inner-products.html#projections",
    "href": "chapters/03-inner-products.html#projections",
    "title": "Inner Products and Geometry",
    "section": "9 Projections",
    "text": "9 Projections\n\n\nTheorem (Orthogonal projection)\n\nLet \\(W\\) be a finite-dimensional subspace of \\(V\\). For every \\(v\\in V\\), there exists a unique decomposition: \\[\nv = w + w^\\perp,\n\\quad w\\in W,\\; w^\\perp\\in W^\\perp.\n\\]\nThe vector \\(w\\) is the orthogonal projection of \\(v\\) onto \\(W\\).\n\nThis theorem is the geometric heart of least squares.",
    "crumbs": [
      "Foundations",
      "Inner Products and Geometry"
    ]
  },
  {
    "objectID": "chapters/03-inner-products.html#computational-example-python",
    "href": "chapters/03-inner-products.html#computational-example-python",
    "title": "Inner Products and Geometry",
    "section": "10 Computational Example (Python)",
    "text": "10 Computational Example (Python)\nimport numpy as np\n\n# vectors\nu = np.array([1.0, 2.0])\nv = np.array([2.0, -1.0])\n\n# inner product, norms, cosine\ninner = u @ v\nnorm_u = np.linalg.norm(u)\nnorm_v = np.linalg.norm(v)\ncos_theta = inner / (norm_u * norm_v)\n\ninner, norm_u, norm_v, cos_theta",
    "crumbs": [
      "Foundations",
      "Inner Products and Geometry"
    ]
  },
  {
    "objectID": "chapters/03-inner-products.html#geometry-of-data",
    "href": "chapters/03-inner-products.html#geometry-of-data",
    "title": "Inner Products and Geometry",
    "section": "11 Geometry of Data",
    "text": "11 Geometry of Data\n\nEuclidean distance corresponds to squared loss.\nCosine similarity ignores magnitude and focuses on direction.\nOrthogonality means no linear correlation.\nProjections give the geometric meaning of regression.",
    "crumbs": [
      "Foundations",
      "Inner Products and Geometry"
    ]
  },
  {
    "objectID": "chapters/03-inner-products.html#preview-least-squares",
    "href": "chapters/03-inner-products.html#preview-least-squares",
    "title": "Inner Products and Geometry",
    "section": "12 Preview: Least Squares",
    "text": "12 Preview: Least Squares\nGiven \\(A \\in \\mathbb{R}^{m \\times n}\\) and \\(b \\in \\mathbb{R}^m\\), least squares finds the projection of \\(b\\) onto the column space of \\(A\\).\n\\[\n\\min_{x \\in \\mathbb{R}^n} \\|Ax - b\\|_2^2\n\\]\nThis problem is solved entirely using inner products and orthogonality.",
    "crumbs": [
      "Foundations",
      "Inner Products and Geometry"
    ]
  },
  {
    "objectID": "chapters/03-inner-products.html#summary",
    "href": "chapters/03-inner-products.html#summary",
    "title": "Inner Products and Geometry",
    "section": "13 Summary",
    "text": "13 Summary\n\nInner products turn algebra into geometry.\nNorms, angles, and distances arise naturally.\nOrthogonality and projections are central concepts.\nLeast squares and PCA are geometric problems.",
    "crumbs": [
      "Foundations",
      "Inner Products and Geometry"
    ]
  },
  {
    "objectID": "chapters/08-spectral-clustering.html",
    "href": "chapters/08-spectral-clustering.html",
    "title": "Spectral Clustering",
    "section": "",
    "text": "Many datasets exhibit cluster structure that cannot be captured by linear separation in the original feature space.\nSpectral clustering approaches this problem by:\n\nrepresenting data as a graph,\nusing eigenvectors of a graph Laplacian,\nembedding data into a low-dimensional spectral space,\nperforming clustering in that space.\n\nThis method is fundamentally based on spectral theory and connects linear algebra, graph theory, and geometry.",
    "crumbs": [
      "Data Science Applications",
      "Spectral Clustering"
    ]
  },
  {
    "objectID": "chapters/08-spectral-clustering.html#motivation",
    "href": "chapters/08-spectral-clustering.html#motivation",
    "title": "Spectral Clustering",
    "section": "",
    "text": "Many datasets exhibit cluster structure that cannot be captured by linear separation in the original feature space.\nSpectral clustering approaches this problem by:\n\nrepresenting data as a graph,\nusing eigenvectors of a graph Laplacian,\nembedding data into a low-dimensional spectral space,\nperforming clustering in that space.\n\nThis method is fundamentally based on spectral theory and connects linear algebra, graph theory, and geometry.",
    "crumbs": [
      "Data Science Applications",
      "Spectral Clustering"
    ]
  },
  {
    "objectID": "chapters/08-spectral-clustering.html#from-data-to-graphs",
    "href": "chapters/08-spectral-clustering.html#from-data-to-graphs",
    "title": "Spectral Clustering",
    "section": "2 From Data to Graphs",
    "text": "2 From Data to Graphs\nLet \\(\\{x_1,\\dots,x_n\\} \\subset \\mathbb{R}^d\\) be a dataset.\nWe construct a weighted, undirected graph: - nodes correspond to data points, - edges encode similarity.",
    "crumbs": [
      "Data Science Applications",
      "Spectral Clustering"
    ]
  },
  {
    "objectID": "chapters/08-spectral-clustering.html#similarity-matrix",
    "href": "chapters/08-spectral-clustering.html#similarity-matrix",
    "title": "Spectral Clustering",
    "section": "3 Similarity Matrix",
    "text": "3 Similarity Matrix\n\n\nDefinition (Similarity matrix)\n\nThe similarity matrix \\(W \\in \\mathbb{R}^{n \\times n}\\) is defined by\n\\[\nW_{ij} = s(x_i, x_j),\n\\]\nwhere \\(s(\\cdot,\\cdot)\\) is a symmetric similarity function.\nCommon choices include: - Gaussian kernel: \\[\n  W_{ij} = \\exp\\!\\left(-\\frac{\\|x_i - x_j\\|^2}{2\\sigma^2}\\right),\n  \\] - \\(k\\)-nearest neighbors (binary or weighted).\nWe assume \\(W_{ij} \\ge 0\\) and \\(W_{ii} = 0\\).",
    "crumbs": [
      "Data Science Applications",
      "Spectral Clustering"
    ]
  },
  {
    "objectID": "chapters/08-spectral-clustering.html#degree-matrix",
    "href": "chapters/08-spectral-clustering.html#degree-matrix",
    "title": "Spectral Clustering",
    "section": "4 Degree Matrix",
    "text": "4 Degree Matrix\n\n\nDefinition (Degree matrix)\n\nThe degree matrix \\(D\\) is diagonal, with entries\n\\[\nD_{ii} = \\sum_{j=1}^n W_{ij}.\n\\]",
    "crumbs": [
      "Data Science Applications",
      "Spectral Clustering"
    ]
  },
  {
    "objectID": "chapters/08-spectral-clustering.html#graph-laplacians",
    "href": "chapters/08-spectral-clustering.html#graph-laplacians",
    "title": "Spectral Clustering",
    "section": "5 Graph Laplacians",
    "text": "5 Graph Laplacians\n\n5.1 Unnormalized Laplacian\n\n\nDefinition (Unnormalized Laplacian)\n\nThe unnormalized graph Laplacian is\n\\[\nL = D - W.\n\\]\n\n\n\n\n5.2 Normalized Laplacians\nTwo normalized variants are commonly used.\n\n\nDefinition (Normalized Laplacians)\n\nThe random-walk Laplacian is\n\\[\nL_{\\mathrm{rw}} = I - D^{-1} W.\n\\]\nThe symmetric normalized Laplacian is\n\\[\nL_{\\mathrm{sym}} = I - D^{-1/2} W D^{-1/2}.\n\\]\n\nIn practice, \\(L_{\\mathrm{sym}}\\) is the most widely used.",
    "crumbs": [
      "Data Science Applications",
      "Spectral Clustering"
    ]
  },
  {
    "objectID": "chapters/08-spectral-clustering.html#spectral-properties",
    "href": "chapters/08-spectral-clustering.html#spectral-properties",
    "title": "Spectral Clustering",
    "section": "6 Spectral Properties",
    "text": "6 Spectral Properties\n\n\nTheorem\n\nFor any graph Laplacian \\(L\\):\n\n\\(L\\) is symmetric positive semidefinite.\nThe smallest eigenvalue is \\(0\\).\nThe multiplicity of the eigenvalue \\(0\\) equals the number of connected components of the graph.\n\n\n\n\n\nWhy the Laplacian is positive semidefinite (click to expand)\n\n\nProof. \n\nProof\n\nFor the unnormalized Laplacian \\(L = D - W\\) and any \\(f \\in \\mathbb{R}^n\\),\n\\[\nf^\\top L f\n= \\frac{1}{2} \\sum_{i,j} W_{ij} (f_i - f_j)^2 \\ge 0.\n\\]\nHence \\(L\\) is positive semidefinite. □",
    "crumbs": [
      "Data Science Applications",
      "Spectral Clustering"
    ]
  },
  {
    "objectID": "chapters/08-spectral-clustering.html#clustering-as-a-graph-cut-problem",
    "href": "chapters/08-spectral-clustering.html#clustering-as-a-graph-cut-problem",
    "title": "Spectral Clustering",
    "section": "7 Clustering as a Graph Cut Problem",
    "text": "7 Clustering as a Graph Cut Problem\nClustering can be formulated as minimizing a graph cut:\n\\[\n\\mathrm{cut}(A,B) = \\sum_{i \\in A,\\, j \\in B} W_{ij}.\n\\]\nHowever, minimizing raw cuts leads to unbalanced partitions.\nNormalized objectives such as: - Ratio Cut - Normalized Cut\nlead naturally to spectral relaxations involving Laplacians.",
    "crumbs": [
      "Data Science Applications",
      "Spectral Clustering"
    ]
  },
  {
    "objectID": "chapters/08-spectral-clustering.html#spectral-relaxation",
    "href": "chapters/08-spectral-clustering.html#spectral-relaxation",
    "title": "Spectral Clustering",
    "section": "8 Spectral Relaxation",
    "text": "8 Spectral Relaxation\nThe discrete clustering problem is NP-hard. Spectral clustering replaces it by a continuous optimization problem:\n\nrelax indicator vectors to real-valued functions,\nimpose orthogonality constraints,\nsolve using eigenvectors of the Laplacian.\n\nThe solution is given by the eigenvectors associated with the smallest nonzero eigenvalues.",
    "crumbs": [
      "Data Science Applications",
      "Spectral Clustering"
    ]
  },
  {
    "objectID": "chapters/08-spectral-clustering.html#spectral-clustering-algorithm",
    "href": "chapters/08-spectral-clustering.html#spectral-clustering-algorithm",
    "title": "Spectral Clustering",
    "section": "9 Spectral Clustering Algorithm",
    "text": "9 Spectral Clustering Algorithm\nUsing the symmetric normalized Laplacian \\(L_{\\mathrm{sym}}\\):\n\nConstruct the similarity matrix \\(W\\).\nCompute \\(D\\) and \\(L_{\\mathrm{sym}}\\).\nCompute the first \\(k\\) eigenvectors of \\(L_{\\mathrm{sym}}\\).\nForm the matrix \\(U \\in \\mathbb{R}^{n \\times k}\\) from these eigenvectors.\nNormalize rows of \\(U\\) to unit length.\nApply \\(k\\)-means to the rows of \\(U\\).\n\nThe final labels are the cluster assignments.",
    "crumbs": [
      "Data Science Applications",
      "Spectral Clustering"
    ]
  },
  {
    "objectID": "chapters/08-spectral-clustering.html#geometric-interpretation",
    "href": "chapters/08-spectral-clustering.html#geometric-interpretation",
    "title": "Spectral Clustering",
    "section": "10 Geometric Interpretation",
    "text": "10 Geometric Interpretation\n\nEigenvectors embed nodes into a low-dimensional space.\nIn this space, clusters become approximately convex and separable.\n\\(k\\)-means works effectively in the spectral embedding.\n\nSpectral clustering is thus: &gt; nonlinear clustering via linear algebra.",
    "crumbs": [
      "Data Science Applications",
      "Spectral Clustering"
    ]
  },
  {
    "objectID": "chapters/08-spectral-clustering.html#computational-example-python",
    "href": "chapters/08-spectral-clustering.html#computational-example-python",
    "title": "Spectral Clustering",
    "section": "11 Computational Example (Python)",
    "text": "11 Computational Example (Python)\n\n\nCode\n```{python}\nimport numpy as np\nfrom sklearn.datasets import make_moons\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics.pairwise import rbf_kernel\nfrom scipy.linalg import eigh\n\n# Generate nonlinearly separable data\nX, _ = make_moons(n_samples=300, noise=0.05)\n\n# Similarity matrix\nW = rbf_kernel(X, gamma=10.0)\n\n# Degree matrix\nD = np.diag(W.sum(axis=1))\n\n# Symmetric normalized Laplacian\nD_inv_sqrt = np.diag(1.0 / np.sqrt(np.diag(D)))\nL_sym = np.eye(len(X)) - D_inv_sqrt @ W @ D_inv_sqrt\n\n# Eigen-decomposition\neigvals, eigvecs = eigh(L_sym)\nU = eigvecs[:, :2]\n\n# Row normalization\nU_norm = U / np.linalg.norm(U, axis=1, keepdims=True)\n\n# k-means\nlabels = KMeans(n_clusters=2, n_init=10).fit_predict(U_norm)\nlabels[:10]\n```\n\n\narray([0, 0, 1, 0, 0, 0, 0, 0, 0, 0], dtype=int32)",
    "crumbs": [
      "Data Science Applications",
      "Spectral Clustering"
    ]
  },
  {
    "objectID": "chapters/08-spectral-clustering.html#choosing-the-number-of-clusters",
    "href": "chapters/08-spectral-clustering.html#choosing-the-number-of-clusters",
    "title": "Spectral Clustering",
    "section": "12 Choosing the Number of Clusters",
    "text": "12 Choosing the Number of Clusters\nChoosing the number of clusters \\(k\\) is a central modeling decision in spectral clustering.\nCommon strategies include:\n\nMultiplicity of eigenvalue \\(0\\):\nIn the ideal case of perfectly disconnected components, the number of connected components equals the multiplicity of the eigenvalue \\(0\\) of the graph Laplacian.\nEigengap heuristic:\nSort eigenvalues \\(\\lambda_1 \\le \\lambda_2 \\le \\cdots\\) of the Laplacian. A large gap between \\(\\lambda_k\\) and \\(\\lambda_{k+1}\\) suggests choosing \\(k\\) clusters.\nStability analysis:\nRun spectral clustering for different values of \\(k\\) and assess the stability of the resulting partitions.\nDomain knowledge:\nStructural or semantic constraints may dictate a natural choice of \\(k\\).\n\nEigenvalue gaps play a role analogous to scree plots in PCA.",
    "crumbs": [
      "Data Science Applications",
      "Spectral Clustering"
    ]
  },
  {
    "objectID": "chapters/08-spectral-clustering.html#when-spectral-clustering-works-well",
    "href": "chapters/08-spectral-clustering.html#when-spectral-clustering-works-well",
    "title": "Spectral Clustering",
    "section": "13 When Spectral Clustering Works Well",
    "text": "13 When Spectral Clustering Works Well\n\n13.1 Advantages\n\ncaptures nonlinear cluster structure,\nflexible choice of similarity functions,\nstrong theoretical grounding via spectral graph theory.\n\n\n\n13.2 Limitations\n\nsensitive to kernel choice and hyperparameters,\neigen-decomposition can be computationally expensive for large \\(n\\),\nscalability often requires approximations (Nyström, sparse graphs).",
    "crumbs": [
      "Data Science Applications",
      "Spectral Clustering"
    ]
  },
  {
    "objectID": "chapters/08-spectral-clustering.html#connections-to-other-methods",
    "href": "chapters/08-spectral-clustering.html#connections-to-other-methods",
    "title": "Spectral Clustering",
    "section": "14 Connections to Other Methods",
    "text": "14 Connections to Other Methods\n\nKernel PCA: spectral embedding of kernel (Gram) matrices.\nDiffusion maps: Markov process interpretation of random walks on graphs.\nGraph neural networks: learnable spectral and message-passing operators.",
    "crumbs": [
      "Data Science Applications",
      "Spectral Clustering"
    ]
  },
  {
    "objectID": "chapters/08-spectral-clustering.html#summary",
    "href": "chapters/08-spectral-clustering.html#summary",
    "title": "Spectral Clustering",
    "section": "15 Summary",
    "text": "15 Summary\n\nSpectral clustering reduces clustering to an eigenvalue problem.\nGraph Laplacians encode connectivity and geometry.\nEigenvectors provide low-dimensional spectral embeddings.\nThe eigengap heuristic guides the choice of the number of clusters.\nSpectral clustering is a paradigmatic application of spectral theory in Data Science.",
    "crumbs": [
      "Data Science Applications",
      "Spectral Clustering"
    ]
  },
  {
    "objectID": "chapters/08-spectral-clustering.html#preview-final-remarks",
    "href": "chapters/08-spectral-clustering.html#preview-final-remarks",
    "title": "Spectral Clustering",
    "section": "16 Preview: Final Remarks",
    "text": "16 Preview: Final Remarks\nSpectral methods unify linear algebra, geometry, and data analysis. In the final chapter, we synthesize the main ideas and compare spectral techniques across applications.",
    "crumbs": [
      "Data Science Applications",
      "Spectral Clustering"
    ]
  },
  {
    "objectID": "chapters/07-regularization-truncated-svd.html",
    "href": "chapters/07-regularization-truncated-svd.html",
    "title": "Regularization and Truncated SVD",
    "section": "",
    "text": "Singular values quantify how strongly a matrix acts along different directions. When some singular values are very small, linear inverse problems become unstable: small perturbations in the data can lead to large changes in the solution.\nRegularization addresses this instability by damping or discarding directions associated with small singular values.\nTwo central regularization strategies are:\n\nTruncated SVD (TSVD),\nTikhonov regularization (ridge regression).\n\nBoth admit a clean spectral interpretation.",
    "crumbs": [
      "Data Science Applications",
      "Regularization and Truncated SVD"
    ]
  },
  {
    "objectID": "chapters/07-regularization-truncated-svd.html#motivation",
    "href": "chapters/07-regularization-truncated-svd.html#motivation",
    "title": "Regularization and Truncated SVD",
    "section": "",
    "text": "Singular values quantify how strongly a matrix acts along different directions. When some singular values are very small, linear inverse problems become unstable: small perturbations in the data can lead to large changes in the solution.\nRegularization addresses this instability by damping or discarding directions associated with small singular values.\nTwo central regularization strategies are:\n\nTruncated SVD (TSVD),\nTikhonov regularization (ridge regression).\n\nBoth admit a clean spectral interpretation.",
    "crumbs": [
      "Data Science Applications",
      "Regularization and Truncated SVD"
    ]
  },
  {
    "objectID": "chapters/07-regularization-truncated-svd.html#ill-posed-linear-problems",
    "href": "chapters/07-regularization-truncated-svd.html#ill-posed-linear-problems",
    "title": "Regularization and Truncated SVD",
    "section": "2 Ill-Posed Linear Problems",
    "text": "2 Ill-Posed Linear Problems\nConsider the linear model\n\\[\nAx \\approx b,\n\\]\nwith \\(A \\in \\mathbb{R}^{m \\times n}\\) and noisy data \\(b\\).\nIf \\(A\\) has small singular values, the least-squares solution\n\\[\nx^\\star = \\arg\\min_x \\|Ax - b\\|_2^2\n\\]\nis unstable.",
    "crumbs": [
      "Data Science Applications",
      "Regularization and Truncated SVD"
    ]
  },
  {
    "objectID": "chapters/07-regularization-truncated-svd.html#least-squares-in-svd-coordinates",
    "href": "chapters/07-regularization-truncated-svd.html#least-squares-in-svd-coordinates",
    "title": "Regularization and Truncated SVD",
    "section": "3 Least Squares in SVD Coordinates",
    "text": "3 Least Squares in SVD Coordinates\nLet\n\\[\nA = U \\Sigma V^\\top\n\\]\nbe the SVD of \\(A\\). The least-squares solution can be written as\n\\[\nx^\\star = \\sum_{i=1}^r \\frac{u_i^\\top b}{\\sigma_i} v_i.\n\\]\nThis expression makes the instability explicit:\n\ncoefficients are divided by \\(\\sigma_i\\),\nsmall \\(\\sigma_i\\) amplify noise.",
    "crumbs": [
      "Data Science Applications",
      "Regularization and Truncated SVD"
    ]
  },
  {
    "objectID": "chapters/07-regularization-truncated-svd.html#truncated-svd-tsvd",
    "href": "chapters/07-regularization-truncated-svd.html#truncated-svd-tsvd",
    "title": "Regularization and Truncated SVD",
    "section": "4 Truncated SVD (TSVD)",
    "text": "4 Truncated SVD (TSVD)\n\n4.1 Definition\n\n\nDefinition (Truncated SVD)\n\nLet \\(k &lt; r = \\mathrm{rank}(A)\\). The truncated SVD solution is defined as\n\\[\nx_k := \\sum_{i=1}^k \\frac{u_i^\\top b}{\\sigma_i} v_i.\n\\]\n\nThis corresponds to discarding all singular components associated with \\(\\sigma_{k+1},\\dots,\\sigma_r\\).\n\n\n\n4.2 Geometric Interpretation\n\nKeep only the \\(k\\) most informative directions.\nProject data onto a \\(k\\)-dimensional dominant subspace.\nDiscard directions dominated by noise.\n\nTSVD is a hard spectral filter.",
    "crumbs": [
      "Data Science Applications",
      "Regularization and Truncated SVD"
    ]
  },
  {
    "objectID": "chapters/07-regularization-truncated-svd.html#optimality-perspective",
    "href": "chapters/07-regularization-truncated-svd.html#optimality-perspective",
    "title": "Regularization and Truncated SVD",
    "section": "5 Optimality Perspective",
    "text": "5 Optimality Perspective\n\n\nTheorem\n\nThe truncated SVD solution minimizes\n\\[\n\\|Ax - b\\|_2\n\\]\nover all solutions \\(x\\) constrained to lie in the span of the first \\(k\\) right singular vectors of \\(A\\).\n\nThus, TSVD solves a constrained optimization problem.",
    "crumbs": [
      "Data Science Applications",
      "Regularization and Truncated SVD"
    ]
  },
  {
    "objectID": "chapters/07-regularization-truncated-svd.html#tikhonov-regularization-ridge",
    "href": "chapters/07-regularization-truncated-svd.html#tikhonov-regularization-ridge",
    "title": "Regularization and Truncated SVD",
    "section": "6 Tikhonov Regularization (Ridge)",
    "text": "6 Tikhonov Regularization (Ridge)\n\n6.1 Definition\nInstead of discarding components, ridge regression penalizes large solutions:\n\\[\nx_\\lambda := \\arg\\min_x \\bigl( \\|Ax - b\\|_2^2 + \\lambda \\|x\\|_2^2 \\bigr),\n\\]\nwith \\(\\lambda &gt; 0\\).\n\n\n\n6.2 Spectral Form\nIn SVD coordinates, the ridge solution is\n\\[\nx_\\lambda = \\sum_{i=1}^r\n\\frac{\\sigma_i}{\\sigma_i^2 + \\lambda}\n(u_i^\\top b)\\, v_i.\n\\]\nEach component is multiplied by a filter factor\n\\[\nf_\\lambda(\\sigma_i) = \\frac{\\sigma_i}{\\sigma_i^2 + \\lambda}.\n\\]",
    "crumbs": [
      "Data Science Applications",
      "Regularization and Truncated SVD"
    ]
  },
  {
    "objectID": "chapters/07-regularization-truncated-svd.html#hard-vs.-soft-spectral-filtering",
    "href": "chapters/07-regularization-truncated-svd.html#hard-vs.-soft-spectral-filtering",
    "title": "Regularization and Truncated SVD",
    "section": "7 Hard vs. Soft Spectral Filtering",
    "text": "7 Hard vs. Soft Spectral Filtering\n\n\n\nMethod\nFilter behavior\n\n\n\n\nTruncated SVD\n\\(f(\\sigma)=1/\\sigma\\) for \\(i\\le k\\), \\(0\\) otherwise\n\n\nRidge\nsmooth damping for all \\(\\sigma_i\\)\n\n\nNo regularization\n\\(1/\\sigma\\) (unstable)\n\n\n\n\nTSVD: hard cutoff\nRidge: soft shrinkage",
    "crumbs": [
      "Data Science Applications",
      "Regularization and Truncated SVD"
    ]
  },
  {
    "objectID": "chapters/07-regularization-truncated-svd.html#biasvariance-trade-off",
    "href": "chapters/07-regularization-truncated-svd.html#biasvariance-trade-off",
    "title": "Regularization and Truncated SVD",
    "section": "8 Bias–Variance Trade-off",
    "text": "8 Bias–Variance Trade-off\nRegularization introduces bias but reduces variance.\n\nSmall \\(k\\) (or large \\(\\lambda\\)):\n\nhigh bias,\nlow variance.\n\nLarge \\(k\\) (or small \\(\\lambda\\)):\n\nlow bias,\nhigh variance.\n\n\nThis trade-off is fundamental in statistical learning.",
    "crumbs": [
      "Data Science Applications",
      "Regularization and Truncated SVD"
    ]
  },
  {
    "objectID": "chapters/07-regularization-truncated-svd.html#computational-example-python",
    "href": "chapters/07-regularization-truncated-svd.html#computational-example-python",
    "title": "Regularization and Truncated SVD",
    "section": "9 Computational Example (Python)",
    "text": "9 Computational Example (Python)\n\n\nCode\n```{python}\nimport numpy as np\n\nrng = np.random.default_rng(0)\nA = rng.normal(size=(50, 30))\nx_true = rng.normal(size=30)\nb = A @ x_true + 0.1 * rng.normal(size=50)\n\nU, s, Vt = np.linalg.svd(A, full_matrices=False)\n\n# TSVD\nk = 10\nx_tsvd = sum((U[:, i] @ b) / s[i] * Vt[i] for i in range(k))\n\n# Ridge\nlam = 0.5\nx_ridge = sum((s[i] / (s[i]**2 + lam)) * (U[:, i] @ b) * Vt[i]\n              for i in range(len(s)))\n\nnp.linalg.norm(x_true - x_tsvd), np.linalg.norm(x_true - x_ridge)\n```\n\n\n(np.float64(3.3811699474285137), np.float64(0.25795525800378033))",
    "crumbs": [
      "Data Science Applications",
      "Regularization and Truncated SVD"
    ]
  },
  {
    "objectID": "chapters/07-regularization-truncated-svd.html#choosing-the-regularization-parameter",
    "href": "chapters/07-regularization-truncated-svd.html#choosing-the-regularization-parameter",
    "title": "Regularization and Truncated SVD",
    "section": "10 Choosing the Regularization Parameter",
    "text": "10 Choosing the Regularization Parameter\n\n10.1 For Truncated SVD\n\nscree plot of singular values,\ncumulative explained energy,\ncross-validation.\n\n\n\n10.2 For Ridge\n\ncross-validation,\nL-curve,\ndiscrepancy principle.",
    "crumbs": [
      "Data Science Applications",
      "Regularization and Truncated SVD"
    ]
  },
  {
    "objectID": "chapters/07-regularization-truncated-svd.html#relation-to-pca-and-dimensionality-reduction",
    "href": "chapters/07-regularization-truncated-svd.html#relation-to-pca-and-dimensionality-reduction",
    "title": "Regularization and Truncated SVD",
    "section": "11 Relation to PCA and Dimensionality Reduction",
    "text": "11 Relation to PCA and Dimensionality Reduction\n\nTruncated SVD corresponds to PCA truncation.\nRidge keeps all components but shrinks them.\nPCA + regression ≈ truncated spectral solution.",
    "crumbs": [
      "Data Science Applications",
      "Regularization and Truncated SVD"
    ]
  },
  {
    "objectID": "chapters/07-regularization-truncated-svd.html#when-to-use-which-method",
    "href": "chapters/07-regularization-truncated-svd.html#when-to-use-which-method",
    "title": "Regularization and Truncated SVD",
    "section": "12 When to Use Which Method",
    "text": "12 When to Use Which Method\n\n12.1 Use Truncated SVD when:\n\na clear spectral gap exists,\ndimensionality reduction is desired,\ninterpretability matters.\n\n\n\n12.2 Use Ridge when:\n\nthe spectrum decays smoothly,\nstability is critical,\nprediction accuracy is the goal.",
    "crumbs": [
      "Data Science Applications",
      "Regularization and Truncated SVD"
    ]
  },
  {
    "objectID": "chapters/07-regularization-truncated-svd.html#summary",
    "href": "chapters/07-regularization-truncated-svd.html#summary",
    "title": "Regularization and Truncated SVD",
    "section": "13 Summary",
    "text": "13 Summary\n\nSmall singular values cause instability.\nTruncated SVD removes unstable directions.\nRidge regularization damps all directions smoothly.\nBoth methods admit clean spectral interpretations.\nRegularization is fundamentally a spectral filtering problem.",
    "crumbs": [
      "Data Science Applications",
      "Regularization and Truncated SVD"
    ]
  },
  {
    "objectID": "chapters/07-regularization-truncated-svd.html#preview-spectral-clustering",
    "href": "chapters/07-regularization-truncated-svd.html#preview-spectral-clustering",
    "title": "Regularization and Truncated SVD",
    "section": "14 Preview: Spectral Clustering",
    "text": "14 Preview: Spectral Clustering\nEigenvectors of graph Laplacians can be used to uncover cluster structure in data.\nNext, we study spectral clustering as another application of spectral theory.\n\n14.1 Scree Plot (Singular Values)\n\n\nCode\n```{python}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Assume: s is the 1D array of singular values from SVD, sorted decreasingly.\n# Example: U, s, Vt = np.linalg.svd(A, full_matrices=False)\n\nfig, ax = plt.subplots()\nax.plot(np.arange(1, len(s) + 1), s, marker=\"o\")\nax.set_xlabel(\"Index i\")\nax.set_ylabel(\"Singular value σᵢ\")\nax.set_title(\"Scree plot of singular values\")\nax.grid(True)\nplt.show()\n```\n\n\n\n\n\n\n\n\n\n\n\n\n14.2 Cumulative Explained Energy\n\n\nCode\n```{python}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nenergy = s**2\ncum_energy = np.cumsum(energy) / np.sum(energy)\n\nfig, ax = plt.subplots()\nax.plot(np.arange(1, len(s) + 1), cum_energy, marker=\"o\")\nax.set_xlabel(\"k\")\nax.set_ylabel(\"Cumulative explained energy\")\nax.set_title(\"Cumulative explained energy vs k\")\nax.set_ylim(0, 1.02)\nax.grid(True)\n\n# Optional: a common threshold (e.g., 90% or 95%)\nthreshold = 0.95\nax.axhline(threshold, linestyle=\"--\")\nplt.show()\n\n# Suggested k for the chosen threshold\nk_star = int(np.searchsorted(cum_energy, threshold) + 1)\nk_star\n```\n\n\n\n\n\n\n\n\n\n22\n\n\n\n\n\n14.3 TSVD Residual Curve: ‖Ax_k − b‖ vs k\n\n\nCode\n```{python}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# We compute x_k and residual norms efficiently using SVD coordinates.\n# x_k = sum_{i=1}^k (u_i^T b / s_i) v_i\n# residual r_k = b - A x_k\n\nUt_b = U.T @ b  # coefficients u_i^T b\nmax_k = len(s)\n\nresidual_norms = []\nfor k in range(1, max_k + 1):\n    # Build x_k via right singular vectors:\n    x_k = (Vt[:k, :].T) @ (Ut_b[:k] / s[:k])\n    r_k = b - A @ x_k\n    residual_norms.append(np.linalg.norm(r_k))\n\nfig, ax = plt.subplots()\nax.plot(np.arange(1, max_k + 1), residual_norms, marker=\"o\")\nax.set_xlabel(\"k\")\nax.set_ylabel(\"Residual norm ‖A x_k − b‖₂\")\nax.set_title(\"TSVD residual vs k\")\nax.grid(True)\nplt.show()\n```\n\n\n\n\n\n\n\n\n\n\n\n14.4 Truncation Error: ‖A − A_k‖_F vs k\n\n\nCode\n```{python}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# For full matrices, the Frobenius truncation error is determined by singular values:\n# ||A - A_k||_F^2 = sum_{i&gt;k} s_i^2\n\ntail_energy = np.cumsum((s[::-1]**2))[::-1]  # tail sums of squares\nfro_errors = np.sqrt(np.r_[tail_energy[1:], 0.0])  # error for k=1..r, last is 0\n\nfig, ax = plt.subplots()\nax.plot(np.arange(1, len(s) + 1), fro_errors, marker=\"o\")\nax.set_xlabel(\"k\")\nax.set_ylabel(\"‖A − A_k‖_F\")\nax.set_title(\"Frobenius truncation error vs k\")\nax.grid(True)\nplt.show()\n```\n\n\n\n\n\n\n\n\n\n\n\n\n14.5 L-curve for Ridge Regularization\n\n\nCode\n```{python}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Ridge solution in SVD coordinates:\n# x_λ = sum_i (s_i / (s_i^2 + λ)) (u_i^T b) v_i\n\nUt_b = U.T @ b\nr = len(s)\n\n# Choose a grid of lambdas (log-spaced)\nlambdas = np.logspace(-6, 2, 60)\n\nx_norms = []\nres_norms = []\n\nfor lam in lambdas:\n    coeffs = (s / (s**2 + lam)) * Ut_b[:r]\n    x_lam = Vt.T @ coeffs\n    res = A @ x_lam - b\n    x_norms.append(np.linalg.norm(x_lam))\n    res_norms.append(np.linalg.norm(res))\n\nfig, ax = plt.subplots()\nax.plot(res_norms, x_norms, marker=\"o\")\nax.set_xlabel(\"‖A x_λ − b‖₂ (residual norm)\")\nax.set_ylabel(\"‖x_λ‖₂ (solution norm)\")\nax.set_title(\"L-curve for ridge regularization\")\nax.grid(True)\nplt.show()\n```",
    "crumbs": [
      "Data Science Applications",
      "Regularization and Truncated SVD"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site",
    "crumbs": [
      "About"
    ]
  }
]