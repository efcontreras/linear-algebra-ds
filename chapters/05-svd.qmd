---
title: "Singular Value Decomposition (SVD)"
---

## Motivation

The **Singular Value Decomposition (SVD)** is the most important matrix factorization
in modern applied linear algebra.

Unlike eigenvalue decompositions, SVD:

- applies to **any matrix** (square or rectangular),
- is numerically stable,
- reveals intrinsic dimensionality,
- provides optimal low-rank approximations.

In Data Science, SVD underlies:
- PCA,
- dimensionality reduction,
- data compression,
- recommender systems,
- latent semantic analysis.

---

## Statement of the SVD

::: {.mathbox .theorem}
<div class="mathbox-title">Theorem (Singular Value Decomposition)</div>

Let $A \in \mathbb{R}^{m \times n}$.
There exist orthogonal matrices

$$
U \in \mathbb{R}^{m \times m}, \qquad
V \in \mathbb{R}^{n \times n},
$$

and a diagonal matrix

$$
\Sigma \in \mathbb{R}^{m \times n},
$$

with nonnegative diagonal entries

$$
\sigma_1 \ge \sigma_2 \ge \cdots \ge \sigma_r > 0,
$$

such that

$$
A = U \Sigma V^\top.
$$

The numbers $\sigma_i$ are the **singular values** of $A$.
:::

---

## Structure of the Decomposition

- Columns of $V$: **right singular vectors**
- Columns of $U$: **left singular vectors**
- $\Sigma$: scaling along orthogonal directions

Only the first $r = \mathrm{rank}(A)$ singular values are nonzero.

---

## Geometric Interpretation

The SVD describes the action of $A$ as:

1. rotation/reflection by $V^\top$,
2. scaling by $\Sigma$,
3. rotation/reflection by $U$.

Thus, $A$ maps the unit sphere in $\mathbb{R}^n$
to an ellipsoid in $\mathbb{R}^m$.

---

## Rank-One Decomposition

The SVD can be written as a sum of rank-one operators:

$$
A = \sum_{i=1}^r \sigma_i \, u_i v_i^\top.
$$

Each term:
- projects onto $v_i$,
- scales by $\sigma_i$,
- maps to direction $u_i$.

---

## Proof Sketch of Existence

<details class="proof-toggle">
<summary>Proof sketch (click to expand)</summary>

::: {.mathbox .proof}
<div class="mathbox-title">Proof (sketch)</div>

1. Consider the symmetric matrix $A^\top A$.
2. By the Spectral Theorem, there exists an orthonormal basis of eigenvectors
   $\{v_i\}$ with eigenvalues $\lambda_i \ge 0$.
3. Define singular values $\sigma_i = \sqrt{\lambda_i}$.
4. For $\sigma_i > 0$, define $u_i = \frac{1}{\sigma_i} A v_i$.
5. Show that $\{u_i\}$ is orthonormal and that

   $$
   A v_i = \sigma_i u_i.
   $$

6. Extend $\{u_i\}$ and $\{v_i\}$ to orthonormal bases of $\mathbb{R}^m$ and $\mathbb{R}^n$.

This yields $A = U \Sigma V^\top$.
<span class="proof-end">□</span>
:::

</details>

---

## Best Rank-$k$ Approximation

One of the most important properties of SVD is optimal low-rank approximation [@eckart1936approximation; @mirsky1960symmetric].

::: {.mathbox .theorem}
<div class="mathbox-title">Theorem (Eckart–Young–Mirsky)</div>

Let $A = \sum_{i=1}^r \sigma_i u_i v_i^\top$ be the SVD of $A$.
For $k < r$, define

$$
A_k := \sum_{i=1}^k \sigma_i u_i v_i^\top.
$$

Then $A_k$ is the best rank-$k$ approximation of $A$ in both:

- Frobenius norm,
- operator (spectral) norm.

That is,

$$
\|A - A_k\| = \min_{\mathrm{rank}(B)\le k} \|A - B\|.
$$
:::

---

<details class="proof-toggle">
<summary>Proof of the Eckart–Young–Mirsky Theorem (rigorous, click to expand)</summary>

::: {.mathbox .proof}
<div class="mathbox-title">Proof (Eckart–Young–Mirsky)</div>

Let $A\in\mathbb{R}^{m\times n}$ have SVD
$$
A=U\Sigma V^\top,
\qquad
\Sigma=\mathrm{diag}(\sigma_1,\dots,\sigma_r)\in\mathbb{R}^{m\times n},
\qquad
\sigma_1\ge \cdots \ge \sigma_r>0,
$$
where $r=\mathrm{rank}(A)$ and we set $\sigma_i=0$ for $i>r$.

For $k<r$, define the truncated SVD
$$
A_k := U\Sigma_k V^\top,
\qquad
\Sigma_k=\mathrm{diag}(\sigma_1,\dots,\sigma_k,0,\dots,0).
$$

We prove that for both the Frobenius norm $\|\cdot\|_F$ and the operator (spectral) norm $\|\cdot\|_2$,
$$
\|A-A_k\| = \min_{\mathrm{rank}(B)\le k}\|A-B\|.
$$

---

## Step 1: Reduce to the diagonal case

Both $\|\cdot\|_F$ and $\|\cdot\|_2$ are **orthogonally invariant**:
for orthogonal $Q_1,Q_2$,
$$
\|Q_1MQ_2\|_F=\|M\|_F,
\qquad
\|Q_1MQ_2\|_2=\|M\|_2.
$$

Fix any matrix $B$ with $\mathrm{rank}(B)\le k$ and set
$$
C := U^\top B V.
$$
Then $\mathrm{rank}(C)=\mathrm{rank}(B)\le k$ and
$$
\|A-B\|
=\|U\Sigma V^\top - B\|
=\|\Sigma - U^\top B V\|
=\|\Sigma - C\|.
$$
Therefore,
$$
\min_{\mathrm{rank}(B)\le k}\|A-B\|
=
\min_{\mathrm{rank}(C)\le k}\|\Sigma-C\|.
$$

So it suffices to prove the theorem for $\Sigma$ and its truncation $\Sigma_k$.

---

## Part A: Frobenius norm optimality

Let $C$ be any matrix with $\mathrm{rank}(C)\le k$ and denote its singular values by
$$
\tau_1\ge \tau_2\ge \cdots \ge \tau_k\ge 0,
\qquad
\tau_i=0 \text{ for } i>k.
$$

We use the identity
$$
\|\Sigma-C\|_F^2
= \|\Sigma\|_F^2 + \|C\|_F^2 - 2\,\mathrm{tr}(\Sigma^\top C).
$$

### Key inequality (von Neumann trace inequality)
For any matrices $X,Y$ of the same size, the trace satisfies
$$
\mathrm{tr}(X^\top Y)\le \sum_{i\ge 1} s_i(X)\,s_i(Y),
$$
where $s_i(\cdot)$ denotes singular values in nonincreasing order.
Applying this to $X=\Sigma$ and $Y=C$ gives
$$
\mathrm{tr}(\Sigma^\top C)
\le
\sum_{i\ge 1} \sigma_i\,\tau_i
=
\sum_{i=1}^k \sigma_i\,\tau_i,
$$
since $\tau_i=0$ for $i>k$.

Hence,
\begin{align*}
\|\Sigma-C\|_F^2
&\ge
\sum_{i\ge 1}\sigma_i^2
+
\sum_{i=1}^k \tau_i^2
-
2\sum_{i=1}^k \sigma_i\tau_i \\
&=
\sum_{i=1}^k(\sigma_i-\tau_i)^2
+
\sum_{i>k}\sigma_i^2 \\
&\ge
\sum_{i>k}\sigma_i^2.
\end{align*}

Therefore,
$$
\|\Sigma-C\|_F \;\ge\; \left(\sum_{i>k}\sigma_i^2\right)^{1/2}.
$$

### Achievability
Choose $C=\Sigma_k$. Then
$$
\|\Sigma-\Sigma_k\|_F^2 = \sum_{i>k}\sigma_i^2,
$$
so equality is achieved and $\Sigma_k$ is a best rank-$k$ approximation in Frobenius norm.  
Returning to $A=U\Sigma V^\top$, we obtain that $A_k=U\Sigma_kV^\top$ is optimal and
$$
\min_{\mathrm{rank}(B)\le k}\|A-B\|_F = \|A-A_k\|_F = \left(\sum_{i>k}\sigma_i^2\right)^{1/2}.
$$

---

## Part B: Spectral norm optimality

We now prove
$$
\min_{\mathrm{rank}(C)\le k}\|\Sigma-C\|_2 = \sigma_{k+1},
$$
(where $\sigma_{k+1}>0$ since $k<r$).

Let $C$ have rank $\le k$. Then its $(k+1)$-th singular value is zero:
$$
\tau_{k+1}(C)=0.
$$

### Key inequality (Weyl-type singular value inequality)
For any matrices $X,Y$ of the same size and any indices $i,j$ with $i+j-1$ valid,
$$
s_{i+j-1}(X+Y)\le s_i(X)+s_j(Y).
$$

Apply this with $X=\Sigma-C$ and $Y=C$, so that $X+Y=\Sigma$.
Choose $i=1$ and $j=k+1$. Then
$$
s_{k+1}(\Sigma)
\le
s_1(\Sigma-C)+s_{k+1}(C)
=
\|\Sigma-C\|_2 + 0.
$$
Hence,
$$
\|\Sigma-C\|_2 \ge s_{k+1}(\Sigma)=\sigma_{k+1}.
$$

### Achievability
Take $C=\Sigma_k$. Then $\Sigma-\Sigma_k$ is diagonal with largest diagonal entry $\sigma_{k+1}$, so
$$
\|\Sigma-\Sigma_k\|_2 = \sigma_{k+1}.
$$

Therefore $\Sigma_k$ is a best rank-$k$ approximation in spectral norm, and transferring back to $A$ gives
$$
\min_{\mathrm{rank}(B)\le k}\|A-B\|_2 = \|A-A_k\|_2 = \sigma_{k+1}.
$$

---

## Conclusion

For both norms,
$$
A_k = U\Sigma_kV^\top \in \arg\min_{\mathrm{rank}(B)\le k}\|A-B\|.
$$
Moreover,
$$
\|A-A_k\|_F = \left(\sum_{i>k}\sigma_i^2\right)^{1/2},
\qquad
\|A-A_k\|_2 = \sigma_{k+1}.
$$

<span class="proof-end">□</span>
:::

</details>

---

## Computational Example (Python)

```python
import numpy as np

rng = np.random.default_rng(0)
A = rng.normal(size=(6, 4))

U, s, Vt = np.linalg.svd(A, full_matrices=False)

U.shape, s, Vt.shape
```

Reconstruction:

```python
A_reconstructed = U @ np.diag(s) @ Vt
np.allclose(A, A_reconstructed)
```
---

## Low-Rank Approximation in Practice

```python
k = 2
A_k = U[:, :k] @ np.diag(s[:k]) @ Vt[:k, :]

np.linalg.norm(A - A_k, ord="fro")
```

## Connection to PCA

Given a centered data matrix $X \in \mathbb{R}^{m \times n}$:

- SVD: $X = U \Sigma V^\top$
- Covariance: $X^\top X = V \Sigma^2 V^\top$

Thus:
- right singular vectors = principal directions,
- squared singular values = explained variance (up to normalization).

PCA can be computed **directly via SVD**, without forming the covariance matrix.

---

## Conditioning and Stability

The **condition number** of $A$ is:

$$
\kappa(A) = \frac{\sigma_{\max}}{\sigma_{\min}}.
$$

- Large $\kappa(A)$ implies ill-conditioned problems.
- Small singular values amplify noise and numerical errors.

This explains why SVD is crucial for:
- diagnosing instability,
- regularization (e.g. truncated SVD).

---

## Data Science Applications

### Dimensionality Reduction
Keep only the first $k$ singular values and vectors to obtain a low-dimensional representation.

### Compression
Store $U_k$, $\Sigma_k$, and $V_k$ instead of the full matrix $A$.

### Recommender Systems
User–item interaction matrices are approximated by low-rank SVD to uncover latent factors.

### Latent Semantic Analysis
Text–term matrices are decomposed to identify latent semantic structure in documents.

---

## What SVD Does *Not* Assume

- no symmetry,
- no invertibility,
- no square shape.

This universality makes SVD the **central tool** of applied linear algebra.

---

## Summary

- SVD generalizes the Spectral Theorem.
- Any matrix admits an orthogonal decomposition.
- Singular values quantify intrinsic dimensionality.
- Truncated SVD yields optimal low-rank approximations.
- PCA and many Data Science methods are special cases of SVD.

---

## Preview: Regularization and Truncated SVD

Small singular values cause instability.
In the next chapter, we study:

- truncated SVD,
- Tikhonov (ridge) regularization,
- bias–variance trade-offs.
