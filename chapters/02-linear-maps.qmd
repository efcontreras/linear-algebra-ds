---
title: "Linear Maps and Matrices"
---

## Motivation

Linear maps formalize the idea of **transforming vectors in a linear way**.
They are the mathematical objects behind:

- feature transformations,
- linear models,
- dimensionality reduction,
- neural network layers (locally).

In practice, linear maps are represented by **matrices**.

---

## Linear Maps

::: {.mathbox .definition}
<div class="mathbox-title">Definition (Linear map)</div>

Let $V$ and $W$ be vector spaces over $\mathbb{R}$.
A function $T : V \to W$ is a **linear map** if for all $u,v \in V$ and
$\alpha \in \mathbb{R}$:

1. $T(u+v) = T(u) + T(v)$  
2. $T(\alpha u) = \alpha T(u)$
:::

Equivalently:
$$
T(\alpha u + \beta v) = \alpha T(u) + \beta T(v).
$$

---

## Examples of Linear Maps

::: {.mathbox .example}
<div class="mathbox-title">Example (Matrix multiplication)</div>

Let $A \in \mathbb{R}^{m \times n}$.  
The map $T(x) = Ax$ from $\mathbb{R}^n$ to $\mathbb{R}^m$ is linear.
:::

::: {.mathbox .example}
<div class="mathbox-title">Example (Projection)</div>

The map projecting vectors in $\mathbb{R}^2$ onto the $x$-axis is linear.
:::

::: {.mathbox .example}
<div class="mathbox-title">Example (Differentiation)</div>

The map $T(f) = f'$ on the space of polynomials is linear.
:::

---

## Kernel and Image

::: {.mathbox .definition}
<div class="mathbox-title">Definition (Kernel)</div>

The **kernel** of a linear map $T : V \to W$ is:
$$
\ker(T) := \{v \in V : T(v) = 0\}.
$$
:::

::: {.mathbox .definition}
<div class="mathbox-title">Definition (Image)</div>

The **image** of $T$ is:
$$
\mathrm{Im}(T) := \{T(v) : v \in V\}.
$$
:::

---

## Rank–Nullity Theorem

::: {.mathbox .theorem}
<div class="mathbox-title">Theorem (Rank–Nullity)</div>

Let $T:V\to W$ be a linear map with $V$ finite-dimensional. Then:

$$
\dim(V) = \dim(\ker T) + \dim(\mathrm{Im}\, T).
$$
:::

<details class="proof-toggle-boxed">
<summary><strong>Proof (click to expand)</strong></summary>

::: {.mathbox .proof}
<div class="mathbox-title">Proof</div>

Let $\{k_1,\dots,k_r\}$ be a basis of $\ker(T)$, so $r=\dim(\ker T)$.

Extend this basis to a basis of $V$:
$$
\{k_1,\dots,k_r, v_{r+1},\dots,v_n\},
$$
where $n=\dim(V)$ (this extension is always possible in finite-dimensional spaces).

We claim that $\{T(v_{r+1}),\dots,T(v_n)\}$ is a basis of $\mathrm{Im}(T)$.

**(1) Spanning.**  
Take any $y\in\mathrm{Im}(T)$. Then $y=T(v)$ for some $v\in V$. Write $v$ in the chosen basis:
$$
v=\sum_{i=1}^r a_i k_i + \sum_{j=r+1}^n b_j v_j.
$$
By linearity and the fact that $T(k_i)=0$ for all $i$,
$$
T(v)=\sum_{i=1}^r a_i T(k_i) + \sum_{j=r+1}^n b_j T(v_j)
= \sum_{j=r+1}^n b_j T(v_j).
$$
Hence $y$ is in the span of $\{T(v_{r+1}),\dots,T(v_n)\}$.

**(2) Linear independence.**  
Assume
$$
\sum_{j=r+1}^n c_j\,T(v_j)=0.
$$
Then
$$
T\Big(\sum_{j=r+1}^n c_j v_j\Big)=0,
$$
so $\sum_{j=r+1}^n c_j v_j \in \ker(T)$. Therefore there exist scalars $\alpha_1,\dots,\alpha_r$ such that
$$
\sum_{j=r+1}^n c_j v_j = \sum_{i=1}^r \alpha_i k_i.
$$
Rearranging gives
$$
\sum_{i=1}^r (-\alpha_i)k_i + \sum_{j=r+1}^n c_j v_j = 0.
$$
But $\{k_1,\dots,k_r, v_{r+1},\dots,v_n\}$ is a basis of $V$, hence linearly independent, so all coefficients must be zero. In particular, $c_j=0$ for every $j=r+1,\dots,n$. Thus $\{T(v_{r+1}),\dots,T(v_n)\}$ is linearly independent.

So $\{T(v_{r+1}),\dots,T(v_n)\}$ is a basis of $\mathrm{Im}(T)$ and has $n-r$ elements, meaning
$$
\dim(\mathrm{Im}(T)) = n-r.
$$
Therefore
$$
\dim(V)=n=r+(n-r)=\dim(\ker T)+\dim(\mathrm{Im}(T)).
$$

<span class="proof-end">□</span>
:::

</details>


This theorem explains the trade-off between:
- **degrees of freedom lost** (kernel),
- **information preserved** (image).

---

## Matrices as Representations of Linear Maps

Let $T : \mathbb{R}^n \to \mathbb{R}^m$ be linear.
There exists a unique matrix $A \in \mathbb{R}^{m \times n}$ such that:
$$
T(x) = Ax.
$$

The columns of $A$ are the images of the canonical basis vectors.

---

## Change of Basis

::: {.mathbox .definition}
<div class="mathbox-title">Definition (Change of basis)</div>

Let $B$ and $C$ be bases of $V$ and $W$, respectively.
The matrix of $T$ depends on the chosen bases.

Different bases yield different matrices, but the same linear map.
:::

This explains why **feature scaling and basis choice matter** in practice.

---

## Geometry of Linear Maps

Linear maps can:
- rotate,
- reflect,
- scale,
- shear,
- project.

All linear models are combinations of these effects.

---

## Computational Example (Python)

```python
import numpy as np

A = np.array([[1, 2],
              [0, 1]])

x = np.array([1, 1])

A @ x
```

The matrix $A$ maps the vector $x$ to a new vector in $\mathbb{R}^2$.

---

## Rank and Data

```python
np.linalg.matrix_rank(A)
```

- The rank measures the dimension of the image.
- Low rank implies redundancy or collinearity.

---

## Linear Models in Data Science

A linear regression model is:

$$
y = Xw
$$

where:
- rows of $X$ are data points,
- columns of $X$ are features,
- $w$ is a parameter vector.

Prediction is simply applying a linear map.

---

## Preview: Least Squares

When $X$ is not invertible, we cannot solve $Xw = y$ exactly.
Instead, we solve:

$$
\min_{w \in \mathbb{R}^n} \|Xw - y\|_2^2
$$

This is a geometric projection problem, studied in the next chapter.

---

## Summary

- Linear maps preserve vector space structure.
- Matrices represent linear maps.
- Kernel and image explain information loss.
- Rank measures effective dimensionality.
- Linear models are applications of linear maps.
