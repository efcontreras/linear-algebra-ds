---
title: "Vectors and Vector Spaces"
---

## Motivation

Linear algebra starts with the notion of a **vector space**: a structure where
objects can be added and scaled.  
In Data Science, vectors appear everywhere:

- a data point is a vector of features,
- an embedding is a vector in $\mathbb{R}^d$,
- a model parameter vector lives in a vector space,
- rows or columns of a data matrix are vectors.

Understanding vector spaces precisely is therefore essential.

---

## Vectors

Intuitively, a vector represents a **direction and magnitude**.  
Algebraically, a vector is simply an element of a vector space.

### Examples
- A vector in $\mathbb{R}^n$:  
  $$
  x = (x_1, \dots, x_n)
  $$
- A polynomial:  
  $$
  p(t) = a_0 + a_1 t + \cdots + a_n t^n
  $$
- A function $f : [0,1] \to \mathbb{R}$

All of these will be vectors once we define the appropriate operations.

---

## Vector Spaces

::: {.mathbox .definition}
<div class="mathbox-title">Definition (Vector space)</div>

Let $\mathbb{K}$ be a field (typically $\mathbb{R}$ or $\mathbb{C}$).
A **vector space** over $\mathbb{K}$ is a set $V$ equipped with:

- vector addition: $V \times V \to V$  
- scalar multiplication: $\mathbb{K} \times V \to V$

satisfying the following axioms for all $u,v,w \in V$ and $\alpha,\beta \in \mathbb{K}$:

1. $u + v = v + u$ (commutativity)  
2. $(u + v) + w = u + (v + w)$ (associativity)  
3. There exists $0 \in V$ such that $v + 0 = v$  
4. For every $v \in V$ there exists $-v$ with $v + (-v) = 0$  
5. $\alpha (v + w) = \alpha v + \alpha w$  
6. $(\alpha + \beta)v = \alpha v + \beta v$  
7. $(\alpha \beta)v = \alpha(\beta v)$  
8. $1 v = v$
:::

---

## Canonical Examples

### Euclidean space

::: {.mathbox .example}
<div class="mathbox-title">Example ($\mathbb{R}^n$)</div>

$\mathbb{R}^n$ with componentwise addition and scalar multiplication is a vector space.
:::

### Function spaces

::: {.mathbox .example}
<div class="mathbox-title">Example (Function space)</div>

Let $V$ be the set of all real-valued functions on $[0,1]$.
Define:
$$
(f+g)(x) := f(x)+g(x), \quad (\alpha f)(x) := \alpha f(x).
$$
Then $V$ is a vector space.
:::

### Polynomial spaces

::: {.mathbox .example}
<div class="mathbox-title">Example (Polynomials)</div>

The set $\mathbb{P}_n$ of polynomials of degree $\le n$ is a vector space.
:::

---

## Subspaces

::: {.mathbox .definition}
<div class="mathbox-title">Definition (Subspace)</div>

A subset $W \subset V$ is a **subspace** if:
1. $0 \in W$  
2. $u,v \in W \Rightarrow u+v \in W$  
3. $\alpha \in \mathbb{K},\; v \in W \Rightarrow \alpha v \in W$
:::

### Subspace test

::: {.mathbox .proposition}
<div class="mathbox-title">Proposition (Subspace test)</div>

A nonempty subset $W \subset V$ is a subspace if and only if
it is closed under addition and scalar multiplication.
:::

---

## Linear combinations and span

::: {.mathbox .definition}
<div class="mathbox-title">Definition (Linear combination)</div>

Given vectors $v_1,\dots,v_k \in V$, a **linear combination** is any vector of the form:
$$
\alpha_1 v_1 + \cdots + \alpha_k v_k,
\quad \alpha_i \in \mathbb{K}.
$$
:::

::: {.mathbox .definition}
<div class="mathbox-title">Definition (Span)</div>

The **span** of $\{v_1,\dots,v_k\}$ is the set of all their linear combinations:
$$
\mathrm{span}(v_1,\dots,v_k).
$$
:::

::: {.mathbox .proposition}
<div class="mathbox-title">Proposition</div>

$\mathrm{span}(v_1,\dots,v_k)$ is a subspace of $V$.
:::

---

## Linear independence

::: {.mathbox .definition}
<div class="mathbox-title">Definition (Linear independence)</div>

Vectors $v_1,\dots,v_k$ are **linearly independent** if
$$
\alpha_1 v_1 + \cdots + \alpha_k v_k = 0
\quad \Rightarrow \quad
\alpha_1=\cdots=\alpha_k=0.
$$
:::

Intuition: no vector can be written as a combination of the others.

---

## Basis and dimension

::: {.mathbox .definition}
<div class="mathbox-title">Definition (Basis)</div>

A **basis** of a vector space $V$ is a linearly independent set that spans $V$.
:::

::: {.mathbox .definition}
<div class="mathbox-title">Definition (Dimension)</div>

The **dimension** of $V$ is the number of vectors in any basis of $V$.
:::

::: {.mathbox .theorem}
<div class="mathbox-title">Theorem</div>

All bases of a finite-dimensional vector space have the same number of elements.
:::

---

## Coordinates

Given a basis $\{e_1,\dots,e_n\}$, any vector $v \in V$ can be written uniquely as:
$$
v = x_1 e_1 + \cdots + x_n e_n.
$$

The coefficients $(x_1,\dots,x_n)$ are the **coordinates** of $v$ in that basis.

---

## Computational example (Python)

```python
import numpy as np

v1 = np.array([1, 0, 1])
v2 = np.array([0, 1, 1])
v3 = np.array([1, 1, 2])

# Check linear dependence
M = np.column_stack([v1, v2, v3])
np.linalg.matrix_rank(M)
```

If the rank is smaller than the number of vectors, they are linearly dependent.

## Connection to Data Science

A dataset with $n$ features lives in $\mathbb{R}^n$.

Feature engineering changes the basis.

Dimensionality reduction (PCA, SVD) finds lower-dimensional subspaces.

Embeddings (word2vec, image embeddings) are vectors in high-dimensional spaces.

Understanding vector spaces is the first step toward understanding geometry of data.