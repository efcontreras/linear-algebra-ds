---
title: "Principal Component Analysis (PCA): A Spectral and SVD-Based Application"
---

## Motivation

**Principal Component Analysis (PCA)** is one of the most widely used techniques
in Data Science for:

- dimensionality reduction,
- data visualization,
- noise filtering,
- feature extraction.

Despite its popularity, PCA is often presented as a purely algorithmic method.
In reality, PCA is a **direct application of spectral theory and the Singular Value Decomposition**.

This chapter presents PCA from a **linear algebra perspective**,
connecting geometry, optimization, and computation.

---

## Setting and Notation

Let

$$
X \in \mathbb{R}^{m \times n}
$$

be a data matrix, where:
- rows correspond to observations,
- columns correspond to features.

We assume that $X$ is **centered**, meaning that each column has zero mean.

---

## PCA as a Variance Maximization Problem

### One-dimensional case

We seek a unit vector $v \in \mathbb{R}^n$ that maximizes the variance
of the projected data:

$$
\max_{\|v\|=1} \; \mathrm{Var}(Xv).
$$

Since $X$ is centered,

$$
\mathrm{Var}(Xv) = \frac{1}{m} \|Xv\|_2^2
= \frac{1}{m} v^\top X^\top X v.
$$

Thus, PCA solves the optimization problem

$$
\max_{\|v\|=1} v^\top \left(\frac{1}{m} X^\top X\right) v.
$$

---

## Spectral Interpretation

The matrix

$$
\Sigma := \frac{1}{m} X^\top X
$$

is the **empirical covariance matrix**.
It is symmetric and positive semidefinite.

By the Spectral Theorem:

- eigenvectors of $\Sigma$ form an orthonormal basis,
- eigenvalues are real and nonnegative.

::: {.mathbox .theorem}
<div class="mathbox-title">Theorem (PCA via Spectral Theory)</div>

The solution of the PCA maximization problem is given by the eigenvector
associated with the largest eigenvalue of $\Sigma$.
:::

Subsequent principal components are obtained by imposing orthogonality constraints.

---

## PCA via Singular Value Decomposition

Instead of forming $\Sigma$, we can compute the SVD of $X$:

$$
X = U \Sigma_X V^\top.
$$

Then:

- columns of $V$ are the **principal directions**,
- singular values $\sigma_i$ satisfy

$$
\lambda_i = \frac{\sigma_i^2}{m},
$$

where $\lambda_i$ are the eigenvalues of the covariance matrix.

::: {.mathbox .remark}
<div class="mathbox-title">Remark</div>

Computing PCA via SVD is numerically more stable than forming
$X^\top X$, especially when the number of features is large.
:::

---

## Low-Dimensional Embedding

Let $V_k \in \mathbb{R}^{n \times k}$ be the matrix of the first $k$
right singular vectors.

The PCA embedding of the data is:

$$
Z = X V_k \in \mathbb{R}^{m \times k}.
$$

Each row of $Z$ is the representation of a data point
in the $k$-dimensional principal subspace.

---

## Optimal Reconstruction Property

Define the rank-$k$ approximation

$$
X_k := U_k \Sigma_k V_k^\top.
$$

::: {.mathbox .theorem}
<div class="mathbox-title">Theorem (Optimal reconstruction)</div>

$X_k$ minimizes the reconstruction error

$$
\|X - Y\|_F
$$

among all matrices $Y$ of rank at most $k$.
:::

Thus, PCA provides the **best linear compression** of the data.

<details class="proof-toggle">
<summary>Proof of the Optimal Reconstruction Theorem (click to expand)</summary>

::: {.mathbox .proof}
<div class="mathbox-title">Proof</div>

Let $X \in \mathbb{R}^{m \times n}$ have Singular Value Decomposition

$$
X = \sum_{i=1}^r \sigma_i \, u_i v_i^\top,
$$

with singular values $\sigma_1 \ge \sigma_2 \ge \cdots \ge \sigma_r > 0$.

For a fixed integer $k < r$, define the truncated SVD

$$
X_k := \sum_{i=1}^k \sigma_i \, u_i v_i^\top.
$$

We claim that $X_k$ minimizes the reconstruction error

$$
\|X - Y\|_F
$$

among all matrices $Y \in \mathbb{R}^{m \times n}$ of rank at most $k$.

---

### Step 1: Orthogonal invariance of the Frobenius norm

The Frobenius norm is invariant under orthogonal transformations.  
That is, for any orthogonal matrices $U$ and $V$,

$$
\|X - Y\|_F = \|U^\top X V - U^\top Y V\|_F.
$$

Applying this with the singular vector bases of $X$, we obtain

$$
\|X - Y\|_F
=
\|\Sigma - \widetilde{Y}\|_F,
$$

where $\widetilde{Y} = U^\top Y V$ and $\mathrm{rank}(\widetilde{Y}) \le k$.

---

### Step 2: Reduction to a diagonal problem

The matrix $\Sigma$ is diagonal with entries $\sigma_1,\dots,\sigma_r$.
Any matrix $\widetilde{Y}$ of rank at most $k$ can have at most $k$
nonzero singular values.

Thus, minimizing $\|\Sigma - \widetilde{Y}\|_F$ reduces to choosing
at most $k$ diagonal entries of $\Sigma$ to keep, and setting the rest to zero.

---

### Step 3: Optimal choice

Since

$$
\|\Sigma - \widetilde{Y}\|_F^2
=
\sum_{i=1}^r (\sigma_i - \widetilde{\sigma}_i)^2,
$$

the minimum is achieved by choosing

$$
\widetilde{\sigma}_i =
\begin{cases}
\sigma_i, & i = 1,\dots,k, \\
0, & i > k.
\end{cases}
$$

This corresponds exactly to $\widetilde{Y} = \Sigma_k$ and hence
$Y = X_k$.

---

### Step 4: Conclusion

Therefore,

$$
\|X - X_k\|_F
=
\min_{\mathrm{rank}(Y)\le k} \|X - Y\|_F,
$$

which proves that the truncated SVD $X_k$ is the **best rank-$k$
approximation** of $X$ in Frobenius norm.

<span class="proof-end">□</span>
:::

</details>


---

## Computational Example (Python)

```python
import numpy as np

rng = np.random.default_rng(0)
X = rng.normal(size=(300, 3))
X[:, 2] = X[:, 0] + 0.1 * rng.normal(size=300)

# center data
X = X - X.mean(axis=0)

U, s, Vt = np.linalg.svd(X, full_matrices=False)

explained_variance = s**2 / np.sum(s**2)
explained_variance
```

## Explained Variance Ratio

The proportion of variance explained by the $k$-th principal component is

$$
\frac{\sigma_k^2}{\sum_i \sigma_i^2}.
$$

This quantity is used in practice to choose the number of components.

---

## Geometric Interpretation

- PCA finds orthogonal directions of maximal variance.
- Data is projected onto a lower-dimensional linear subspace.
- Noise typically concentrates in directions with small singular values.

Geometrically, PCA fits a **best affine subspace** to the data.

---

## When PCA Works Well (and When It Does Not)

### Works well when:
- data lies near a linear subspace,
- noise is approximately isotropic,
- variance is a meaningful notion of information.

### Limitations:
- sensitive to outliers,
- captures only linear structure,
- depends on feature scaling.

---

## Connections to Other Methods

- **Truncated SVD**: computational backbone of PCA  
- **Kernel PCA**: nonlinear extension using kernels  
- **Autoencoders**: nonlinear generalization of PCA  
- **Factor Analysis**: probabilistic variant  

---

## Summary

- PCA is an application of spectral theory and SVD.
- Principal components are eigenvectors of the covariance matrix.
- SVD provides a stable and efficient way to compute PCA.
- PCA yields optimal low-rank approximations.
- Understanding PCA requires understanding linear algebra, not just algorithms.

---

## Preview: Regularization and Truncated SVD

Small singular values amplify noise.
In the next application chapter, we study:

- truncated SVD as a denoising method,
- ridge regression as spectral regularization,
- bias–variance trade-offs.
