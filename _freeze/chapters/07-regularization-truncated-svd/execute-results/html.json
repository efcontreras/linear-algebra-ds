{
  "hash": "61a0420777929356f35041bcd53428c3",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Regularization and Truncated SVD\"\n---\n\n## Motivation\n\nSingular values quantify how strongly a matrix acts along different directions.\nWhen some singular values are **very small**, linear inverse problems become unstable:\nsmall perturbations in the data can lead to large changes in the solution.\n\n**Regularization** addresses this instability by damping or discarding\ndirections associated with small singular values.\n\nTwo central regularization strategies are:\n\n- **Truncated SVD (TSVD)**,\n- **Tikhonov regularization (ridge regression)**.\n\nBoth admit a clean **spectral interpretation**.\n\n---\n\n## Ill-Posed Linear Problems\n\nConsider the linear model\n\n$$\nAx \\approx b,\n$$\n\nwith $A \\in \\mathbb{R}^{m \\times n}$ and noisy data $b$.\n\nIf $A$ has small singular values, the least-squares solution\n\n$$\nx^\\star = \\arg\\min_x \\|Ax - b\\|_2^2\n$$\n\nis unstable.\n\n---\n\n## Least Squares in SVD Coordinates\n\nLet\n\n$$\nA = U \\Sigma V^\\top\n$$\n\nbe the SVD of $A$.\nThe least-squares solution can be written as\n\n$$\nx^\\star = \\sum_{i=1}^r \\frac{u_i^\\top b}{\\sigma_i} v_i.\n$$\n\nThis expression makes the instability explicit:\n\n- coefficients are divided by $\\sigma_i$,\n- small $\\sigma_i$ amplify noise.\n\n---\n\n## Truncated SVD (TSVD)\n\n### Definition\n\n::: {.mathbox .definition}\n<div class=\"mathbox-title\">Definition (Truncated SVD)</div>\n\nLet $k < r = \\mathrm{rank}(A)$.\nThe **truncated SVD solution** is defined as\n\n$$\nx_k := \\sum_{i=1}^k \\frac{u_i^\\top b}{\\sigma_i} v_i.\n$$\n:::\n\nThis corresponds to discarding all singular components\nassociated with $\\sigma_{k+1},\\dots,\\sigma_r$.\n\n---\n\n### Geometric Interpretation\n\n- Keep only the $k$ most informative directions.\n- Project data onto a $k$-dimensional dominant subspace.\n- Discard directions dominated by noise.\n\nTSVD is a **hard spectral filter**.\n\n---\n\n## Optimality Perspective\n\n::: {.mathbox .theorem}\n<div class=\"mathbox-title\">Theorem</div>\n\nThe truncated SVD solution minimizes\n\n$$\n\\|Ax - b\\|_2\n$$\n\nover all solutions $x$ constrained to lie in the span of the first $k$\nright singular vectors of $A$.\n:::\n\nThus, TSVD solves a constrained optimization problem.\n\n---\n\n## Tikhonov Regularization (Ridge)\n\n### Definition\n\nInstead of discarding components, ridge regression penalizes large solutions:\n\n$$\nx_\\lambda := \\arg\\min_x \\bigl( \\|Ax - b\\|_2^2 + \\lambda \\|x\\|_2^2 \\bigr),\n$$\n\nwith $\\lambda > 0$.\n\n---\n\n### Spectral Form\n\nIn SVD coordinates, the ridge solution is\n\n$$\nx_\\lambda = \\sum_{i=1}^r\n\\frac{\\sigma_i}{\\sigma_i^2 + \\lambda}\n(u_i^\\top b)\\, v_i.\n$$\n\nEach component is multiplied by a **filter factor**\n\n$$\nf_\\lambda(\\sigma_i) = \\frac{\\sigma_i}{\\sigma_i^2 + \\lambda}.\n$$\n\n---\n\n## Hard vs. Soft Spectral Filtering\n\n| Method | Filter behavior |\n|------|-----------------|\n| Truncated SVD | $f(\\sigma)=1/\\sigma$ for $i\\le k$, $0$ otherwise |\n| Ridge | smooth damping for all $\\sigma_i$ |\n| No regularization | $1/\\sigma$ (unstable) |\n\n- TSVD: **hard cutoff**\n- Ridge: **soft shrinkage**\n\n---\n\n## Bias–Variance Trade-off\n\nRegularization introduces bias but reduces variance.\n\n- Small $k$ (or large $\\lambda$):\n  - high bias,\n  - low variance.\n- Large $k$ (or small $\\lambda$):\n  - low bias,\n  - high variance.\n\nThis trade-off is fundamental in statistical learning.\n\n---\n\n## Computational Example (Python)\n\n::: {#7fc79a67 .cell execution_count=1}\n```` { .cell-code}\n```{{python}}\nimport numpy as np\n\nrng = np.random.default_rng(0)\nA = rng.normal(size=(50, 30))\nx_true = rng.normal(size=30)\nb = A @ x_true + 0.1 * rng.normal(size=50)\n\nU, s, Vt = np.linalg.svd(A, full_matrices=False)\n\n# TSVD\nk = 10\nx_tsvd = sum((U[:, i] @ b) / s[i] * Vt[i] for i in range(k))\n\n# Ridge\nlam = 0.5\nx_ridge = sum((s[i] / (s[i]**2 + lam)) * (U[:, i] @ b) * Vt[i]\n              for i in range(len(s)))\n\nnp.linalg.norm(x_true - x_tsvd), np.linalg.norm(x_true - x_ridge)\n```\n\n````\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\n(np.float64(3.3811699474285137), np.float64(0.25795525800378033))\n```\n:::\n:::\n\n\n## Choosing the Regularization Parameter\n\n### For Truncated SVD\n- scree plot of singular values,\n- cumulative explained energy,\n- cross-validation.\n\n### For Ridge\n- cross-validation,\n- L-curve,\n- discrepancy principle.\n\n---\n\n## Relation to PCA and Dimensionality Reduction\n\n- Truncated SVD corresponds to PCA truncation.\n- Ridge keeps all components but shrinks them.\n- PCA + regression ≈ truncated spectral solution.\n\n---\n\n## When to Use Which Method\n\n### Use Truncated SVD when:\n- a clear spectral gap exists,\n- dimensionality reduction is desired,\n- interpretability matters.\n\n### Use Ridge when:\n- the spectrum decays smoothly,\n- stability is critical,\n- prediction accuracy is the goal.\n\n---\n\n## Summary\n\n- Small singular values cause instability.\n- Truncated SVD removes unstable directions.\n- Ridge regularization damps all directions smoothly.\n- Both methods admit clean spectral interpretations.\n- Regularization is fundamentally a spectral filtering problem.\n\n---\n\n## Preview: Spectral Clustering\n\nEigenvectors of graph Laplacians can be used\nto uncover cluster structure in data.\n\nNext, we study **spectral clustering** as another\napplication of spectral theory.\n\n\n\n### Scree Plot (Singular Values)\n\n::: {#c7c11cfa .cell execution_count=2}\n```` { .cell-code}\n```{{python}}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Assume: s is the 1D array of singular values from SVD, sorted decreasingly.\n# Example: U, s, Vt = np.linalg.svd(A, full_matrices=False)\n\nfig, ax = plt.subplots()\nax.plot(np.arange(1, len(s) + 1), s, marker=\"o\")\nax.set_xlabel(\"Index i\")\nax.set_ylabel(\"Singular value σᵢ\")\nax.set_title(\"Scree plot of singular values\")\nax.grid(True)\nplt.show()\n```\n\n````\n\n::: {.cell-output .cell-output-display}\n![](07-regularization-truncated-svd_files/figure-html/cell-3-output-1.png){width=585 height=449}\n:::\n:::\n\n\n---\n\n\n\n### Cumulative Explained Energy\n\n::: {#1ff75da8 .cell execution_count=3}\n```` { .cell-code}\n```{{python}}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nenergy = s**2\ncum_energy = np.cumsum(energy) / np.sum(energy)\n\nfig, ax = plt.subplots()\nax.plot(np.arange(1, len(s) + 1), cum_energy, marker=\"o\")\nax.set_xlabel(\"k\")\nax.set_ylabel(\"Cumulative explained energy\")\nax.set_title(\"Cumulative explained energy vs k\")\nax.set_ylim(0, 1.02)\nax.grid(True)\n\n# Optional: a common threshold (e.g., 90% or 95%)\nthreshold = 0.95\nax.axhline(threshold, linestyle=\"--\")\nplt.show()\n\n# Suggested k for the chosen threshold\nk_star = int(np.searchsorted(cum_energy, threshold) + 1)\nk_star\n```\n\n````\n\n::: {.cell-output .cell-output-display}\n![](07-regularization-truncated-svd_files/figure-html/cell-4-output-1.png){width=589 height=449}\n:::\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n22\n```\n:::\n:::\n\n\n---\n\n### TSVD Residual Curve: ‖Ax_k − b‖ vs k\n\n::: {#a689e87b .cell execution_count=4}\n```` { .cell-code}\n```{{python}}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# We compute x_k and residual norms efficiently using SVD coordinates.\n# x_k = sum_{i=1}^k (u_i^T b / s_i) v_i\n# residual r_k = b - A x_k\n\nUt_b = U.T @ b  # coefficients u_i^T b\nmax_k = len(s)\n\nresidual_norms = []\nfor k in range(1, max_k + 1):\n    # Build x_k via right singular vectors:\n    x_k = (Vt[:k, :].T) @ (Ut_b[:k] / s[:k])\n    r_k = b - A @ x_k\n    residual_norms.append(np.linalg.norm(r_k))\n\nfig, ax = plt.subplots()\nax.plot(np.arange(1, max_k + 1), residual_norms, marker=\"o\")\nax.set_xlabel(\"k\")\nax.set_ylabel(\"Residual norm ‖A x_k − b‖₂\")\nax.set_title(\"TSVD residual vs k\")\nax.grid(True)\nplt.show()\n```\n\n````\n\n::: {.cell-output .cell-output-display}\n![](07-regularization-truncated-svd_files/figure-html/cell-5-output-1.png){width=585 height=449}\n:::\n:::\n\n\n### Truncation Error: ‖A − A_k‖_F vs k\n\n::: {#2e7dbfbb .cell execution_count=5}\n```` { .cell-code}\n```{{python}}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# For full matrices, the Frobenius truncation error is determined by singular values:\n# ||A - A_k||_F^2 = sum_{i>k} s_i^2\n\ntail_energy = np.cumsum((s[::-1]**2))[::-1]  # tail sums of squares\nfro_errors = np.sqrt(np.r_[tail_energy[1:], 0.0])  # error for k=1..r, last is 0\n\nfig, ax = plt.subplots()\nax.plot(np.arange(1, len(s) + 1), fro_errors, marker=\"o\")\nax.set_xlabel(\"k\")\nax.set_ylabel(\"‖A − A_k‖_F\")\nax.set_title(\"Frobenius truncation error vs k\")\nax.grid(True)\nplt.show()\n```\n\n````\n\n::: {.cell-output .cell-output-display}\n![](07-regularization-truncated-svd_files/figure-html/cell-6-output-1.png){width=585 height=449}\n:::\n:::\n\n\n---\n\n### L-curve for Ridge Regularization\n\n::: {#ec0c61b2 .cell execution_count=6}\n```` { .cell-code}\n```{{python}}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Ridge solution in SVD coordinates:\n# x_λ = sum_i (s_i / (s_i^2 + λ)) (u_i^T b) v_i\n\nUt_b = U.T @ b\nr = len(s)\n\n# Choose a grid of lambdas (log-spaced)\nlambdas = np.logspace(-6, 2, 60)\n\nx_norms = []\nres_norms = []\n\nfor lam in lambdas:\n    coeffs = (s / (s**2 + lam)) * Ut_b[:r]\n    x_lam = Vt.T @ coeffs\n    res = A @ x_lam - b\n    x_norms.append(np.linalg.norm(x_lam))\n    res_norms.append(np.linalg.norm(res))\n\nfig, ax = plt.subplots()\nax.plot(res_norms, x_norms, marker=\"o\")\nax.set_xlabel(\"‖A x_λ − b‖₂ (residual norm)\")\nax.set_ylabel(\"‖x_λ‖₂ (solution norm)\")\nax.set_title(\"L-curve for ridge regularization\")\nax.grid(True)\nplt.show()\n```\n\n````\n\n::: {.cell-output .cell-output-display}\n![](07-regularization-truncated-svd_files/figure-html/cell-7-output-1.png){width=589 height=450}\n:::\n:::\n\n\n",
    "supporting": [
      "07-regularization-truncated-svd_files"
    ],
    "filters": [],
    "includes": {}
  }
}