{
  "hash": "634fb766a12a52665ddc3b22e1339da9",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Spectral Clustering\"\n---\n\n## Motivation\n\nMany datasets exhibit **cluster structure** that cannot be captured by\nlinear separation in the original feature space.\n\nSpectral clustering approaches this problem by:\n\n- representing data as a **graph**,\n- using eigenvectors of a graph Laplacian,\n- embedding data into a low-dimensional spectral space,\n- performing clustering in that space.\n\nThis method is fundamentally based on **spectral theory** and\nconnects linear algebra, graph theory, and geometry.\n\n---\n\n## From Data to Graphs\n\nLet $\\{x_1,\\dots,x_n\\} \\subset \\mathbb{R}^d$ be a dataset.\n\nWe construct a weighted, undirected graph:\n- nodes correspond to data points,\n- edges encode similarity.\n\n---\n\n## Similarity Matrix\n\n::: {.mathbox .definition}\n<div class=\"mathbox-title\">Definition (Similarity matrix)</div>\n\nThe **similarity matrix** $W \\in \\mathbb{R}^{n \\times n}$ is defined by\n\n$$\nW_{ij} = s(x_i, x_j),\n$$\n\nwhere $s(\\cdot,\\cdot)$ is a symmetric similarity function.\n\nCommon choices include:\n- Gaussian kernel:\n  $$\n  W_{ij} = \\exp\\!\\left(-\\frac{\\|x_i - x_j\\|^2}{2\\sigma^2}\\right),\n  $$\n- $k$-nearest neighbors (binary or weighted).\n\nWe assume $W_{ij} \\ge 0$ and $W_{ii} = 0$.\n:::\n\n---\n\n## Degree Matrix\n\n::: {.mathbox .definition}\n<div class=\"mathbox-title\">Definition (Degree matrix)</div>\n\nThe **degree matrix** $D$ is diagonal, with entries\n\n$$\nD_{ii} = \\sum_{j=1}^n W_{ij}.\n$$\n:::\n\n---\n\n## Graph Laplacians\n\n### Unnormalized Laplacian\n\n::: {.mathbox .definition}\n<div class=\"mathbox-title\">Definition (Unnormalized Laplacian)</div>\n\nThe unnormalized graph Laplacian is\n\n$$\nL = D - W.\n$$\n:::\n\n---\n\n### Normalized Laplacians\n\nTwo normalized variants are commonly used.\n\n::: {.mathbox .definition}\n<div class=\"mathbox-title\">Definition (Normalized Laplacians)</div>\n\nThe **random-walk Laplacian** is\n\n$$\nL_{\\mathrm{rw}} = I - D^{-1} W.\n$$\n\nThe **symmetric normalized Laplacian** is\n\n$$\nL_{\\mathrm{sym}} = I - D^{-1/2} W D^{-1/2}.\n$$\n:::\n\nIn practice, $L_{\\mathrm{sym}}$ is the most widely used.\n\n---\n\n## Spectral Properties\n\n::: {.mathbox .theorem}\n<div class=\"mathbox-title\">Theorem</div>\n\nFor any graph Laplacian $L$:\n\n1. $L$ is symmetric positive semidefinite.\n2. The smallest eigenvalue is $0$.\n3. The multiplicity of the eigenvalue $0$ equals the number of\n   connected components of the graph.\n:::\n\n---\n\n<details class=\"proof-toggle\">\n<summary>Why the Laplacian is positive semidefinite (click to expand)</summary>\n\n::: {.mathbox .proof}\n<div class=\"mathbox-title\">Proof</div>\n\nFor the unnormalized Laplacian $L = D - W$ and any $f \\in \\mathbb{R}^n$,\n\n$$\nf^\\top L f\n= \\frac{1}{2} \\sum_{i,j} W_{ij} (f_i - f_j)^2 \\ge 0.\n$$\n\nHence $L$ is positive semidefinite.\n<span class=\"proof-end\">□</span>\n:::\n\n</details>\n\n---\n\n## Clustering as a Graph Cut Problem\n\nClustering can be formulated as minimizing a graph cut:\n\n$$\n\\mathrm{cut}(A,B) = \\sum_{i \\in A,\\, j \\in B} W_{ij}.\n$$\n\nHowever, minimizing raw cuts leads to unbalanced partitions.\n\nNormalized objectives such as:\n- **Ratio Cut**\n- **Normalized Cut**\n\nlead naturally to spectral relaxations involving Laplacians.\n\n---\n\n## Spectral Relaxation\n\nThe discrete clustering problem is NP-hard.\nSpectral clustering replaces it by a continuous optimization problem:\n\n- relax indicator vectors to real-valued functions,\n- impose orthogonality constraints,\n- solve using eigenvectors of the Laplacian.\n\nThe solution is given by the eigenvectors associated with the **smallest\nnonzero eigenvalues**.\n\n---\n\n## Spectral Clustering Algorithm\n\nUsing the symmetric normalized Laplacian $L_{\\mathrm{sym}}$:\n\n1. Construct the similarity matrix $W$.\n2. Compute $D$ and $L_{\\mathrm{sym}}$.\n3. Compute the first $k$ eigenvectors of $L_{\\mathrm{sym}}$.\n4. Form the matrix $U \\in \\mathbb{R}^{n \\times k}$ from these eigenvectors.\n5. Normalize rows of $U$ to unit length.\n6. Apply $k$-means to the rows of $U$.\n\nThe final labels are the cluster assignments.\n\n---\n\n## Geometric Interpretation\n\n- Eigenvectors embed nodes into a low-dimensional space.\n- In this space, clusters become approximately convex and separable.\n- $k$-means works effectively in the spectral embedding.\n\nSpectral clustering is thus:\n> **nonlinear clustering via linear algebra**.\n\n---\n\n## Computational Example (Python)\n\n::: {#c616b56e .cell execution_count=1}\n```` { .cell-code}\n```{{python}}\nimport numpy as np\nfrom sklearn.datasets import make_moons\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics.pairwise import rbf_kernel\nfrom scipy.linalg import eigh\n\n# Generate nonlinearly separable data\nX, _ = make_moons(n_samples=300, noise=0.05)\n\n# Similarity matrix\nW = rbf_kernel(X, gamma=10.0)\n\n# Degree matrix\nD = np.diag(W.sum(axis=1))\n\n# Symmetric normalized Laplacian\nD_inv_sqrt = np.diag(1.0 / np.sqrt(np.diag(D)))\nL_sym = np.eye(len(X)) - D_inv_sqrt @ W @ D_inv_sqrt\n\n# Eigen-decomposition\neigvals, eigvecs = eigh(L_sym)\nU = eigvecs[:, :2]\n\n# Row normalization\nU_norm = U / np.linalg.norm(U, axis=1, keepdims=True)\n\n# k-means\nlabels = KMeans(n_clusters=2, n_init=10).fit_predict(U_norm)\nlabels[:10]\n```\n\n````\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\narray([0, 0, 1, 0, 0, 0, 0, 0, 0, 0], dtype=int32)\n```\n:::\n:::\n\n\n## Choosing the Number of Clusters\n\nChoosing the number of clusters $k$ is a central modeling decision in spectral clustering.\n\nCommon strategies include:\n\n- **Multiplicity of eigenvalue $0$**:  \n  In the ideal case of perfectly disconnected components, the number of\n  connected components equals the multiplicity of the eigenvalue $0$\n  of the graph Laplacian.\n\n- **Eigengap heuristic**:  \n  Sort eigenvalues $\\lambda_1 \\le \\lambda_2 \\le \\cdots$ of the Laplacian.\n  A large gap between $\\lambda_k$ and $\\lambda_{k+1}$ suggests choosing $k$ clusters.\n\n- **Stability analysis**:  \n  Run spectral clustering for different values of $k$ and assess\n  the stability of the resulting partitions.\n\n- **Domain knowledge**:  \n  Structural or semantic constraints may dictate a natural choice of $k$.\n\nEigenvalue gaps play a role analogous to scree plots in PCA.\n\n---\n\n## When Spectral Clustering Works Well\n\n### Advantages\n- captures nonlinear cluster structure,\n- flexible choice of similarity functions,\n- strong theoretical grounding via spectral graph theory.\n\n### Limitations\n- sensitive to kernel choice and hyperparameters,\n- eigen-decomposition can be computationally expensive for large $n$,\n- scalability often requires approximations (Nyström, sparse graphs).\n\n---\n\n## Connections to Other Methods\n\n- **Kernel PCA**: spectral embedding of kernel (Gram) matrices.\n- **Diffusion maps**: Markov process interpretation of random walks on graphs.\n- **Graph neural networks**: learnable spectral and message-passing operators.\n\n---\n\n## Summary\n\n- Spectral clustering reduces clustering to an eigenvalue problem.\n- Graph Laplacians encode connectivity and geometry.\n- Eigenvectors provide low-dimensional spectral embeddings.\n- The eigengap heuristic guides the choice of the number of clusters.\n- Spectral clustering is a paradigmatic application of spectral theory in Data Science.\n\n---\n\n## Preview: Final Remarks\n\nSpectral methods unify linear algebra, geometry, and data analysis.\nIn the final chapter, we synthesize the main ideas and compare\nspectral techniques across applications.\n\n",
    "supporting": [
      "08-spectral-clustering_files"
    ],
    "filters": [],
    "includes": {}
  }
}